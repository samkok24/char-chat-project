# AI 모델 벤치마크 결과 (최종)

**테스트 일시**: 2026-02-22
**캐릭터**: 255833e3-35d2-4058-a128-8fac7ac39c6b (루나)
**오프닝**: set_1
**모델당 실행 횟수**: 10
**응답 길이 설정**: medium
**Gemini thinkingBudget**: 128 (SDK 1.64.0)
**GPT reasoning_effort**: low (openai SDK 2.21.0)

---

## 종합 비교 (전 모델)

| 모델 | TTFB(s) | 총시간(s) | 글자수 | 말풍선 | 한국어% | 잘림 | 대사없음 | 1,000회 비용 |
|------|---------|----------|--------|--------|---------|------|---------|-------------|
| Claude Haiku 4.5 | **0.94**±0.29 | 4.61±0.77 | 377±88 | 6.0 | 83% | 0/10 | 0/10 | $3.86 |
| Claude Sonnet 4 | 1.79±0.35 | 9.83±1.17 | 386±59 | 5.7 | 85% | 0/10 | 0/10 | $14.69 |
| Claude Sonnet 4.5 | 1.64±**0.22** | 10.79±**0.79** | 434±**33** | 7.0 | 82% | 0/10 | 0/10 | $15.77 |
| Claude Opus 4.1 | 3.03±0.25 | 14.24±2.54 | 413±57 | 8.0 | 80% | 0/10 | 0/10 | $76.50 |
| Claude Opus 4.5 | 1.52±0.16 | 9.81±1.56 | 402±70 | 5.9 | 83% | 0/10 | 0/10 | $75.23 |
| Gemini 2.5 Flash | 1.9±0.4 | **2.8**±0.6 | 362±103 | 3.4 | **91%** | 3/10 | 0/10 | **$0.70** |
| Gemini 2.5 Pro | 3.0±0.6 | 5.8±0.8 | 510±102 | **7.3** | 86% | 1/10 | 1/10 | $7.07 |
| Gemini 3 Pro Preview | 5.1±4.8 | 9.0±4.8 | **627**±85 | 5.8 | **89%** | 0/10 | 0/10 | $8.05 |
| GPT-4.1 | 1.35±0.63 | 3.66±0.93 | 362±67 | 3.1 | 89% | 0/10 | 0/10 | $8.34 |
| GPT-4.1 mini | **1.14**±0.59 | **3.03**±1.51 | 229±38 | 1.8 | **100%** | 0/10 | 2/10 | $1.35 |
| GPT-5 mini | 5.08±1.21 | 7.66±1.89 | 302±46 | 2.0 | **97%** | 0/10 | 0/10 | $2.01 |
| GPT-5.1 | 3.56±1.77 | 7.45±1.76 | 449±52 | 4.0 | 87% | 0/10 | 0/10 | $12.24 |

> 비용 산출 기준: 입력 2,000토큰/턴, 출력=글자수x1.5토큰, reasoning 모델은 추론 토큰 ~300개 추가 (출력 단가 적용)
> Gemini thinking 토큰은 무과금. GPT reasoning 토큰은 출력 단가로 과금.

---

## GPT 모델 상세 결과

### GPT-4.1

| 항목 | 수치 |
|------|------|
| TTFB | 1.35s ± 0.63 |
| 총시간 | 3.66s ± 0.93 |
| 글자수 | 362 ± 67 |
| 말풍선 | 3.1 |
| 한국어% | 89% |
| 잘림/대사없음 | 0/10, 0/10 |
| 비고 | 비추론 모델. 빠르고 안정적. Chat Completions API 사용 |

### GPT-4.1 mini

| 항목 | 수치 |
|------|------|
| TTFB | 1.14s ± 0.59 |
| 총시간 | 3.03s ± 1.51 |
| 글자수 | 229 ± 38 |
| 말풍선 | 1.8 |
| 한국어% | 100% |
| 잘림/대사없음 | 0/10, **2/10** |
| 비고 | 글자수 너무 적고 대사없음 2/10 → **서비스 부적합** |

### GPT-5 mini (reasoning_effort=low)

| 항목 | 수치 |
|------|------|
| TTFB | 5.08s ± 1.21 |
| 총시간 | 7.66s ± 1.89 |
| 글자수 | 302 ± 46 |
| 말풍선 | 2.0 |
| 한국어% | 97% |
| 잘림/대사없음 | 0/10, 0/10 |
| 비고 | reasoning_effort 미설정 시 전멸(0/10). low로 해결. 한국어 비율 최상위급 |

### GPT-5.1 (reasoning_effort=low)

| 항목 | 수치 |
|------|------|
| TTFB | 3.56s ± 1.77 |
| 총시간 | 7.45s ± 1.76 |
| 글자수 | 449 ± 52 |
| 말풍선 | 4.0 |
| 한국어% | 87% |
| 잘림/대사없음 | 0/10, 0/10 |
| 비고 | medium→low 변경으로 TTFB 12.72s→3.56s (72% 개선). 글자수는 유지(449) |

---

## 속도 순위 (TTFB)

| 순위 | 모델 | TTFB | 총시간 |
|------|------|------|--------|
| 1 | **Claude Haiku 4.5** | 0.94s | 4.61s |
| 2 | GPT-4.1 mini | 1.14s | 3.03s |
| 3 | GPT-4.1 | 1.35s | 3.66s |
| 4 | Claude Opus 4.5 | 1.52s | 9.81s |
| 5 | Claude Sonnet 4.5 | 1.64s | 10.79s |
| 6 | Claude Sonnet 4 | 1.79s | 9.83s |
| 7 | Gemini 2.5 Flash | 1.9s | 2.8s |
| 8 | Gemini 2.5 Pro | 3.0s | 5.8s |
| 9 | Claude Opus 4.1 | 3.03s | 14.24s |
| 10 | GPT-5.1 | 3.56s | 7.45s |
| 11 | Gemini 3 Pro Preview | 5.1s | 9.0s |
| 12 | GPT-5 mini | 5.08s | 7.66s |

## 품질 순위 (글자수 x 한국어비율)

| 순위 | 모델 | 평균 글자 | 말풍선 | 한국어% | 잘림 |
|------|------|----------|--------|---------|------|
| 1 | **Gemini 3 Pro Preview** | 627자 | 5.8 | 89% | 0/10 |
| 2 | Gemini 2.5 Pro | 510자 | 7.3 | 86% | 1/10 |
| 3 | GPT-5.1 | 449자 | 4.0 | 87% | 0/10 |
| 4 | Claude Sonnet 4.5 | 434자 | 7.0 | 82% | 0/10 |
| 5 | Claude Opus 4.1 | 413자 | 8.0 | 80% | 0/10 |
| 6 | Claude Opus 4.5 | 402자 | 5.9 | 83% | 0/10 |
| 7 | Claude Sonnet 4 | 386자 | 5.7 | 85% | 0/10 |
| 8 | Claude Haiku 4.5 | 377자 | 6.0 | 83% | 0/10 |
| 9 | GPT-4.1 | 362자 | 3.1 | 89% | 0/10 |
| 10 | Gemini 2.5 Flash | 362자 | 3.4 | 91% | 3/10 |
| 11 | GPT-5 mini | 302자 | 2.0 | 97% | 0/10 |
| 12 | GPT-4.1 mini | 229자 | 1.8 | 100% | 0/10 |

## 안정성 순위 (글자수 편차 낮을수록 좋음)

| 순위 | 모델 | 글자 편차 | TTFB 편차 | 총시간 편차 |
|------|------|----------|----------|-----------|
| 1 | **Claude Sonnet 4.5** | ±33 | ±0.22 | ±0.79 |
| 2 | GPT-4.1 mini | ±38 | ±0.59 | ±1.51 |
| 3 | GPT-5 mini | ±46 | ±1.21 | ±1.89 |
| 4 | GPT-5.1 | ±52 | ±1.77 | ±1.76 |
| 5 | Claude Opus 4.1 | ±57 | ±0.25 | ±2.54 |
| 6 | Claude Sonnet 4 | ±59 | ±0.35 | ±1.17 |
| 7 | GPT-4.1 | ±67 | ±0.63 | ±0.93 |
| 8 | Claude Opus 4.5 | ±70 | ±0.16 | ±1.56 |
| 9 | Gemini 3 Pro Preview | ±85 | ±4.8 | ±4.8 |
| 10 | Claude Haiku 4.5 | ±88 | ±0.29 | ±0.77 |
| 11 | Gemini 2.5 Pro | ±102 | ±0.6 | ±0.8 |
| 12 | Gemini 2.5 Flash | ±103 | ±0.4 | ±0.6 |

---

## API 가격 비교

| 모델 | Input $/1M | Output $/1M | 1회 비용 | 1,000회 | 10,000회 |
|------|-----------|-------------|---------|---------|----------|
| Gemini 2.5 Flash | $0.15 | $0.60 | $0.0007 | **$0.70** | $7.00 |
| GPT-4.1 mini | $0.40 | $1.60 | $0.0014 | $1.35 | $13.50 |
| GPT-5 mini | $0.25 | $2.00 | $0.0020 | $2.01 | $20.10 |
| Claude Haiku 4.5 | $0.80 | $4.00 | $0.0039 | $3.86 | $38.60 |
| Gemini 2.5 Pro | $1.25 | $10.00 | $0.0071 | $7.07 | $70.70 |
| Gemini 3 Pro Preview | $1.25 | $10.00 | $0.0081 | $8.05 | $80.50 |
| GPT-4.1 | $2.00 | $8.00 | $0.0083 | $8.34 | $83.40 |
| GPT-5.1 | $1.25 | $10.00 | $0.0122 | $12.24 | $122.40 |
| Claude Sonnet 4 | $3.00 | $15.00 | $0.0147 | $14.69 | $146.90 |
| Claude Sonnet 4.5 | $3.00 | $15.00 | $0.0158 | $15.77 | $157.70 |
| Claude Opus 4.1 | $15.00 | $75.00 | $0.0765 | $76.50 | $765.00 |
| Claude Opus 4.5 | $15.00 | $75.00 | $0.0752 | $75.23 | $752.30 |

> GPT-5 mini, GPT-5.1의 reasoning 토큰은 output 단가로 과금 (별도 단가 없음)
> Gemini thinking 토큰은 무과금

---

## 경쟁사 가격 비교

### 크랙 (crack.wrtn.ai) — 1크래커 ≈ 1원

| 티어 | 모델 | 가격/턴 | 무료 |
|------|------|--------|------|
| 일반챗 | Claude 3 Haiku | 무료 | 350크래커/일 |
| 파워챗 | Gemini 2.5 Flash | 20원 | |
| 프로챗 | Gemini 2.5 Pro | 50~58원 | |
| 슈퍼챗 1.5 | Claude Sonnet 4 | 50원 | |
| 슈퍼챗 2.0 | Claude Sonnet 4.5 | 50원 | |
| 하이퍼챗 | (미공개) | 75원 | |

### 바베챗 (babechat.ai)

| 티어 | 모델 | 가격/턴 | 무료 |
|------|------|--------|------|
| 프로챗 | Gemini 2.5 Pro | 76~98원 | 10회/일 |
| Pro 패스 | Gemini 2.5 Pro | 67~75원 | 정액제 |
| 3 Flash Think | Gemini 3 Flash | 33~38원 | |
| Flash 무제한 | Gemini 2.5 Flash | 정액제 | |

---

## 서비스 단가 추천 (1루비 = 100원, $1 ≈ 1,430원)

### API 원가 vs 추천 판매가

| 모델 | API 원가/턴 | 추천 판매가 | 루비 | 마진 | 경쟁사 대비 |
|------|-----------|-----------|------|------|-----------|
| Claude Haiku 4.5 | 5.6원 | **무료** | 0 | -5.6원 (유입비) | 크랙 동일 |
| Gemini 2.5 Flash | 1.0원 | 20원 | 0.2 | 95% | 크랙 동일 |
| GPT-5 mini | 2.9원 | 30원 | 0.3 | 90% | 크랙 파워챗(20원) 대비 살짝 높음 |
| GPT-4.1 | 11.9원 | 30원 | 0.3 | 60% | - |
| Gemini 2.5 Pro | 10.1원 | 50원 | 0.5 | 80% | 크랙 프로(50원) 동일, 바베챗(76원) 대비 저렴 |
| Gemini 3 Pro Preview | 11.5원 | 50원 | 0.5 | 77% | 최고 품질(627자), 유일 |
| GPT-5.1 | 17.5원 | 50원 | 0.5 | 65% | GPT 브랜드 프리미엄 |
| Claude Sonnet 4 | 21.3원 | 50원 | 0.5 | 57% | 크랙 동일 |
| Claude Sonnet 4.5 | 21.8원 | 70원 | 0.7 | 69% | 크랙(50원) 대비 높지만 최고 안정성 |
| Claude Opus 4.1 | 107.9원 | **미제공** | - | 적자 | 1루비로도 적자 |
| Claude Opus 4.5 | 107.3원 | **미제공** | - | 적자 | 1루비로도 적자 |
| GPT-4.1 mini | 1.9원 | **미제공** | - | - | 품질 부족 (229자, 대사없음 2/10) |

### 추천 티어 구조

```
┌──────────────────────────────────────────────────────────┐
│  무료 (0 ruby)     Claude Haiku 4.5                      │
│  ─────────────────────────────────────────────────────── │
│  Basic (0.2 ruby = 20원)   Gemini 2.5 Flash              │
│  ─────────────────────────────────────────────────────── │
│  Standard (0.3 ruby = 30원) GPT-5 mini                   │
│  ─────────────────────────────────────────────────────── │
│  Pro (0.5 ruby = 50원)     Sonnet 4, Gemini 2.5 Pro,     │
│                            Gemini 3 Pro, GPT-5.1         │
│  ─────────────────────────────────────────────────────── │
│  Premium (0.7 ruby = 70원) Sonnet 4.5                    │
└──────────────────────────────────────────────────────────┘
```

### 티어별 근거

**무료 — Claude Haiku 4.5**
- 무료 모델로 최적: TTFB 0.94s(최고 속도), 잘림 0/10, 안정적
- API 원가 5.6원/턴은 유저 유입 비용으로 감수
- 크랙도 Haiku 무료 운영 (검증된 전략)

**Basic 0.2 ruby (20원) — Gemini 2.5 Flash**
- API 원가 1.0원 → 마진 95% (최고 수익률)
- 가장 빠른 총시간(2.8s), 한국어 91%
- 단점: 잘림 3/10 → "가끔 잘리지만 빠르고 저렴" 포지셔닝
- 크랙 파워챗과 동일 가격(20원)

**Standard 0.3 ruby (30원) — GPT-5 mini**
- API 원가 2.9원 → 마진 90%
- "GPT" 브랜드를 저가에 체험하는 진입점
- 한국어 97%(최상위), 잘림 0/10
- 단점: TTFB 5.08s로 느림, 글자수 302로 적음

**Pro 0.5 ruby (50원) — 4개 모델**
- Sonnet 4: 안정적 Claude (마진 57%)
- Gemini 2.5 Pro: 긴 글(510자), 말풍선 7.3 (마진 80%)
- Gemini 3 Pro Preview: 최고 품질 627자 (마진 77%)
- GPT-5.1: GPT 최고급, 449자 (마진 65%)
- 크랙 슈퍼챗(50원)과 동일, 바베챗(76원)보다 저렴

**Premium 0.7 ruby (70원) — Claude Sonnet 4.5**
- 가장 안정적 (글자수 편차 ±33, 시간 편차 ±0.79)
- 434자, 말풍선 7개, 잘림 0/10
- 크랙(50원)보다 높지만 "프리미엄 안정성"으로 차별화
- 마진 69%

**미제공 — Opus, GPT-4.1 mini**
- Opus 4.1/4.5: API 원가 107원 → 1루비(100원)에도 적자. Sonnet 대비 품질 차이 미미
- GPT-4.1 mini: 229자/1.8말풍선으로 품질 부족. 대사없음 2/10

### 마진 시뮬레이션 (월 10만 턴 기준)

| 시나리오 | 분포 | 매출(원) | API원가(원) | 마진(원) | 마진% |
|---------|------|---------|-----------|---------|-------|
| 무료 위주 | 60% 무료 + 30% Basic + 10% Pro | 1,100,000 | 575,000 | 525,000 | 48% |
| 유료 위주 | 30% 무료 + 30% Standard + 30% Pro + 10% Premium | 3,600,000 | 797,000 | 2,803,000 | 78% |
| 프로 집중 | 20% 무료 + 20% Basic + 40% Pro + 20% Premium | 4,800,000 | 996,000 | 3,804,000 | 79% |

> 서버/인프라 비용 별도. API 원가만 기준.

---

## reasoning_effort 비교 (GPT-5.x)

| 모델 | effort | TTFB | 총시간 | 글자수 | 성공률 |
|------|--------|------|--------|--------|--------|
| GPT-5 mini | 미설정 | - | - | 0 | **0/10 전멸** |
| GPT-5 mini | low | 5.08s | 7.66s | 302 | **10/10** |
| GPT-5.1 | medium | 12.72s | 17.56s | 451 | 10/10 |
| GPT-5.1 | low | **3.56s** | **7.45s** | 449 | 10/10 |

- GPT-5 계열은 `reasoning_effort` 미설정 시 reasoning에 출력 토큰 전부 소비 → 빈 응답
- `low`로 설정하면 TTFB 대폭 개선 + 품질 유지
- GPT-5.1: medium→low로 TTFB 72% 개선 (12.72s→3.56s), 글자수 차이 없음 (451→449)
- **low 확정** (Gemini thinkingBudget=128과 같은 맥락)

---

## thinkingBudget 비교 (Gemini, 0 vs 128)

| 모델 | budget | TTFB | 총시간 | 글자수 | 말풍선 | 잘림 |
|------|--------|------|--------|--------|--------|------|
| 2.5 Pro | 0 | - | - | **전멸** | - | - |
| 2.5 Pro | 128 | 3.0s | 5.8s | 510 | 7.3 | 1/10 |
| 2.5 Flash | 0 | **1.0s** | **2.0s** | 310 | 2.5 | 2/10 |
| 2.5 Flash | 128 | 1.9s | 2.8s | 362 | 3.4 | 3/10 |
| 3 Pro | 0 | - | - | **전멸** | - | - |
| 3 Pro | 128 | 5.1s | 9.0s | 627 | 5.8 | 0/10 |

- budget=0: 2.5 Pro와 3 Pro는 thinking 없이 작동 불가 (빈 응답)
- budget=128: 최소 thinking으로 속도 최적화 + 정상 출력 보장
- **128 확정**

---

## 가성비 순위 (서비스 제공 모델만)

| 순위 | 모델 | 품질점수 | 1,000회 API비용 | 가성비 |
|------|------|---------|---------------|--------|
| 1 | **Gemini 2.5 Flash** | 329 | $0.70 | 압도적 |
| 2 | **GPT-5 mini** | 293 | $2.01 | 매우 좋음 |
| 3 | **Claude Haiku 4.5** | 313 | $3.86 | 매우 좋음 |
| 4 | **Gemini 3 Pro Preview** | 558 | $8.05 | 좋음 |
| 5 | **Gemini 2.5 Pro** | 439 | $7.07 | 좋음 |
| 6 | GPT-5.1 | 391 | $12.24 | 좋음 |
| 7 | Claude Sonnet 4.5 | 356 | $15.77 | 보통 |
| 8 | Claude Sonnet 4 | 328 | $14.69 | 보통 |

> 품질점수 = 글자수 x 한국어비율 (예: 627 x 0.89 = 558)

---

## 핵심 인사이트

- **Opus급 모델 제거**: API 원가 107원 → 1루비(100원)에도 적자, Sonnet 대비 품질 차이 미미
- **GPT-4.1 mini 제거**: 229자/1.8말풍선, 대사없음 2/10으로 서비스 부적합
- **GPT-5 mini 복구 성공**: reasoning_effort=low + max_output_tokens 4096 클램프로 전멸→10/10
- **GPT-5.1 TTFB 72% 개선**: reasoning_effort medium→low, 품질 손실 없음
- **Gemini = 최고 가성비**: thinking 토큰 무과금, API 원가 대비 최고 마진
- **Claude = 최고 안정성**: Sonnet 4.5 글자수 편차 ±33 (전 모델 1위)
- **GPT = 브랜드 가치**: "GPT" 이름값으로 차별화, 한국어 비율 높음 (87~97%)

---

## 적용된 코드 변경사항

1. **google-genai SDK**: 1.2.0 → 1.64.0 (thinkingBudget 지원)
2. **anthropic SDK**: 0.36.2 → 0.83.0 (httpx 0.28 호환)
3. **openai SDK**: 1.47.1 → 2.21.0 (httpx 0.28 호환)
4. **`_make_thinking_config(128)`**: Gemini thinking 모델에 thinkingBudget=128 적용
5. **`_is_gemini_thinking_model()`**: 2.5/3 계열 통합 감지 함수
6. **maxOutputTokens 클램프**: Gemini 3 계열도 min 4096 적용
7. **`_reasoning_effort_for_model()`**: GPT-5 전체 계열에 reasoning_effort="low" 적용 (기존 5.1/5.2만 → 5-mini 포함)
8. **max_output_tokens 클램프**: GPT-5 reasoning 모델에 min 4096 적용 (SDK/REST x stream/non-stream 4곳)
