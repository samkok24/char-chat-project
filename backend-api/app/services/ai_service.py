"""
AI ëª¨ë¸ê³¼ì˜ ìƒí˜¸ì‘ìš©ì„ ë‹´ë‹¹í•˜ëŠ” ì„œë¹„ìŠ¤
- í˜„ì¬ëŠ” Gemini, Claude, OpenAI ëª¨ë¸ì„ ì§€ì› (í–¥í›„ í™•ì¥ ê°€ëŠ¥)
- ê° ëª¨ë¸ì˜ ì‘ë‹µì„ ì¼ê´€ëœ í˜•ì‹ìœ¼ë¡œ ë°˜í™˜í•˜ëŠ” ê²ƒì„ ëª©í‘œë¡œ í•¨
"""
import google.generativeai as genai
import anthropic  # Claude API ë¼ì´ë¸ŒëŸ¬ë¦¬
from typing import Literal, Optional, AsyncGenerator
from app.core.config import settings
from .vision_service import stage1_keywords_from_image_url, stage1_keywords_from_image_url as _stage1, _http_get_bytes
import mimetypes
import logging
import imghdr
from io import BytesIO
from PIL import Image
import base64
import asyncio
import time

logger = logging.getLogger(__name__)

# âœ… Vision ê²°ê³¼ ìºì‹œ(ì„±ëŠ¥/ì•ˆì •):
# - ê°™ì€ image_urlë¡œ ì§§ì€ ì‹œê°„ì— ì—¬ëŸ¬ ë²ˆ(ì˜ˆ: í”„ë¡œí•„ 2ë‹¨ê³„ ìë™ìƒì„±) í˜¸ì¶œë˜ë©´,
#   ë§¤ë²ˆ ì´ë¯¸ì§€ ë‹¤ìš´ë¡œë“œ + Claude Vision í˜¸ì¶œë¡œ 10~30ì´ˆê°€ ì¶”ê°€ëœë‹¤.
# - TTL ìºì‹œë¡œ "2ë²ˆì§¸ í˜¸ì¶œë¶€í„°" ì¦‰ì‹œ ë°˜í™˜í•´ UXë¥¼ ê°œì„ í•œë‹¤.
_VISION_TAGS_CACHE: dict[str, tuple[float, dict, dict]] = {}
_VISION_TAGS_CACHE_TTL_SEC = 600  # 10ë¶„
_VISION_TAGS_CACHE_MAX = 256

# Claude ëª¨ë¸ëª… ìƒìˆ˜ (ì „ì—­ ì°¸ì¡°ìš©)
# NOTE:
# - ClaudeëŠ” 4.0+ë§Œ ì‚¬ìš© (3.x ì§€ì› ì¢…ë£Œ ëŒ€ì‘)
# - Anthropic APIì˜ model ê°’ì€ "ë³„ì¹­(ì˜ˆ: claude-sonnet-4)"ì´ ì•„ë‹ˆë¼ "ìŠ¤ëƒ…ìƒ· ëª¨ë¸ëª…(ë‚ ì§œ í¬í•¨)"ì´ ì•ˆì •ì ì´ë‹¤.
#   (ë³„ì¹­ì€ ê³„ì •/ê¶Œí•œ/ë²„ì „ì— ë”°ë¼ 404(not_found)ë¡œ ì‹¤íŒ¨í•˜ëŠ” ì‚¬ë¡€ê°€ ìˆì–´ ìŠ¤ëƒ…ìƒ·ì„ SSOTë¡œ ì‚¬ìš©í•œë‹¤.)
CLAUDE_MODEL_PRIMARY = 'claude-sonnet-4-5-20250929'
CLAUDE_MODEL_LEGACY = 'claude-sonnet-4-20250514'  # í›„ë°© í˜¸í™˜/í´ë°±(êµ¬ë²„ì „ ì €ì¥ê°’ ëŒ€ì‘)

GPT_MODEL_PRIMARY = 'gpt-5'

# ì•ˆì „ ë¬¸ìì—´ ë³€í™˜ ìœ í‹¸
def _as_text(val) -> str:
    try:
        if val is None:
            return ""
        if isinstance(val, (list, tuple, set)):
            return ", ".join([str(v) for v in val if str(v).strip()])
        return str(val)
    except Exception:
        return ""


def _format_history_block(history: object, *, max_items: int = 20, max_chars: int = 4000) -> str:
    """
    ëª¨ë¸ ì…ë ¥ í”„ë¡¬í”„íŠ¸ì— í¬í•¨í•  "ìµœê·¼ ëŒ€í™”" ë¸”ë¡ì„ ìƒì„±í•œë‹¤.

    ë°°ê²½/ì˜ë„:
    - ì¼ë¶€ í˜¸ì¶œ(íŠ¹íˆ ì›ì‘ì±—)ì—ì„œ historyë¥¼ êµ¬ì„±í•´ ë„˜ê¸°ì§€ë§Œ, ê³¼ê±° êµ¬í˜„ì—ì„œëŠ” ì´ë¥¼ í”„ë¡¬í”„íŠ¸ì— ë°˜ì˜í•˜ì§€ ì•Šì•„
      ëª¨ë¸ì´ ì§ì „ ëŒ€í™” ë‚´ìš©ì„ ë§ê°í•˜ê³  ì„¤ì •/ê³ ìœ ëª…ì‚¬ë¥¼ ì¦‰í¥ì ìœ¼ë¡œ ì¬ì‘ì„±í•˜ëŠ” ë¬¸ì œê°€ ìˆì—ˆë‹¤.
    - history êµ¬ì¡°ê°€ í˜¸ì¶œì²˜ë§ˆë‹¤ ì¡°ê¸ˆì”© ë‹¤ë¥¼ ìˆ˜ ìˆìœ¼ë¯€ë¡œ(dict/list/object) ë°©ì–´ì ìœ¼ë¡œ íŒŒì‹±í•œë‹¤.

    í˜•ì‹(ê°€ë…ì„± ìš°ì„ , KISS):
    - "ì‚¬ìš©ì/ìºë¦­í„°/ì‹œìŠ¤í…œ" ë¼ë²¨ì„ ë¶™ì—¬ í…ìŠ¤íŠ¸ í˜•íƒœë¡œ ì§ë ¬í™”í•œë‹¤.
    - ê³¼ë„í•œ í† í° ì‚¬ìš©ì„ ë§‰ê¸° ìœ„í•´ max_items/max_charsë¡œ ì œí•œí•œë‹¤.
    """
    try:
        if not history or not isinstance(history, list):
            return ""

        # ìµœì‹  Nê°œë§Œ ê³ ë ¤ (í˜¸ì¶œìê°€ ì´ë¯¸ ì˜ë¼ì„œ ì£¼ë”ë¼ë„ ì´ì¤‘ ë°©ì–´)
        items = history[-max_items:] if len(history) > max_items else history

        def _extract_role_and_text(item: object) -> tuple[str, str]:
            # dict í˜•íƒœ: {"role": "...", "parts": [text] } / {"role": "...", "content": "..."}
            if isinstance(item, dict):
                role = str(item.get("role") or "").strip().lower()
                parts = item.get("parts")
                if isinstance(parts, list) and parts:
                    txt = _as_text(parts[0]).strip()
                else:
                    txt = _as_text(item.get("content")).strip()
                return role, txt

            # ê°ì²´ í˜•íƒœ: .role / .content
            role = ""
            txt = ""
            try:
                role = str(getattr(item, "role", "") or "").strip().lower()
            except Exception:
                role = ""
            try:
                txt = _as_text(getattr(item, "content", "")).strip()
            except Exception:
                txt = ""
            # ë§ˆì§€ë§‰ í´ë°±: ë¬¸ìì—´
            if not txt and isinstance(item, str):
                txt = item.strip()
            return role, txt

        lines: list[str] = []
        for it in items:
            role, txt = _extract_role_and_text(it)
            if not txt:
                continue
            # ì§€ë‚˜ì¹˜ê²Œ ê¸´ ê°œë³„ ë©”ì‹œì§€ëŠ” ì˜ë¼ì„œ í¬í•¨(í† í° í­ì£¼ ë°©ì§€)
            if len(txt) > 3000:
                txt = txt[:3000]

            if role in ("user", "human"):
                label = "ì‚¬ìš©ì"
            elif role in ("system",):
                label = "ì‹œìŠ¤í…œ"
            else:
                # model/assistant/character ë“±ì€ ëª¨ë‘ 'ìºë¦­í„°'ë¡œ í†µì¼(ì›ì‘ì±—/ì¼ë°˜ì±— ê³µí†µ)
                label = "ìºë¦­í„°"

            lines.append(f"{label}: {txt}")

        if not lines:
            return ""

        # max_chars ë°©ì–´: ë’¤(ìµœì‹ )ë¶€í„° ì±„ì›Œì„œ ì˜ë¼ë‚¸ë‹¤.
        picked: list[str] = []
        total = 0
        for ln in reversed(lines):
            add = len(ln) + (1 if picked else 0)
            if total + add > max_chars:
                break
            picked.append(ln)
            total += add
        picked.reverse()

        if not picked:
            return ""
        return "\n\n[ìµœê·¼ ëŒ€í™”]\n" + "\n".join(picked) + "\n"
    except Exception:
        return ""

 # --- Gemini AI ì„¤ì • ---
genai.configure(api_key=settings.GEMINI_API_KEY)
claude_client = anthropic.AsyncAnthropic(api_key=settings.CLAUDE_API_KEY)
# --- OCR ì œê±°: ê¸°ì¡´ PaddleOCR ê²½ëŸ‰ ì‚¬ìš© êµ¬ê°„ì„ ì™„ì „ ë¹„í™œì„±í™” ---
def _extract_numeric_phrases_ocr_bytes(img_bytes: bytes) -> list[str]:
    # PaddleOCR ì œê±°ë¡œ ë” ì´ìƒ ì‹¤í–‰í•˜ì§€ ì•ŠìŒ
    return []

def _parse_user_intent(user_hint: str) -> dict:
    """ìì—°ì–´ ì…ë ¥ì—ì„œ ê°„ë‹¨í•œ ì˜ë„/í†¤/ì‹œì /ì†ë„ ë“±ì„ íœ´ë¦¬ìŠ¤í‹±ìœ¼ë¡œ ì¶”ì¶œ(ì¶”ê°€ í˜¸ì¶œ ì—†ì´).
    ë°˜í™˜: { intent, stance, tone, pace, continue, remix, constraints, transform_tags }
    """
    hint = (user_hint or "").strip().lower()
    # ê¸°ë³¸ê°’
    intent = None
    stance = None
    tone = None
    pace = None
    want_continue = False
    want_remix = False
    constraints: list[str] = []
    tags: list[str] = []

    # í•œêµ­ì–´ í‚¤ì›Œë“œ(ì†Œë¬¸ì ë³€í™˜ ì „ì œ â†’ í•œê¸€ì—” ì˜í–¥ ì—†ìŒ)
    def _has(*keys: str) -> bool:
        return any(k in user_hint for k in keys)

    # intent
    if _has("ì—°ì• ", "ì‚¬ë‘", "ë°ì´íŠ¸", "ì¸"):
        intent = "romance"
        tone = tone or "ì„¤ë ˜/ì„œì •"
    if _has("ë³µìˆ˜", "ì‘ì§•", "í†µìˆ˜"):
        intent = intent or "revenge"
    if _has("ìŠ¤ë¦´ëŸ¬", "ê³µí¬", "í˜¸ëŸ¬", "ë¯¸ìŠ¤í„°ë¦¬", "ì¶”ë¦¬", "ëŠì™€ë¥´"):
        intent = intent or "thriller"

    # stance
    if _has("1ì¸ì¹­", "ì¼ì¸ì¹­", "ë‚˜ë¡œ"):
        stance = "first"
    if _has("3ì¸ì¹­", "ì‚¼ì¸ì¹­", "ê·¸ë…€", "ê·¸ë¡œ"):
        stance = stance or "third"

    # tone
    if _has("ì”ì”", "ë”°ëœ»", "íë§"):
        tone = tone or "ì”ì”/ë”°ëœ»"
    if _has("í›„í‚¹", "ëª°ì…", "ìê·¹"):
        tone = tone or "í›„í‚¹/ê°•ë ¬"

    # pace
    if _has("ë¹ ë¥´ê²Œ", "ì†ë„ê°", "í…œí¬ ë¹ "):
        pace = "fast"
    if _has("ì²œì²œíˆ", "ëŠë¦¬ê²Œ"):
        pace = pace or "slow"

    # control flags
    if _has("ì´ì–´ì¤˜", "ì´ì–´ ì¨", "ê³„ì† ì¨"):
        want_continue = True
    if _has("ë°”ê¿”ì¤˜", "ë‹¤ë¥´ê²Œ", "ëŠë‚Œìœ¼ë¡œ ë°”ê¿”"):
        want_remix = True

    # transform tags(UI íƒœê·¸ì™€ ì ‘ì )
    if _has("ë¡œë§¨ìŠ¤"):
        tags.append("ë¡œë§¨ìŠ¤")
    if _has("ì”ì”"):
        tags.append("ì”ì”í•˜ê²Œ")
    if _has("ìœ„íŠ¸", "ë°ˆ"):
        tags.append("ë°ˆìŠ¤ëŸ½ê²Œ")
    if stance == "first":
        tags.append("1ì¸ì¹­ì‹œì ")
    if stance == "third":
        tags.append("3ì¸ì¹­ì‹œì ")

    # constraints
    if _has("íšŒì‚¬", "ì§ì¥", "ìƒì‚¬"):
        constraints.append("ì‹¤ëª…/íšŒì‚¬ëª…/ì§í•¨ ê¸ˆì§€")

    return {
        "intent": intent,
        "stance": stance,
        "tone": tone,
        "pace": pace,
        "continue": want_continue,
        "remix": want_remix,
        "constraints": constraints,
        "transform_tags": tags,
    }

# (í”„ë¦¬ì›Œë° ë¡¤ë°±) ì—…ë¡œë“œ í”„ë¦¬ì›Œë° ìœ í‹¸ ì œê±°


# OpenAI ì„¤ì •
from openai import AsyncOpenAI
import openai
client = AsyncOpenAI(api_key=settings.OPENAI_API_KEY)


# -------------------------------
# Vision-grounded helpers (Gemini)
# -------------------------------
# Gemini ì•ˆì „ ì„¤ì •(ì°¨ë‹¨ ì™„í™”)
DEFAULT_SAFETY_OPEN = [
    {"category": "HARM_CATEGORY_DANGEROUS_CONTENT", "threshold": "BLOCK_NONE"},
    {"category": "HARM_CATEGORY_HARASSMENT", "threshold": "BLOCK_NONE"},
    {"category": "HARM_CATEGORY_HATE_SPEECH", "threshold": "BLOCK_NONE"},
    {"category": "HARM_CATEGORY_SEXUAL_CONTENT", "threshold": "BLOCK_NONE"},
    {"category": "HARM_CATEGORY_VIOLENCE", "threshold": "BLOCK_NONE"},
]

async def tag_image_keywords(image_url: str, model: str = 'claude') -> dict:
    """
    ê°•í™”ëœ ì´ë¯¸ì§€ íƒœê¹…: Claude Vision ìš°ì„  ì‚¬ìš©ìœ¼ë¡œ ë” ì •í™•í•œ ë¶„ì„
    """
    try:
        import requests
        import base64
        import json
        
        # ì´ë¯¸ì§€ ë‹¤ìš´ë¡œë“œ ë° base64 ì¸ì½”ë”© + MIME íƒì§€
        response = requests.get(image_url, timeout=10)
        img_bytes = response.content

        # --- pHash ìºì‹œ ì¡°íšŒ(ê²½ëŸ‰ average hash) ---
        try:
            from app.core.database import redis_client as _redis
            def _avg_hash(bytes_data: bytes, hash_size: int = 8) -> str:
                img = Image.open(BytesIO(bytes_data)).convert('L').resize((hash_size, hash_size), Image.BILINEAR)
                pixels = list(img.getdata())
                avg = sum(pixels) / len(pixels)
                bits = ''.join('1' if p > avg else '0' for p in pixels)
                return hex(int(bits, 2))[2:].rjust((hash_size*hash_size)//4, '0')
            ahash = _avg_hash(img_bytes)
            cache_key = f"vision:ahash:{ahash}:tags"
            # URL ê¸°ë°˜ í‚¤(ì¿¼ë¦¬ ì œê±°)
            cache_key_url = None
            try:
                p = urlparse(image_url)
                url_no_q = urlunparse((p.scheme, p.netloc, p.path, '', '', ''))
                cache_key_url = f"vision:url:{url_no_q}:tags"
                cached_url = await _redis.get(cache_key_url)
                if cached_url:
                    try:
                        txt = cached_url.decode('utf-8') if isinstance(cached_url, (bytes, bytearray)) else str(cached_url)
                        data = json.loads(txt)
                        if isinstance(data, dict):
                            logging.info("Vision tags cache hit")
                            return data
                    except Exception:
                        pass
            except Exception:
                pass
            cached = await _redis.get(cache_key)
            if cached:
                try:
                    txt = cached.decode('utf-8') if isinstance(cached, (bytes, bytearray)) else str(cached)
                    data = json.loads(txt)
                    if isinstance(data, dict):
                        logging.info("Vision tags cache hit")
                        return data
                except Exception:
                    pass
        except Exception:
            ahash = None
            cache_key_url = None
        image_data = base64.b64encode(img_bytes).decode('utf-8')
        # ìš°ì„ ìˆœìœ„: ì‘ë‹µ í—¤ë” â†’ ë°”ì´íŠ¸ ì‹œê·¸ë‹ˆì²˜ â†’ ê¸°ë³¸ê°’
        ct = (response.headers.get('Content-Type') or '').lower()
        if ct.startswith('image/'):
            image_mime = ct.split(';')[0].strip()
        else:
            kind = imghdr.what(None, h=img_bytes)
            mime_map = {
                'jpeg': 'image/jpeg', 'jpg': 'image/jpeg', 'png': 'image/png',
                'gif': 'image/gif', 'webp': 'image/webp', 'bmp': 'image/bmp'
            }
            image_mime = mime_map.get(kind, 'image/jpeg')
        
        prompt = (
            "ì´ë¯¸ì§€ë¥¼ ë§¤ìš° ìì„¸íˆ ë¶„ì„í•´ì„œ ìŠ¤í† ë¦¬í…”ë§ì— í•„ìš”í•œ ëª¨ë“  ì •ë³´ë¥¼ ì¶”ì¶œí•˜ì„¸ìš”.\n"
            "JSON í˜•ì‹ìœ¼ë¡œë§Œ ì‘ë‹µ:\n"
            "{\n"
            "  \"place\": \"êµ¬ì²´ì ì¸ ì¥ì†Œ (ì˜ˆ: ë¶ë¹„ëŠ” ì¹´í˜ í…Œë¼ìŠ¤, í™©ëŸ‰í•œ ì‚¬ë§‰ ë„ë¡œ)\",\n"
            "  \"objects\": [\"ëˆˆì— ë„ëŠ” ëª¨ë“  ì‚¬ë¬¼ë“¤\"],\n"
            "  \"lighting\": \"ì¡°ëª… ìƒíƒœì™€ ì‹œê°„ëŒ€\",\n"
            "  \"weather\": \"ë‚ ì”¨ë‚˜ ê³„ì ˆê°\",\n"
            "  \"mood\": \"ì „ì²´ì ì¸ ë¶„ìœ„ê¸°\",\n"
            "  \"colors\": [\"ì£¼ìš” ìƒ‰ìƒë“¤\"],\n"
            "  \"textures\": [\"ì§ˆê°, ì¬ì§ˆ\"],\n"
            "  \"sounds_implied\": [\"ì•”ì‹œë˜ëŠ” ì†Œë¦¬ë“¤\"],\n"
            "  \"smells_implied\": [\"ì•”ì‹œë˜ëŠ” ëƒ„ìƒˆë“¤\"],\n"
            "  \"temperature\": \"ì²´ê° ì˜¨ë„\",\n"
            "  \"movement\": \"ì›€ì§ì„ì´ë‚˜ ë™ì  ìš”ì†Œ\",\n"
            "  \"focal_point\": \"ì‹œì„ ì´ ì§‘ì¤‘ë˜ëŠ” ê³³\",\n"
            "  \"story_hooks\": [\"ìŠ¤í† ë¦¬ ì „ê°œ ê°€ëŠ¥í•œ ìš”ì†Œë“¤\"],\n"
            "  \"in_image_text\": [\"ì´ë¯¸ì§€ ì•ˆì— ë³´ì´ëŠ” ëª¨ë“  í…ìŠ¤íŠ¸ë¥¼ ì›ë¬¸ ê·¸ëŒ€ë¡œ(ì˜¤íƒˆì í¬í•¨)\"],\n"
            "  \"numeric_phrases\": [\"ìˆ«ì+ë‹¨ìœ„ê°€ í•¨ê»˜ ìˆëŠ” ë¬¸êµ¬(ì˜ˆ: '500í‚¤ë¡œ', '500ì›')\"]\n"
            "}"
        )
        
        # Claude Vision ì‹œë„
        if model == 'claude':
            try:
                txt = await get_claude_completion(
                    prompt,
                    max_tokens=1800,
                    model=CLAUDE_MODEL_PRIMARY,
                    image_base64=image_data,
                    image_mime=image_mime
                )
                
                # JSON ì¶”ì¶œ
                if '```json' in txt:
                    txt = txt.split('```json')[1].split('```')[0].strip()
                elif '```' in txt:
                    txt = txt.split('```')[1].split('```')[0].strip()
                    
                data = json.loads(txt)
                if isinstance(data, dict):
                    logging.info("Claude Vision tagging successful")
                    # ìºì‹œ ì €ì¥
                    try:
                        if cache_key_url:
                            await _redis.setex(cache_key_url, 86400, json.dumps(data, ensure_ascii=False))
                        if ahash:
                            await _redis.setex(cache_key, 86400, json.dumps(data, ensure_ascii=False))
                    except Exception:
                        pass
                    return data
            except Exception as e:
                logging.error(f"Claude Vision tagging failed: {e}")
        
        # Gemini í´ë°±
        try:
            import google.generativeai as genai
            import os
            from PIL import Image
            from io import BytesIO
            
            genai.configure(api_key=os.getenv('GEMINI_API_KEY'))
            
            img = Image.open(BytesIO(response.content))
            mm_model = genai.GenerativeModel('gemini-2.5-pro')
            
            response = mm_model.generate_content([prompt, img])
            txt = response.text
            
            if '```json' in txt:
                txt = txt.split('```json')[1].split('```')[0].strip()
            elif '```' in txt:
                txt = txt.split('```')[1].split('```')[0].strip()
                
            data = json.loads(txt)
            if isinstance(data, dict):
                logging.info("Gemini Vision tagging successful")
                try:
                    if cache_key_url:
                        await _redis.setex(cache_key_url, 86400, json.dumps(data, ensure_ascii=False))
                    if ahash:
                        await _redis.setex(cache_key, 86400, json.dumps(data, ensure_ascii=False))
                except Exception:
                    pass
                return data
                
        except Exception as e:
            logging.error(f"Gemini Vision tagging failed: {e}")
            
    except Exception as e:
        logging.error(f"Enhanced image tagging failed: {e}")
        
    # í´ë°±: ê¸°ë³¸ íƒœê¹…
    return {"place": "", "objects": [], "lighting": "", "weather": "", "mood": ""}

async def extract_image_narrative_context(image_url: str, model: str = 'claude') -> dict:
    """
    ì¸ë¬¼/ê´€ê³„/ë¶„ìœ„ê¸°/ì—°ì¶œ ì •ë³´ë¥¼ êµ¬ì¡°í™”í•´ ì¶”ì¶œ.
    subjects: [{role?, age_range?, gender?, attire?, emotion?, pose?}]
    relations: [{a_idx, b_idx, relation, evidence}]
    camera: {angle, distance, lens_hint}
    palette: [keywords]
    genre_cues: [keywords]
    narrative_axes: {desire, conflict, stakes}  # ì•”ì‹œì ì´ë©´ ì§§ê²Œ ì œì•ˆ
    tone: {mood_words, pace}
    """
    try:
        import requests
        import base64
        import json
        
        # ì´ë¯¸ì§€ ë‹¤ìš´ë¡œë“œ ë° base64 ì¸ì½”ë”© + MIME íƒì§€
        response = requests.get(image_url, timeout=10)
        img_bytes = response.content

        # --- pHash ìºì‹œ ì¡°íšŒ(ì»¨í…ìŠ¤íŠ¸) ---
        try:
            from app.core.database import redis_client as _redis
            def _avg_hash(bytes_data: bytes, hash_size: int = 8) -> str:
                img = Image.open(BytesIO(bytes_data)).convert('L').resize((hash_size, hash_size), Image.BILINEAR)
                pixels = list(img.getdata())
                avg = sum(pixels) / len(pixels)
                bits = ''.join('1' if p > avg else '0' for p in pixels)
                return hex(int(bits, 2))[2:].rjust((hash_size*hash_size)//4, '0')
            ahash = _avg_hash(img_bytes)
            cache_key = f"vision:ahash:{ahash}:ctx"
            cached = await _redis.get(cache_key)
            if cached:
                try:
                    txt = cached.decode('utf-8') if isinstance(cached, (bytes, bytearray)) else str(cached)
                    data = json.loads(txt)
                    if isinstance(data, dict):
                        logging.info("Vision ctx cache hit")
                        return data
                except Exception:
                    pass
        except Exception:
            ahash = None
        image_data = base64.b64encode(img_bytes).decode('utf-8')
        ct = (response.headers.get('Content-Type') or '').lower()
        if ct.startswith('image/'):
            image_mime = ct.split(';')[0].strip()
        else:
            kind = imghdr.what(None, h=img_bytes)
            mime_map = {
                'jpeg': 'image/jpeg', 'jpg': 'image/jpeg', 'png': 'image/png',
                'gif': 'image/gif', 'webp': 'image/webp', 'bmp': 'image/bmp'
            }
            image_mime = mime_map.get(kind, 'image/jpeg')
        
        schema_prompt = (
            "ì´ë¯¸ì§€ë¥¼ ë¶„ì„í•´ ì•„ë˜ ìŠ¤í‚¤ë§ˆì˜ JSONìœ¼ë¡œë§Œ ì‘ë‹µí•˜ì„¸ìš”.\n"
            "- ìƒìƒ/ì¶”ì¸¡ ê¸ˆì§€, ë³´ì´ëŠ” ë‹¨ì„œ ìœ„ì£¼. ì•”ì‹œëŠ” narrative_axesì—ì„œ 'hint'ë¡œ ê°„ë‹¨íˆ.\n"
            "- is_selfie: ì…€ì¹´ì¸ì§€ íŒë‹¨ (ê±°ìš¸ ì…€ì¹´, íŒ” ë»—ì–´ ì°ê¸°, ì…€ì¹´ë´‰ ë“± ëª¨ë‘ í¬í•¨)\n"
            "- person_count: ë³´ì´ëŠ” ì¸ë¬¼ ìˆ˜ (0=ì¸ë¬¼ì—†ìŒ)\n"
            "- style_mode: ì¥ë©´ì˜ ìŠ¤íƒ€ì¼ì„ 'snap' ë˜ëŠ” 'genre' ì¤‘ í•˜ë‚˜ë¡œ ì œì•ˆ.\n"
            "- confidence: 0~1 ì‹¤ìˆ˜ë¡œ íŒë‹¨ ì‹ ë¢°ë„. 0.5ëŠ” ì¤‘ë¦½.\n"
            "- cues: íŒë‹¨ì— ì‚¬ìš©í•œ ê·¼ê±° í‚¤ì›Œë“œ ë°°ì—´(ì˜ˆ: selfie, weapon, magic, everyday, cafe ë“±).\n"
            "ìŠ¤í‚¤ë§ˆ: {\n"
            "  subjects:[{role?:string, age_range?:string, gender?:string, attire?:string, emotion?:string, pose?:string}],\n"
            "  relations:[{a_idx:int, b_idx:int, relation:string, evidence:string}],\n"
            "  camera:{angle?:string, distance?:string, lens_hint?:string, is_selfie?:boolean},\n"
            "  palette:[string], genre_cues:[string],\n"
            "  narrative_axes:{desire?:string, conflict?:string, stakes?:string},\n"
            "  tone:{mood_words?:[string], pace?:string},\n"
            "  person_count:int,\n"
            "  style_mode?:string,\n"
            "  confidence?:number,\n"
            "  cues?:[string]\n"
            "}"
        )
        
        # Claude Vision ì‹œë„
        if model == 'claude':
            try:
                txt = await get_claude_completion(
                    schema_prompt,
                    max_tokens=1800,
                    model=CLAUDE_MODEL_PRIMARY,
                    image_base64=image_data,
                    image_mime=image_mime
                )
                
                # JSON ì¶”ì¶œ
                if '```json' in txt:
                    txt = txt.split('```json')[1].split('```')[0].strip()
                elif '```' in txt:
                    txt = txt.split('```')[1].split('```')[0].strip()
                    
                data = json.loads(txt)
                if isinstance(data, dict):
                    logging.info("Claude Vision narrative context successful")
                    try:
                        if ahash:
                            await _redis.setex(cache_key, 86400, json.dumps(data, ensure_ascii=False))
                    except Exception:
                        pass
                    return data
            except Exception as e:
                logging.error(f"Claude Vision narrative context failed: {e}")
        
        # Gemini í´ë°±
        try:
            txt = await get_gemini_completion(schema_prompt + f"\nimage_url: {image_url}", max_tokens=600, model='gemini-2.5-pro')
            data = json.loads(txt)
            if isinstance(data, dict):
                try:
                    if ahash:
                        await _redis.setex(cache_key, 86400, json.dumps(data, ensure_ascii=False))
                except Exception:
                    pass
                return data
        except Exception:
            pass
        return {}
    except Exception:
        return {}

async def analyze_image_tags_and_context(image_url: str, model: str = 'claude') -> tuple[dict, dict]:
    """ë‹¨ì¼ Vision í˜¸ì¶œë¡œ íƒœê·¸(tags)ì™€ ì»¨í…ìŠ¤íŠ¸(context)ë¥¼ ë™ì‹œì— ì¶”ì¶œí•©ë‹ˆë‹¤.
    ì‹¤íŒ¨ ì‹œ í˜¸ì¶œìê°€ í´ë°±ì„ ì‚¬ìš©í•˜ë„ë¡ ì˜ˆì™¸ë¥¼ ë˜ì§‘ë‹ˆë‹¤.
    """
    try:
        # ìºì‹œ íˆíŠ¸
        try:
            key = str(image_url or "").strip()
            if key:
                hit = _VISION_TAGS_CACHE.get(key)
                if hit:
                    ts, tags, ctx = hit
                    if (time.time() - float(ts)) <= _VISION_TAGS_CACHE_TTL_SEC:
                        return tags or {}, ctx or {}
        except Exception:
            pass

        logging.info("Vision combine: start (unified tags+context)")
        import requests, base64, json
        # ì´ë¯¸ì§€ ë‹¤ìš´ë¡œë“œ ë° MIME ì¶”ì •
        resp = requests.get(image_url, timeout=10)
        # âœ… ë°©ì–´: 4xx/5xxë©´ ì¦‰ì‹œ ì‹¤íŒ¨ ì²˜ë¦¬(HTML/ì—ëŸ¬ ë°”ë””ë¥¼ ì´ë¯¸ì§€ë¡œ ì˜¤ì¸ ë°©ì§€)
        resp.raise_for_status()
        img_bytes = resp.content
        ct = (resp.headers.get('Content-Type') or '').lower()
        if ct.startswith('image/'):
            image_mime = ct.split(';')[0].strip()
        else:
            kind = imghdr.what(None, h=img_bytes)
            image_mime = {
                'jpeg': 'image/jpeg', 'jpg': 'image/jpeg', 'png': 'image/png',
                'gif': 'image/gif', 'webp': 'image/webp', 'bmp': 'image/bmp'
            }.get(kind, 'image/jpeg')
            # âœ… ë°©ì–´: content-typeë„ ì´ë¯¸ì§€ê°€ ì•„ë‹ˆê³ , imghdrë„ ëª» ë§ì¶”ë©´ ì´ë¯¸ì§€ê°€ ì•„ë‹Œ ì‘ë‹µìœ¼ë¡œ ê°„ì£¼
            if kind is None:
                raise ValueError(f"image_url is not an image (status={resp.status_code}, ct={ct}, url={image_url})")
        image_b64 = base64.b64encode(img_bytes).decode('utf-8')
        # í†µí•© ìŠ¤í‚¤ë§ˆ í”„ë¡¬í”„íŠ¸(ê±´ì¡°/ì‚¬ì‹¤ ì „ìš©)
        prompt = (
            "ì´ë¯¸ì§€ë¥¼ ì‚¬ì‹¤ì ìœ¼ë¡œë§Œ ê¸°ìˆ í•˜ë¼. ì¶”ì¸¡/ë¹„ìœ /ê°íƒ„ ê¸ˆì§€. ì¥ë¥´/ë¬´ë“œ í˜•ìš©ì‚¬ ê¸ˆì§€(fantasy/noir/surreal/mysterious/cinematic ë“±). ëª¨ë¥´ë©´ 'unknown'.\n"
            "JSON ìœ¼ë¡œë§Œ ì¶œë ¥í•˜ë¼.\n"
            "{\n"
            "  \"tags\": {\n"
            "    \"place\": one_of['cafe','street','park','campus','indoor','home','office','store','beach','mountain','unknown'],\n"
            "    \"objects\": [noun-only strings],\n"
            "    \"lighting\": one_of['daylight','indoor','night','overcast','sunset','unknown'],\n"
            "    \"weather\": one_of['clear','cloudy','rain','snow','unknown'],\n"
            "    \"colors\": [basic color words],\n"
            "    \"textures\": [noun-only],\n"
            "    \"sounds_implied\": [noun-only],\n"
            "    \"smells_implied\": [noun-only],\n"
            "    \"temperature\": one_of['warm','cool','neutral','unknown'],\n"
            "    \"movement\": one_of['still','slight','visible','unknown'],\n"
            "    \"focal_point\": string,\n"
            "    \"story_hooks\": [noun phrases],\n"
            "    \"in_image_text\": [exact text], \"numeric_phrases\": [string]\n"
            "  },\n"
            "  \"context\": {\n"
            "    \"person_count\": number,\n"
            "    \"camera\": {angle:one_of['eye','overhead','low','unknown'], distance:one_of['wide','medium','close','unknown'], is_selfie:boolean},\n"
            "    \"style_mode\": one_of['snap','genre'], \"confidence\": number\n"
            "  }\n"
            "}"
        )
        # âœ… Claude ìš°ì„  í˜¸ì¶œ â†’ ì‹¤íŒ¨ ì‹œ Geminië¡œ í´ë°±
        #
        # ë°°ê²½:
        # - ìš´ì˜/ë¡œì»¬ í™˜ê²½ì— ë”°ë¼ Claude í‚¤/ê¶Œí•œ ë¬¸ì œê°€ ìˆìœ¼ë©´ Visionì´ í•­ìƒ ì‹¤íŒ¨í•˜ë©°,
        #   ì´ ê²½ìš° ìºë¦­í„° ìë™ìƒì„±ì´ ì´ë¯¸ì§€ì™€ ë¬´ê´€í•œ "í´ë°±"ìœ¼ë¡œ ë–¨ì–´ì§„ë‹¤.
        # - ì´ë¯¸ì§€ëŠ” ì„œë¹„ìŠ¤ í•µì‹¬ì´ë¯€ë¡œ, Gemini Visionìœ¼ë¡œ 2ì°¨ í´ë°±ì„ ì œê³µí•´ ê°€ìš©ì„±ì„ í™•ë³´í•œë‹¤.
        data = None
        provider = "unknown"
        try:
            txt = await get_claude_completion(
                prompt,
                temperature=0.1,
                max_tokens=1000,
                model=CLAUDE_MODEL_PRIMARY,
                image_base64=image_b64,
                image_mime=image_mime
            )
            if '```json' in txt:
                txt = txt.split('```json')[1].split('```')[0].strip()
            elif '```' in txt:
                txt = txt.split('```')[1].split('```')[0].strip()
            parsed = json.loads(txt)
            if isinstance(parsed, dict):
                data = parsed
                provider = "claude"
        except Exception as e:
            try:
                logging.warning(f"Vision combine: Claude failed -> fallback to Gemini ({e})")
            except Exception:
                pass

        if data is None:
            try:
                from PIL import Image
                from io import BytesIO
                import google.generativeai as genai

                img = Image.open(BytesIO(img_bytes))
                # ëª¨ë¸ íŒíŠ¸ê°€ ë“¤ì–´ì™€ë„ ì•ˆì „í•˜ê²Œ ê¸°ë³¸ê°’ ì‚¬ìš©
                gm = genai.GenerativeModel('gemini-2.5-pro')
                generation_config = genai.types.GenerationConfig(
                    temperature=0.1,
                    max_output_tokens=900,
                )
                resp2 = await gm.generate_content_async([prompt, img], generation_config=generation_config)
                txt2 = ""
                try:
                    txt2 = resp2.text or ""
                except Exception:
                    txt2 = ""
                if '```json' in txt2:
                    txt2 = txt2.split('```json')[1].split('```')[0].strip()
                elif '```' in txt2:
                    txt2 = txt2.split('```')[1].split('```')[0].strip()
                parsed2 = json.loads(txt2) if txt2 else {}
                if isinstance(parsed2, dict):
                    data = parsed2
                    provider = "gemini"
            except Exception as e:
                try:
                    logging.error(f"Vision combine: Gemini fallback failed: {e}")
                except Exception:
                    pass
                data = None

        if not isinstance(data, dict):
            raise ValueError("combined response is not dict")

        try:
            logging.info(f"Vision combine: success (provider={provider})")
        except Exception:
            pass

        tags_out = (data.get('tags') or {}) if isinstance(data.get('tags') or {}, dict) else {}
        ctx_out = (data.get('context') or {}) if isinstance(data.get('context') or {}, dict) else {}
        # ìºì‹œ ì €ì¥(ê°„ë‹¨ LRU: ì´ˆê³¼ ì‹œ ì„ì˜ 1ê°œ ì œê±°)
        try:
            key = str(image_url or "").strip()
            if key:
                if len(_VISION_TAGS_CACHE) >= _VISION_TAGS_CACHE_MAX:
                    try:
                        _VISION_TAGS_CACHE.pop(next(iter(_VISION_TAGS_CACHE)))
                    except Exception:
                        _VISION_TAGS_CACHE.clear()
                _VISION_TAGS_CACHE[key] = (time.time(), tags_out, ctx_out)
        except Exception:
            pass
        return tags_out, ctx_out
    except Exception:
        # í˜¸ì¶œì í´ë°±
        raise

def build_image_grounding_block(tags: dict, pov: str | None = None, style_prompt: str | None = None, ctx: dict | None = None, username: str | None = None, story_mode: str | None = None, user_hint: str = "") -> str:
    # ì‹œì  ìë™ ê²°ì • ë¡œì§
    if ctx and not pov:
        # SNAP ëª¨ë“œ: ëª¨ë“  ì‚¬ì§„ì€ ìœ ì € ë³¸ì¸ì˜ ê²½í—˜/ìˆœê°„ â†’ ë¬´ì¡°ê±´ 1ì¸ì¹­
        if story_mode == "snap":
            # ì—°ì• /ë¡œë§¨ìŠ¤ í‚¤ì›Œë“œ ì ìˆ˜í™” ì‹œìŠ¤í…œ (ì •ì œ + ê°€ì¤‘ì¹˜ ì°¨ë“±í™”)
            keyword_scores = {
                # í™•ì‹¤í•œ ë¡œë§¨ìŠ¤ ì˜ë„ - 2ì 
                "ì—°ì• ": 2, "ë°ì´íŠ¸": 2, "ì¢‹ì•„í•´": 2, "ì‚¬ë‘": 2, "ê³ ë°±": 2,
                "ì²«í‚¤ìŠ¤": 2, "í‚¤ìŠ¤": 2, "í¬ì˜¹": 2, "ì•ˆì•„": 2, "ìŠ¤í‚¨ì‹­": 2,
                "ë¡œë§¨í‹±": 2, "ë¡œë§¨ìŠ¤": 2,
                
                # ê°•í•œ ë¡œë§¨ìŠ¤/ì„±ì  í‘œí˜„ - 2ì 
                "ì•¼í•œ": 2, "ì„¹ì‹œ": 2, "ê´€ëŠ¥": 2, "ìœ í˜¹": 2, "ë°€ë‹¹": 2, "ì¸": 2, "ë‹¬ë‹¬": 2,
                "ì¹¨ëŒ€": 2, "ìˆ¨ì†Œë¦¬": 2, "ì²´ì˜¨": 2, "ì†ì‚­": 2,
                
                # ì„œë¸Œì»¬ì³ ë¡œë§¨ìŠ¤ - 1ì 
                "ì™€ì´í”„": 1, "í—ˆë‹ˆ": 1, "ì¸¤ë°ë ˆ": 1, "ì–€ë°ë ˆ": 1, "ë°ë ˆ": 1,
                
                # ì—¬ì„±í–¥ - 1ì 
                "ë‚¨ì£¼": 1, "ì§‘ì°©": 1, "ì†Œìœ ìš•": 1,
                
                # ë‚¨ì„±í–¥ - 1ì 
                "íˆë¡œì¸": 1, "ì—¬ì£¼": 1, "ê³µëµ": 1,
                
                # ì•½í•œ ë¡œë§¨ìŠ¤ ì•”ì‹œ - 0.5ì  (ë‹¨ë…ìœ¼ë¡œëŠ” ë¶ˆì¶©ë¶„)
                "ì„¤ë ˆ": 0.5, "ì†ì¡": 0.5, "ëª¨ì—": 0.5,
                "ì€ë°€": 0.5,
            }
            
            # ë³µí•© í‘œí˜„ (ë¬¸ë§¥ í¬í•¨)
            compound_expressions = {
                # ë™ì‚¬í˜• ë³µí•© í‘œí˜„ - 2ì 
                "ì—°ì• í•˜ê³ ": 2, "ì—°ì• í•˜ëŠ”": 2, "ë°ì´íŠ¸í•˜ê³ ": 2, "ë°ì´íŠ¸í•˜ëŠ”": 2,
                "ì‚¬ë‘í•˜ê³ ": 2, "ì‚¬ë‘í•˜ëŠ”": 2, "ì¢‹ì•„í•˜ê³ ": 2, "ì¢‹ì•„í•˜ëŠ”": 2,
                
                # ê´€ê³„ í‚¤ì›Œë“œ (í™•ì‹¤í•œ ë¡œë§¨ìŠ¤) - 2ì 
                "ì—¬ìì¹œêµ¬": 2, "ì—¬ì¹œ": 2, "ë‚¨ìì¹œêµ¬": 2, "ë‚¨ì¹œ": 2,
                "ì• ì¸": 2, "ì—°ì¸": 2,
                
                # êµ¬ì–´ì²´ ì§€ì¹­ - 1.5ì 
                "ì–˜ë‘": 1.5, "ìŸ¤ë‘": 1.5, "ì € ì‚¬ëŒì´ë‘": 1.5,
                "ì´ ì‚¬ëŒì´ë‘": 1.5, "ì´ ì‚¬ëŒê³¼": 1.5, "ì´ ì—¬ìë‘": 1.5, "ì´ ë‚¨ìë‘": 1.5,
                "ê·¸ë…€ì™€": 1.5, "ê·¸ì™€": 1.5, "ê·¸ë…€ë‘": 1.5, "ê·¸ë‘": 1.5,
                
                # ë™ë°˜ í‘œí˜„ - 2ì  (ì´ë¯¸ì§€ ë¬¸ë§¥ì—ì„œëŠ” ê°•í•œ ë¡œë§¨ìŠ¤ ì‹ í˜¸)
                "ê°™ì´": 2, "í•¨ê»˜": 2,
            }
            
            # ìê¸° ì²´í—˜ í‚¤ì›Œë“œ (ì´ê²Œ ìˆìœ¼ë©´ ë¡œë§¨ìŠ¤ ì ìˆ˜ ë¬´ì‹œ)
            self_keywords = [
                "ë‚´ê°€ ì´ë ‡ê²Œ", "ë‚˜ë„ ì´ëŸ°", "ì´ëŸ° ëŠë‚Œ", "ì´ëŸ° ìˆœê°„",
                "ë‚˜ì˜€ìœ¼ë©´", "ë‚˜ë¼ë©´", "ë‚´ ì…ì¥", "ë‚˜í•œí…Œë„", "ë‚´ ëª¨ìŠµ"
            ]
            
            # ì ìˆ˜ ê³„ì‚°
            hint_lower = user_hint.lower()
            romance_score = 0.0
            
            # ë³µí•© í‘œí˜„ ë¨¼ì € ì²´í¬ (ìš°ì„ ìˆœìœ„ ë†’ìŒ)
            for expr, score in compound_expressions.items():
                if expr in hint_lower:
                    romance_score += score
            
            # ë‹¨ì¼ í‚¤ì›Œë“œ ì²´í¬
            for keyword, score in keyword_scores.items():
                if keyword in hint_lower:
                    romance_score += score
            
            has_self = any(kw in user_hint for kw in self_keywords)
            
            # 1.5ì  ì´ìƒì´ê³ , ìê¸° ì²´í—˜ í‚¤ì›Œë“œê°€ ì—†ìœ¼ë©´ ë¡œë§¨ìŠ¤ ëª¨ë“œ
            if romance_score >= 1.5 and not has_self:
                pov = "1ì¸ì¹­ 'ë‚˜'(ìœ ì €). ì´ë¯¸ì§€ ì† ì¸ë¬¼ì€ 'ê·¸ë…€/ê·¸'ë¡œ ì§€ì¹­í•˜ê³ , ìœ ì €ì™€ì˜ ë¡œë§¨í‹±í•œ ìƒí˜¸ì‘ìš©ì„ ì¤‘ì‹¬ìœ¼ë¡œ ì„œìˆ ."
            else:
                # ê¸°ë³¸: ì´ë¯¸ì§€ ì† ì¸ë¬¼ = ë‚˜
                pov = "1ì¸ì¹­ 'ë‚˜'"
        else:
            # GENRE ëª¨ë“œ: ë¡œë§¨ìŠ¤ ì¥ë¥´ëŠ” í•­ìƒ 1ì¸ì¹­
            person_count = ctx.get('person_count', 0)
            camera = ctx.get('camera', {})
            is_selfie = camera.get('is_selfie', False)
            
            is_romance = False
            if user_hint:
                hint_lower = user_hint.lower()
                romance_score = 0.0
                
                # ë³µí•© í‘œí˜„ ì²´í¬
                compound_expressions = {
                    "ì—°ì• í•˜ê³ ": 2, "ì—°ì• í•˜ëŠ”": 2, "ë°ì´íŠ¸í•˜ê³ ": 2, "ë°ì´íŠ¸í•˜ëŠ”": 2,
                    "ì‚¬ë‘í•˜ê³ ": 2, "ì‚¬ë‘í•˜ëŠ”": 2, "ì¢‹ì•„í•˜ê³ ": 2, "ì¢‹ì•„í•˜ëŠ”": 2,
                    "ì—¬ìì¹œêµ¬": 2, "ì—¬ì¹œ": 2, "ë‚¨ìì¹œêµ¬": 2, "ë‚¨ì¹œ": 2,
                    "ì• ì¸": 2, "ì—°ì¸": 2,
                    "ì–˜ë‘": 1.5, "ìŸ¤ë‘": 1.5, "ì € ì‚¬ëŒì´ë‘": 1.5,
                    "ì´ ì‚¬ëŒì´ë‘": 1.5, "ì´ ì‚¬ëŒê³¼": 1.5, "ì´ ì—¬ìë‘": 1.5, "ì´ ë‚¨ìë‘": 1.5,
                    "ê·¸ë…€ì™€": 1.5, "ê·¸ì™€": 1.5, "ê·¸ë…€ë‘": 1.5, "ê·¸ë‘": 1.5,
                    "ê°™ì´": 2, "í•¨ê»˜": 2,
                }
                
                for expr, score in compound_expressions.items():
                    if expr in hint_lower:
                        romance_score += score
                
                # ë‹¨ì¼ í‚¤ì›Œë“œ ì²´í¬
                keyword_scores = {
                    "ì—°ì• ": 2, "ë°ì´íŠ¸": 2, "ì¢‹ì•„í•´": 2, "ì‚¬ë‘": 2, "ê³ ë°±": 2,
                    "ì²«í‚¤ìŠ¤": 2, "í‚¤ìŠ¤": 2, "í¬ì˜¹": 2, "ì•ˆì•„": 2, "ìŠ¤í‚¨ì‹­": 2,
                    "ë¡œë§¨í‹±": 2, "ë¡œë§¨ìŠ¤": 2,
                    "ì•¼í•œ": 2, "ì„¹ì‹œ": 2, "ê´€ëŠ¥": 2, "ìœ í˜¹": 2, "ë°€ë‹¹": 2, "ì¸": 2, "ë‹¬ë‹¬": 2,
                    "ì¹¨ëŒ€": 2, "ìˆ¨ì†Œë¦¬": 2, "ì²´ì˜¨": 2, "ì†ì‚­": 2,
                    "ì™€ì´í”„": 1, "í—ˆë‹ˆ": 1, "ì¸¤ë°ë ˆ": 1, "ì–€ë°ë ˆ": 1, "ë°ë ˆ": 1,
                    "ë‚¨ì£¼": 1, "ì§‘ì°©": 1, "ì†Œìœ ìš•": 1,
                    "íˆë¡œì¸": 1, "ì—¬ì£¼": 1, "ê³µëµ": 1,
                    "ì„¤ë ˆ": 0.5, "ì†ì¡": 0.5, "ëª¨ì—": 0.5, "ì€ë°€": 0.5,
                }
                
                for keyword, score in keyword_scores.items():
                    if keyword in hint_lower:
                        romance_score += score
                
                # ìê¸° ì²´í—˜ í‚¤ì›Œë“œ ì²´í¬
                self_keywords = [
                    "ë‚´ê°€ ì´ë ‡ê²Œ", "ë‚˜ë„ ì´ëŸ°", "ì´ëŸ° ëŠë‚Œ", "ì´ëŸ° ìˆœê°„",
                    "ë‚˜ì˜€ìœ¼ë©´", "ë‚˜ë¼ë©´", "ë‚´ ì…ì¥", "ë‚˜í•œí…Œë„", "ë‚´ ëª¨ìŠµ"
                ]
                has_self = any(kw in user_hint for kw in self_keywords)
                
                # 1.5ì  ì´ìƒì´ê³ , ìê¸° ì²´í—˜ í‚¤ì›Œë“œê°€ ì—†ìœ¼ë©´ ë¡œë§¨ìŠ¤
                is_romance = romance_score >= 1.5 and not has_self
            
            # âœ… ìš°ì„ ìˆœìœ„ì— ë”°ë¼ ì‹œì  ê²°ì •
            if is_romance:  # âœ… ë¡œë§¨ìŠ¤ê°€ ìµœìš°ì„ !
                pov = "1ì¸ì¹­ 'ë‚˜'(ìœ ì €). ì´ë¯¸ì§€ ì† ì¸ë¬¼ì€ 'ê·¸ë…€/ê·¸'ë¡œ ì§€ì¹­í•˜ê³ , ìœ ì €ì™€ì˜ ë¡œë§¨í‹±í•œ ìƒí˜¸ì‘ìš©ì„ ì¤‘ì‹¬ìœ¼ë¡œ ì„œìˆ ."
            elif person_count == 0:
                pov = "1ì¸ì¹­ 'ë‚˜'"
            elif is_selfie:
                pov = "1ì¸ì¹­ 'ë‚˜'"
            else:
                pov = "3ì¸ì¹­ ê´€ì°°ì"
    
    place = _as_text(tags.get("place")).strip()
    objects = ", ".join([str(x) for x in (tags.get("objects") or []) if str(x).strip()])
    lighting = _as_text(tags.get("lighting")).strip()
    weather = _as_text(tags.get("weather")).strip()
    mood = _as_text(tags.get("mood")).strip()
    
    # ê°•í™”ëœ íƒœê·¸ ì •ë³´
    colors = ", ".join([str(x) for x in (tags.get("colors") or []) if str(x).strip()])
    textures = ", ".join([str(x) for x in (tags.get("textures") or []) if str(x).strip()])
    sounds = ", ".join([str(x) for x in (tags.get("sounds_implied") or []) if str(x).strip()])
    smells = ", ".join([str(x) for x in (tags.get("smells_implied") or []) if str(x).strip()])
    temperature = _as_text(tags.get("temperature")).strip()
    movement = _as_text(tags.get("movement")).strip()
    focal_point = _as_text(tags.get("focal_point")).strip()
    story_hooks = tags.get("story_hooks") or []
    
    # ì´ë¯¸ì§€ ë‚´ í…ìŠ¤íŠ¸(ìµœìš°ì„  ì‚¬ì‹¤)
    in_texts = [str(x) for x in (tags.get("in_image_text") or []) if str(x).strip()]
    numeric_phrases = [str(x) for x in (tags.get("numeric_phrases") or []) if str(x).strip()]
    
    # ğŸ†• "unknown" í•„í„°ë§ í—¬í¼
    def _valid(val: str) -> bool:
        return val and val.lower() != "unknown"

    lines = [
        "[ê³ ì • ì¡°ê±´ - ì´ë¯¸ì§€ ê·¸ë¼ìš´ë”©]",
        ("[ìµœìš°ì„  ì‚¬ì‹¤ - ì´ë¯¸ì§€ ë‚´ í…ìŠ¤íŠ¸] " + "; ".join(in_texts)) if in_texts else None,
        ("[ìˆ˜ì¹˜/ë‹¨ìœ„ ë¬¸êµ¬] " + "; ".join(numeric_phrases)) if numeric_phrases else None,
        f"ì¥ì†Œ: {place}" if _valid(place) else None,
        f"ì˜¤ë¸Œì íŠ¸: {objects}" if objects else None,
        f"ì¡°ëª…/ì‹œê°„ëŒ€: {lighting}" if _valid(lighting) else None,
        f"ë‚ ì”¨: {weather}" if _valid(weather) else None,
        f"ë¬´ë“œ: {mood}" if _valid(mood) else None,
        f"ì£¼ìš” ìƒ‰ìƒ: {colors}" if colors else None,
        f"ì§ˆê°/ì¬ì§ˆ: {textures}" if textures else None,
        f"ì•”ì‹œë˜ëŠ” ì†Œë¦¬: {sounds}" if sounds else None,
        f"ì•”ì‹œë˜ëŠ” ëƒ„ìƒˆ: {smells}" if smells else None,
        f"ì²´ê° ì˜¨ë„: {temperature}" if _valid(temperature) else None,
        f"ì›€ì§ì„/ë™ì  ìš”ì†Œ: {movement}" if _valid(movement) else None,
        f"ì‹œì„  ì§‘ì¤‘ì : {focal_point}" if focal_point else None,
        "",
        "ê·œì¹™: ì´ë¯¸ì§€ì— í¬í•¨ëœ í…ìŠ¤íŠ¸(ìœ„ ìµœìš°ì„  ì‚¬ì‹¤)ë¥¼ 1ìˆœìœ„ë¡œ ë°˜ì˜í•˜ë¼. ìˆ«ì/ë‹¨ìœ„ë¥¼ ì ˆëŒ€ ì™œê³¡í•˜ì§€ ë§ë¼.",
        "ê·œì¹™: ìœ„ ëª¨ë“  ìš”ì†Œë“¤ì„ ìì—°ìŠ¤ëŸ½ê²Œ ë…¹ì—¬ë‚´ì–´ ìƒìƒí•œ ì¥ë©´ì„ ë§Œë“¤ì–´ë¼.",
        "ê·œì¹™: ì˜¤ê°ì„ í™œìš©í•´ ë…ìê°€ ê·¸ ê³µê°„ì— ìˆëŠ” ë“¯í•œ ëª°ì…ê°ì„ ì œê³µí•˜ë¼.",
        "ê·œì¹™: ì´ë¯¸ì§€ì— ì¡´ì¬í•˜ì§€ ì•ŠëŠ” ìš”ì†Œë¥¼ ì¶”ê°€í•˜ì§€ ë§ë¼.",
        "ê·œì¹™: ë©”íƒ€ë°œì–¸ ê¸ˆì§€. show-don't-tell. ì¸ë¬¼ì˜ í–‰ë™ê³¼ ëŒ€ì‚¬ë¡œ í‘œí˜„í•˜ë¼.",
    ]
    
    # ìŠ¤í† ë¦¬ í›… ì¶”ê°€
    if story_hooks:
        lines.append("")
        lines.append("ìŠ¤í† ë¦¬ ì „ê°œ ê°€ëŠ¥ ìš”ì†Œ:")
        for hook in story_hooks[:3]:  # ìµœëŒ€ 3ê°œë§Œ
            lines.append(f"- {hook}")
    # ì¶”ê°€ ë§¥ë½(ì¸ë¬¼/ê´€ê³„/ì—°ì¶œ)
    if isinstance(ctx, dict) and ctx:
        subs = ctx.get("subjects") or []
        if subs:
            sub_strs = []
            for i, s in enumerate(subs):
                desc = ", ".join([
                    str(s.get("role")) if s.get("role") else "",
                    str(s.get("age_range")) if s.get("age_range") else "",
                    str(s.get("gender")) if s.get("gender") else "",
                    str(s.get("attire")) if s.get("attire") else "",
                    str(s.get("emotion")) if s.get("emotion") else "",
                    str(s.get("pose")) if s.get("pose") else "",
                ])
                sub_strs.append(f"#{i}: {desc}")
            lines.append("ì¸ë¬¼ ë‹¨ì„œ: " + "; ".join([x for x in sub_strs if x.strip()]))
        rels = ctx.get("relations") or []
        if rels:
            rel_strs = []
            for r in rels:
                rel_strs.append(f"{r.get('a_idx')}â†”{r.get('b_idx')}: {r.get('relation')} ({r.get('evidence')})")
            lines.append("ê´€ê³„ ë‹¨ì„œ: " + "; ".join(rel_strs))
        cam = ctx.get("camera") or {}
        cam_line = ", ".join([x for x in [cam.get("angle"), cam.get("distance"), cam.get("lens_hint")] if x])
        if cam_line:
            lines.append("ì—°ì¶œ: " + cam_line)
        pal = ctx.get("palette") or []
        if pal:
            lines.append("ìƒ‰ì¡°: " + ", ".join([str(x) for x in pal]))
        genres = ctx.get("genre_cues") or []
        if genres:
            lines.append("ì¥ë¥´ ë‹¨ì„œ: " + ", ".join([str(x) for x in genres]))
        axes = ctx.get("narrative_axes") or {}
        axes_line = ", ".join([f"ìš•êµ¬:{axes.get('desire')}" if axes.get('desire') else "", f"ê°ˆë“±:{axes.get('conflict')}" if axes.get('conflict') else "", f"ìœ„í—˜:{axes.get('stakes')}" if axes.get('stakes') else ""]).strip(', ')
        if axes_line:
            lines.append("ì„œì‚¬ ì¶•(íŒíŠ¸): " + axes_line)
    if pov:
        # 1ì¸ì¹­ ì‹œì ì¼ ë•Œ username ì‚¬ìš©
        if "1ì¸ì¹­" in pov and username:
            lines.append(f"ì‹œì : 1ì¸ì¹­ 'ë‚˜' (í™”ìì˜ ì´ë¦„: {username})")
            lines.append(f"ê·œì¹™: 1ì¸ì¹­ í™”ì 'ë‚˜'ì˜ ì´ë¦„ì´ {username}ì„ì„ ìì—°ìŠ¤ëŸ½ê²Œ ë“œëŸ¬ë‚´ë¼.")
        else:
            lines.append(f"ì‹œì : {pov} (ìì—°ìŠ¤ëŸ¬ìš´ ë‚´ì /ê·¼ì ‘ ì‹œì )")
    if style_prompt:
        lines.append(f"ë¬¸ì²´: {style_prompt}")
    return "\n".join([ln for ln in lines if ln])

async def generate_image_prompt_from_story(story_text: str, original_tags: dict = None) -> str:
    """ìŠ¤í† ë¦¬ í…ìŠ¤íŠ¸ë¥¼ ë°”íƒ•ìœ¼ë¡œ ì´ë¯¸ì§€ ìƒì„± í”„ë¡¬í”„íŠ¸ë¥¼ ë§Œë“­ë‹ˆë‹¤."""
    try:
        prompt = f"""ë‹¤ìŒ ìŠ¤í† ë¦¬ì˜ í•µì‹¬ ì¥ë©´ì„ í‘œí˜„í•  ì´ë¯¸ì§€ ìƒì„± í”„ë¡¬í”„íŠ¸ë¥¼ ì˜ì–´ë¡œ ì‘ì„±í•˜ì„¸ìš”.

ìŠ¤í† ë¦¬:
{story_text[:800]}

ìš”êµ¬ì‚¬í•­:
- ì˜ì–´ë¡œ ì‘ì„±
- êµ¬ì²´ì ì¸ ì‹œê° ë¬˜ì‚¬
- 50ë‹¨ì–´ ì´ë‚´
- í”„ë¡¬í”„íŠ¸ë§Œ ì¶œë ¥ (ì„¤ëª… ì—†ìŒ)"""

        if original_tags:
            if original_tags.get('palette'):
                prompt += f"\nìƒ‰ê° ì°¸ê³ : {original_tags['palette']}"
            if original_tags.get('mood'):
                prompt += f"\në¶„ìœ„ê¸°: {original_tags['mood']}"

        response = await get_claude_completion(prompt, temperature=0.2)
        return response.strip()[:200]  # ìµœëŒ€ 200ì
    except Exception as e:
        logger.error(f"Failed to generate image prompt: {e}")
        return "A scene from a Korean webnovel, cinematic lighting, emotional atmosphere"

async def write_story_from_image_grounded(image_url: str, user_hint: str = "", pov: str | None = None, style_prompt: str | None = None,
                                          story_mode: str | None = None, username: str | None = None,
                                          model: Literal["gemini","claude","gpt"] = "gemini", sub_model: str | None = "gemini-2.5-pro",
                                          vision_tags: dict | None = None, vision_ctx: dict | None = None) -> str:
    """ì´ë¯¸ì§€ íƒœê¹…â†’ê³ ì •ì¡°ê±´ í”„ë¡¬í”„íŠ¸â†’ì§‘í•„(ìê°€ê²€ì¦ì€ 1íŒ¨ìŠ¤ ë‚´ì¥)"""
    import time
    t0 = time.time()
    
    # Stage-1 lightweight grounding (fallback-friendly)
    kw2, caption = stage1_keywords_from_image_url(image_url)
    t1 = time.time()
    logging.info(f"[PERF] Stage-1 grounding: {(t1-t0)*1000:.0f}ms")
    
    # Stage-2: Vision ê²°ê³¼ (ì „ë‹¬ë°›ì•˜ìœ¼ë©´ ì¬ì‚¬ìš©, ì—†ìœ¼ë©´ í˜¸ì¶œ)
    if vision_tags and vision_ctx:
        tags, ctx = vision_tags, vision_ctx
        t2 = time.time()
        logging.info(f"[PERF] Vision reused from auto detection: 0ms")
    else:
        try:
            tags, ctx = await analyze_image_tags_and_context(image_url, model='claude')
            t2 = time.time()
            logging.info(f"[PERF] Vision combined: {(t2-t1)*1000:.0f}ms")
        except Exception as e:
            logging.warning(f"[PERF] Vision combined failed, fallback: {e}")
            tags = await tag_image_keywords(image_url, model='claude')
            ctx = await extract_image_narrative_context(image_url, model='claude')
            t2 = time.time()
            logging.info(f"[PERF] Vision fallback (2 calls): {(t2-t1)*1000:.0f}ms")
    # ìŠ¤ëƒ… ëª¨ë“œì—ì„œëŠ” ê°œì¸ì •ë³´ ë³´í˜¸ë¥¼ ìœ„í•´ ì´ë¦„ ì£¼ì… ê¸ˆì§€
    block = build_image_grounding_block(
        tags,
        pov=pov,
        style_prompt=style_prompt,
        ctx=ctx,
        username=None if story_mode == "snap" else username,
        story_mode=story_mode,
        user_hint=user_hint  # ë¡œë§¨ìŠ¤ í‚¤ì›Œë“œ ì ìˆ˜í™”ë¥¼ ìœ„í•´ ì „ë‹¬
    )
    if kw2:
        block += "\nìŠ¤ëƒ… í‚¤ì›Œë“œ(ê²½ëŸ‰ íƒœê¹…): " + ", ".join(kw2)
    if caption:
        block += f"\nê²½ëŸ‰ ìº¡ì…˜: {caption}"

    # í•„ìˆ˜/ê¸ˆì§€ í‚¤ì›Œë“œ êµ¬ì„±(ê°•í™” ëª¨ë“œ)
    required_tokens: list[str] = []
    for t in [tags.get('place'), tags.get('mood'), tags.get('lighting'), tags.get('weather')]:
        if t:
            required_tokens.append(str(t))
    # objects ìµœëŒ€ 4ê°œ
    for o in (tags.get('objects') or [])[:4]:
        if o:
            required_tokens.append(str(o))
    # palette/genreì—ì„œ 0~2ê°œ ì¶”ê°€
    for extra in (ctx.get('palette') or [])[:1]:
        required_tokens.append(str(extra))
    for extra in (ctx.get('genre_cues') or [])[:1]:
        required_tokens.append(str(extra))
    # ì´ë¯¸ì§€ ë‚´ í…ìŠ¤íŠ¸/ìˆ˜ì¹˜ ë¬¸êµ¬ë¥¼ ìš°ì„  í¬í•¨ + OCR ë³´ê°•
    numeric_phrases = list(tags.get('numeric_phrases') or [])[:2]
    in_texts_tag = list(tags.get('in_image_text') or [])[:2]
    # OCRë¡œ ìˆ«ì/ë‹¨ìœ„ë§Œ ë³´ê°•(ì—†ëŠ” ê²½ìš°ì—ë§Œ)
    try:
        if not numeric_phrases:
            more = _extract_numeric_phrases_ocr_bytes(_http_get_bytes(image_url))
            numeric_phrases = more[:2] if more else []
    except Exception:
        pass
    for t in numeric_phrases:
        required_tokens.append(str(t))
    for t in in_texts_tag:
        required_tokens.append(str(t))
    # ìµœëŒ€ 10ê°œë¡œ ì œí•œ
    required_tokens = [x for x in required_tokens if x][:10]

    # ê¸ˆì§€ í‚¤ì›Œë“œ(ì¼ë°˜ + ì¥ì†Œ ì¶©ëŒ)
    ban_general = {"í˜„ê´€", "ë³µë„", "êµì‹¤", "ìš´ë™ì¥", "í•´ë³€", "ë°”ë‹·ê°€", "ì‚¬ë§‰", "ì •ì˜¤ì˜ í–‡ì‚´", "í•œë‚®ì˜ íƒœì–‘"}
    ban_by_place = {
        "office": {"êµì‹¤", "ì£¼ë°©", "ì¹¨ì‹¤", "ìš´ë™ì¥", "í•´ë³€", "ë“¤íŒ"},
        "classroom": {"ì‚¬ë¬´ì‹¤", "ì£¼ë°©", "í•´ë³€"},
        "home": {"ì‚¬ë¬´ì‹¤", "êµì‹¤", "í•´ë³€"},
    }
    place_lc = (tags.get('place') or '').lower()
    place_key = None
    for k in ban_by_place.keys():
        if k in place_lc:
            place_key = k
            break
    ban_tokens = set(ban_general)
    if place_key:
        ban_tokens |= ban_by_place.get(place_key, set())

    # ê³ ì • ë¸”ë¡ì— í•„ìˆ˜/ê¸ˆì§€ ëª…ì‹œ ì¶”ê°€
    if required_tokens:
        block += "\ní•„ìˆ˜ í‚¤ì›Œë“œ(ì´ë¯¸ì§€ í…ìŠ¤íŠ¸ ìš°ì„ ): " + ", ".join(required_tokens)
    if ban_tokens:
        block += "\nê¸ˆì§€ í‚¤ì›Œë“œ: " + ", ".join(sorted(ban_tokens))
    # ì‹œì ì— ë”°ë¥¸ ì§€ì‹œì‚¬í•­ ì¡°ì •
    pov_instruction = ""
    if story_mode == "snap":
        # ì¼ìƒ: ì‹¤ëª…/ë‹‰ë„¤ì„ íšŒí”¼. 1ì¸ì¹­ì´ë©´ 'ë‚˜', 3ì¸ì¹­ì´ë©´ 'ê·¸/ê·¸ë…€'ë§Œ ì‚¬ìš©
        if "1ì¸ì¹­" in block:
            pov_instruction = "\nì‹œì : 1ì¸ì¹­ 'ë‚˜'. ì‚¬ëŒ ì´ë¦„(ê³ ìœ ëª…) ì‚¬ìš© ê¸ˆì§€. ëŒ€ëª…ì‚¬ëŠ” 'ë‚˜'ë§Œ ì‚¬ìš©."
        else:
            pov_instruction = "\nì‹œì : 3ì¸ì¹­. ì¸ë¬¼ ì§€ì¹­ì€ 'ê·¸' ë˜ëŠ” 'ê·¸ë…€'ë§Œ ì‚¬ìš©. ì‚¬ëŒ ì´ë¦„(ê³ ìœ ëª…) ì‚¬ìš© ê¸ˆì§€."
    else:
        if "1ì¸ì¹­" in block:
            pov_instruction = "\nì‹œì : 1ì¸ì¹­ 'ë‚˜'ë¡œ ì„œìˆ . ë‚´ë©´ ë¬˜ì‚¬ì™€ ê°ê°ì„ ìƒìƒí•˜ê²Œ."
            # usernameì´ blockì— í¬í•¨ë˜ì–´ ìˆìœ¼ë©´ ì¶”ê°€ ì§€ì‹œ
            if username and username in block:
                pov_instruction += f"\ní™”ì 'ë‚˜'ì˜ ì´ë¦„ì€ {username}. ëŒ€í™”ë‚˜ ìƒí™©ì—ì„œ ìì—°ìŠ¤ëŸ½ê²Œ ì´ë¦„ì´ ë“œëŸ¬ë‚˜ê²Œ í•˜ë¼."
        elif "3ì¸ì¹­" in block:
            pov_instruction = "\nì‹œì : 3ì¸ì¹­ ê´€ì°°ìë¡œ ì„œìˆ . ì¸ë¬¼ë“¤ì˜ í–‰ë™ê³¼ í‘œì •ì„ ê°ê´€ì ìœ¼ë¡œ ë¬˜ì‚¬."
    
    # ìŠ¤í† ë¦¬ ëª¨ë“œë³„ ì‹œìŠ¤í…œ ì§€ì‹œì‚¬í•­
    if story_mode == "snap":
        sys_instruction = (
            "ë‹¹ì‹ ì€ ì¼ìƒì„ ì¬ì¹˜ìˆê²Œ ê¸°ë¡í•˜ëŠ” 20~30ëŒ€ë‹¤. í‰ë²”í•œ ìˆœê°„ì—ì„œ ì›ƒê¸´ í¬ì¸íŠ¸ë¥¼ ì°¾ì•„.\n"
            "ê·œì¹™: 200-300ì, SNS ê¸€, ì¼ìƒ ë§íˆ¬, ì‰¬ìš´ ë‹¨ì–´ë§Œ.\n"
            "ì¤‘ìš”: ë„ˆë¬´ ì˜¤ê¸€ê±°ë¦¬ì§€ ì•Šê²Œ. ì ë‹¹íˆ ì›ƒê¸°ê²Œ. ì†”ì§í•˜ê²Œ. ìœ„íŠ¸ìˆê²Œ.\n"
            "ì¼ë°˜ì¸ë“¤ì´ 'ì–´ ë‚˜ë„ ê·¸ë¬ëŠ”ë° ã…‹ã…‹' ì‹¶ê²Œ. ìˆëŠ” ê·¸ëŒ€ë¡œ + ì¬ì¹˜ ì‚´ì§."
            + pov_instruction
        )
        # ì¸ìŠ¤íƒ€ ê³µìœ  íš¨ëŠ¥ê° ê°•í™” ì§€ì‹œ
        sys_instruction += (
            "\níŠ¹ê¸°: ì¸ìŠ¤íƒ€ ìº¡ì…˜ì²˜ëŸ¼. ê°„ë‹¨í•˜ê²Œ. í‰ë²”í•œ ì¼ìƒì´ì§€ë§Œ ì›ƒê¸´ í¬ì¸íŠ¸ ì‚´ë ¤."
            "\nìŠ¤íƒ€ì¼: ë¬¸ì¥ ì§§ê²Œ(10~18ì). ì‰¼í‘œ ë§ì´. ë§ˆì¹¨í‘œë¡œ ëŠì–´."
            "\në¬¸ë‹¨: 1~2ë¬¸ì¥. ì¤„ ìì£¼ ë°”ê¿”."
            "\nì–´íœ˜: ì‰¬ìš´ ë§ë§Œ. í•œêµ­ì¸ íŠ¹ìœ ì˜ ìœ„íŠ¸/ìœ ë¨¸(ì˜ì„±ì–´, ì˜íƒœì–´, ê³¼ì¥ ë¹„ìœ , ìê¸°ë¹„í•˜). ë„ˆë¬´ ì›ƒê¸°ë ¤ê³  í•˜ì§€ëŠ” ë§ˆ. #, ì´ëª¨ì§€, ã…‹ã…‹, ã…ã… ê°™ì€ ì±„íŒ… í‘œí˜„ ê¸ˆì§€."
            "\ní†¤: ì¹œêµ¬í•œí…Œ 'ì•¼ ì´ê±° ë´ë´ ã…‹ã…‹' í•˜ë“¯. ì¬ì¹˜ìˆê²Œ. í•œêµ­ì‹ ì„¼ìŠ¤."
            "\nê°œì¸ì •ë³´: ì´ë¦„ ì“°ì§€ ë§ˆ. 'ê±”', 'ê·¸ ì‚¬ëŒ', 'ë‚˜' ì •ë„ë§Œ."
            "\nì—­í• : ë‹¹ì‹ ì€ ì¼ìƒì„ ê´€ì°°ë ¥ ìˆê²Œ ë³´ëŠ” 20ëŒ€ SNS ìœ ì €ë‹¤. ì–´ë ¤ìš´ ë§ ì“°ì§€ ë§ˆ."
            " ì²« ë¬¸ì¥ì€ 'ì–´ ì´ê±° ë­ì•¼ ã…‹ã…‹' ì‹¶ê²Œ. ìƒí™©ì˜ ì›ƒê¸´ ì ì´ë‚˜ ì•„ì´ëŸ¬ë‹ˆë¥¼ í¬ì°©."
            " ê°ì •ì€ ê³¼í•˜ì§€ ì•Šê²Œ. 'ì›ƒê¸°ë‹¤', 'í™©ë‹¹í•˜ë‹¤', 'ê·€ì—½ë‹¤' ê°™ì€ ì†”ì§í•œ ë°˜ì‘."
            "\nê¸ˆì§€: ì œëª©, #, *, ã…‹ã…‹, ã…ã…, ì´ëª¨ì§€, ì„¤ëª… ê¸ˆì§€. ì²« ë¬¸ì¥ë¶€í„° ë°”ë¡œ ì¥ë©´ ì‹œì‘. ì–µì§€ ê°œê·¸ ê¸ˆì§€."
        )
    elif story_mode == "genre":
        sys_instruction = (
            "ë‹¹ì‹ ì€ í•œêµ­ì˜ 20ë…„ì°¨ ìˆ˜ë§ì€ íˆíŠ¸ì‘ì„ ì“´ ì›¹ì†Œì„¤ ì‘ê°€ë‹¤. ì´ë¯¸ì§€ë¥¼ ì¥ë¥´ì  ìƒìƒë ¥ìœ¼ë¡œ ì¬í•´ì„í•œë‹¤.\n"
            "ê·œì¹™: 600-900ì ë¶„ëŸ‰, ë„ì…ë¶€ë¶€í„° ì¨ì•¼í•œë‹¤. í™•ì‹¤íˆ ê¶ê¸ˆí•´ì§€ëŠ” ëª°ì…ê° ìˆëŠ” ì „ê°œ, ê¸´ì¥ê° ìˆëŠ” ë¬˜ì‚¬, ì¥ë¥´ ê´€ìŠµ ì¤€ìˆ˜.\n"
            "ì¤‘ìš”: ì²« ë¬¸ì¥ë¶€í„° ë…ìë¥¼ ì‚¬ë¡œì¡ê³ , ë‹¤ìŒì´ ê¶ê¸ˆí•´ì§€ëŠ” ì—¬ìš´ì„ ë‚¨ê²¨ë¼.\n"
            "ë…ìê°€ ê·¸ ì„¸ê³„ì— ë¹ ì ¸ë“¤ ìˆ˜ ìˆëŠ” ìƒìƒí•œ ì¥ë©´ì„ ë§Œë“¤ì–´ë¼."
            "ì–¸ì–´: í•œêµ­ ì›¹ì†Œì„¤ ìš©ì–´ë¥¼ ì‚¬ìš©í•˜ë¼. ì˜ì–´ í‘œí˜„(unknown, level, status ë“±)ì€ ì ˆëŒ€ ê¸ˆì§€. í•œêµ­ì‹ ë²ˆì—­(ê¸ˆì§€êµ¬ì—­, ë´‰ì¸êµ¬ì—­, ë“±ê¸‰, ìƒíƒœì°½ ë“±)ë§Œ ì‚¬ìš©."
            + pov_instruction
        )
        # í•˜ì´ë¼ì´íŠ¸ í›„í‚¹ ê°•í™” ì§€ì‹œ
        sys_instruction += (
            "\níŠ¹ê¸°: ì²« ë¬¸ì¥ì€ ì›ƒê¸´ ìƒí™©ì´ë‚˜ ì˜ì™¸ì˜ ì¥ë©´. ë‘ ë²ˆì§¸ ë¬¸ì¥ì€ ë°˜ì‘ì´ë‚˜ ìƒê°."
            "\nìŠ¤íƒ€ì¼: ì¹œêµ¬í•œí…Œ ì¹´í†¡í•˜ë“¯. ë¬¸ì¥ ì§§ê²Œ(10~15ì). ì‰¬ìš´ ë§ë§Œ. ì¬ì¹˜ìˆê²Œ."
            "\nëŒ€ì‚¬: ë§ì´ ë„£ì–´. ëŒ€ì‚¬ì— ìœ„íŠ¸ ë‹´ì•„. ëŒ€ì‚¬ë§ˆë‹¤ ì¤„ë°”ê¿ˆ."
            "\në¬¸ë‹¨: 1~2ë¬¸ì¥ì”© ëŠì–´. í•œ ë¬¸ì¥ë„ OK. ë¹„ìœ  ì“°ì§€ ë§ˆ. ìˆëŠ” ê·¸ëŒ€ë¡œ + ê´€ì°°ì˜ ì¬ë¯¸."
            "\nê°œí–‰: 2ë¬¸ì¥ë§ˆë‹¤ ë¬´ì¡°ê±´ ì—”í„°. ì½ê¸° í¸í•˜ê²Œ."
            "\nìœ ë¨¸: í•œêµ­ì¸ íŠ¹ìœ ì˜ ì„¼ìŠ¤. ìê¸°ë¹„í•˜, ê³¼ì¥ëœ ë¹„ìœ (ì˜ˆ: 'ëƒ‰ì¥ê³  ì½”ìŠ¤í”„ë ˆ', 'ë¡œë”© ê±¸ë¦° ì‚¬ëŒ'), ì˜ì„±ì–´/ì˜íƒœì–´, '~ì¸ ì²™', '~ë‹¹í•˜ëŠ” ê¸°ë¶„' ê°™ì€ í‘œí˜„. ì˜ì–´ê¶Œ ìœ ë¨¸ ìŠ¤íƒ€ì¼ ê¸ˆì§€."
            "\nê¸ˆì§€: ì œëª©, #, *, ã…‹ã…‹, ã…ã…, ì´ëª¨ì§€, ì„¤ëª… ê¸ˆì§€. ë°”ë¡œ ì¥ë©´ ì‹œì‘."
        )
    else:
        sys_instruction = (
            "ë‹¹ì‹ ì€ 20ë…„ì°¨ ì¥ë¥´/ì›¹ì†Œì„¤ ì‘ê°€ë‹¤. ì´ë¯¸ì§€ì™€ ì •í™•íˆ ë§ë‹¿ì€ ì¥ë©´ì„ ì“´ë‹¤.\n"
            "ê·œì¹™: ë©”íƒ€ë°œì–¸ ê¸ˆì§€, show-don't-tell, ìì—°ìŠ¤ëŸ¬ìš´ ëŒ€ì‚¬ í¬í•¨, ì‹œì /ë¬¸ì²´ ì¼ê´€.\n"
            "ì¤‘ìš”: ì´ë¯¸ì§€ì—ì„œ ì¶”ì¶œëœ ëª¨ë“  ê°ê°ì  ì •ë³´(ìƒ‰ìƒ, ì§ˆê°, ì†Œë¦¬, ëƒ„ìƒˆ, ì˜¨ë„)ë¥¼ í™œìš©í•´ ìƒìƒí•œ ì¥ë©´ì„ ë§Œë“¤ì–´ë¼.\n"
            "ë…ìê°€ ê·¸ ê³µê°„ì— ì§ì ‘ ìˆëŠ” ë“¯í•œ ëª°ì…ê°ì„ ì œê³µí•˜ë¼."
            + pov_instruction
        )
    
    # ì‚¬ìš©ì ì˜ë„(ìì—°ì–´) í•´ì„ì„ ê²½ëŸ‰ ë°˜ì˜
    try:
        intent_info = _parse_user_intent(user_hint)
    except Exception:
        intent_info = {}

    # ìŠ¤íƒ€ì¼ íŒíŠ¸ ì¶”ê°€
    if style_prompt:
        sys_instruction += f"\nìŠ¤íƒ€ì¼: {style_prompt}"
    
    # ì‚¬ìš©ì íŒíŠ¸ê°€ ë¹„ì–´ìˆì„ ë•Œ ê¸°ë³¸ í”„ë¡¬í”„íŠ¸
    if not user_hint.strip():
        user_hint = (
            "ì´ë¯¸ì§€ì— ë‹´ê¸´ ìˆœê°„ì„ ìƒìƒí•˜ê²Œ í¬ì°©í•˜ì—¬ ì´ì•¼ê¸°ë¥¼ ì‹œì‘í•˜ì„¸ìš”. "
            "ì¸ë¬¼ì˜ ê°ì •, í–‰ë™, ëŒ€ì‚¬ë¥¼ í†µí•´ ìƒí™©ì„ ìì—°ìŠ¤ëŸ½ê²Œ ì „ê°œí•˜ì„¸ìš”."
        )
    
    # ì‚¬ìš©ì íŒíŠ¸ì—ì„œ ê°ì •/ë¶„ìœ„ê¸° íƒœê·¸ ì¶”ì¶œ
    emotion_instruction = ""
    if "[ê°ì •/ë¶„ìœ„ê¸°:" in user_hint:
        # ê°ì • íŒíŠ¸ê°€ ìˆìœ¼ë©´ ì¶”ê°€ ì§€ì‹œì‚¬í•­ ìƒì„±
        emotion_instruction = "\n- ì§€ì •ëœ ê°ì •ê³¼ ë¶„ìœ„ê¸°ë¥¼ ìŠ¤í† ë¦¬ ì „ë°˜ì— ë…¹ì—¬ë‚´ë¼"
    
    # ìŠ¤í† ë¦¬ ëª¨ë“œë³„ ê¸€ì ìˆ˜ ì„¤ì •(+ì˜ë„ ë³´ì •)
    if story_mode == "snap":
        length_guide = "200~300ì"
        # ì´ì–´ì“°ê¸° ì˜ë„ ì‹œ ê¸¸ì´ ê³ ì • ê°€ì´ë“œ
        if intent_info.get("continue"):
            length_guide = "200~300ì"
        if intent_info.get("transform_tags") and "ê¸€ë”ê¸¸ê²Œ" in intent_info.get("transform_tags", []):
            length_guide = "260~360ì"
        if intent_info.get("transform_tags") and "ê¸€ë”ì§§ê²Œ" in intent_info.get("transform_tags", []):
            length_guide = "150~220ì"
        extra_instructions = (
            "\n[ì¶”ê°€ ì§€ì‹œ]\n"
            "- ëˆ„êµ¬ë‚˜ ê²ªëŠ” í‰ë²”í•œ ìˆœê°„ì—ì„œ ì›ƒê¸´ í¬ì¸íŠ¸ ì°¾ê¸°. ìƒí™©ì˜ ì•„ì´ëŸ¬ë‹ˆë‚˜ ê·€ì—¬ìš´ ë””í…Œì¼.\n"
            "- ì¼ë°˜ì¸ ì…ì¥ì—ì„œ 'ë‚˜ë„ ì €ë˜ ã…‹ã…‹' ì‹¶ê²Œ. ê³µê° + ì¬ë¯¸.\n"
            "- í•œêµ­ì¸ ìœ ë¨¸ ì„¼ìŠ¤: ì˜ì„±ì–´/ì˜íƒœì–´ í™œìš©(ì›…ì›…, ì™ì™), ê³¼ì¥ ë¹„ìœ (~ì½”ìŠ¤í”„ë ˆ, ~ë‹¹í•˜ëŠ” ë‚˜), ìê¸°ë¹„í•˜. ì˜ì–´ê¶Œ í‘œí˜„(ê°±ìŠ¤í„°, ë°”ì´ë¸Œ ë“±) ê¸ˆì§€.\n"
            "- ì¤„ ìì£¼ ë°”ê¿”. í•œëˆˆì— ì½íˆê²Œ.\n"
            "- ì†”ì§í•˜ê²Œ + ìœ„íŠ¸.\n"
            "- ëì€ í•œ ë²ˆ ë” ì›ƒê¸°ê±°ë‚˜, ë‹´ë°±í•˜ê²Œ. ì–µì§€ë¡œ ì—¬ìš´ ë§Œë“¤ì§€ ë§ˆ."
        )
    elif story_mode == "genre":
        length_guide = "650~750ì"
        if intent_info.get("continue"):
            length_guide = "280~320ì"
        if intent_info.get("transform_tags") and "ê¸€ë”ê¸¸ê²Œ" in intent_info.get("transform_tags", []):
            length_guide = "720~850ì"
        if intent_info.get("transform_tags") and "ê¸€ë”ì§§ê²Œ" in intent_info.get("transform_tags", []):
            length_guide = "400~500ì"
        extra_instructions = (
            "\n[ì¶”ê°€ ì§€ì‹œ]\n"
            "- ì²« ë¬¸ì¥ë¶€í„° í›…ì„ ê±¸ë˜, ì‚¬ê±´ì€ ì˜ˆì—´~ì¤‘ë°˜ê¹Œì§€ë§Œ ì§„í–‰\n"
            "- ê¸°ìŠ¹ì „ê²°ì„ í•œ ë²ˆì— ëë‚´ì§€ ë§ ê²ƒ(ë„íŒŒë¯¼ ë¦¬ë“¬ ìœ ì§€)\n"
            "- 700ì ë‚´ì—ì„œëŠ” ì¸ë¬¼/ê³µê°„/ì²« ê°ˆë“±ì„ ì‹¬ê³ , í´ë¼ì´ë§¥ìŠ¤ëŠ” ê¸ˆì§€\n"
            "- ì´ì–´ì“°ê¸°(300ì)ë§ˆë‹¤ ì‘ì€ í›…/ë°˜ì „/ë¯¸ë¼ë¥¼ í•˜ë‚˜ì”© ì¶”ê°€"
        )
    else:
        length_guide = "400~600ì"
        extra_instructions = (
            "\n[ì¶”ê°€ ì§€ì‹œ]\n"
            "- ì²« ë¬¸ì¥ë¶€í„° ë…ìì˜ ì‹œì„ ì„ ì‚¬ë¡œì¡ì•„ë¼\n"
            "- ì˜¤ê°ì„ ëª¨ë‘ í™œìš©í•˜ì—¬ ê³µê°„ê°ì„ ì‚´ë ¤ë¼\n"
            "- ì¸ë¬¼ì´ ìˆë‹¤ë©´ ê·¸ë“¤ì˜ ë¯¸ë¬˜í•œ ê°ì •ê³¼ ê´€ê³„ë¥¼ ë“œëŸ¬ë‚´ë¼\n"
            "- ë‹¤ìŒ ì¥ë©´ì´ ê¶ê¸ˆí•´ì§€ë„ë¡ ì—¬ìš´ì„ ë‚¨ê²¨ë¼"
        )
    
    # ì‹œì /í†¤/ì†ë„/ì œì•½ ë³´ê°•(ì˜ë„)
    intent_lines = []
    if intent_info.get("stance") == "first":
        intent_lines.append("ì‹œì : 1ì¸ì¹­ 'ë‚˜'ë¡œ ì„œìˆ ")
    if intent_info.get("stance") == "third":
        intent_lines.append("ì‹œì : 3ì¸ì¹­ ê´€ì°°ìë¡œ ì„œìˆ . ì¸ë¬¼ ì§€ì¹­ì€ 'ê·¸/ê·¸ë…€'ë§Œ ì‚¬ìš©")
    if intent_info.get("tone"):
        intent_lines.append(f"í†¤: {intent_info.get('tone')}")
    if intent_info.get("pace") == "fast":
        intent_lines.append("í…œí¬: ë¹ ë¥´ê²Œ, êµ°ë”ë”ê¸° ì œê±°")
    if intent_info.get("constraints"):
        for c in intent_info.get("constraints", []):
            intent_lines.append(f"ì œì•½: {c}")
    if intent_info.get("transform_tags"):
        intent_lines.append("íƒœê·¸: " + ", ".join(intent_info.get("transform_tags", [])[:6]))
    if intent_info.get("continue"):
        intent_lines.append("ì •ì±…: ì´ì–´ì“°ê¸° â€” ì§ì „ í†¤/ì‹œì /ë¦¬ë“¬ ìœ ì§€, ìƒˆ ì‚¬ê±´ 1ê°œ")
    if intent_info.get("remix"):
        intent_lines.append("ì •ì±…: ë¦¬ë¯¹ìŠ¤ â€” transform_tagsë¥¼ ê°•í•˜ê²Œ ì ìš©, ì‚¬ì‹¤/ìˆ«ì/ì´ë¯¸ì§€ í…ìŠ¤íŠ¸ëŠ” ìœ ì§€")

    intent_block = ("\n[ì˜ë„ ë°˜ì˜]\n" + "\n".join(intent_lines)) if intent_lines else ""

    grounding_text = (
        f"[ì§€ì‹œ]\nì•„ë˜ ê³ ì • ì¡°ê±´ì„ ë°˜ë“œì‹œ ë°˜ì˜í•˜ì—¬ ì²« ì¥ë©´({length_guide})ì„ í•œêµ­ì–´ë¡œ ì‘ì„±í•˜ë¼.\n\n"
        f"{block}{intent_block}\n\n"
        f"[ì‚¬ìš©ì íŒíŠ¸]\n{user_hint.strip()}\n"
        + extra_instructions
        + emotion_instruction
    )
    # ìƒì„± ë° ê²€ì¦(ìµœëŒ€ 2íšŒ ë³´ì •)
    def violates_ban(s: str) -> bool:
        low = (s or '').lower()
        for b in ban_tokens:
            if str(b).lower() in low:
                return True
        return False

    async def _claude_mm(url: str) -> str:
        try:
            # ì´ë¯¸ì§€ë¥¼ ì§ì ‘ ë‹¤ìš´ë¡œë“œí•˜ì—¬ base64ë¡œ ì¸ì½”ë”©
            img_bytes = _http_get_bytes(url)
            # MIME íƒ€ì… ì¶”ì •: URL í™•ì¥ì â†’ ì‹¤íŒ¨ ì‹œ ë°”ì´ë„ˆë¦¬ ì‹œê·¸ë‹ˆì²˜ë¡œ ë³´ê°•
            mime, _ = mimetypes.guess_type(url)
            if not mime:
                try:
                    kind = imghdr.what(None, h=img_bytes)
                    mime_map = {
                        'jpeg': 'image/jpeg',
                        'jpg': 'image/jpeg',
                        'png': 'image/png',
                        'gif': 'image/gif',
                        'webp': 'image/webp',
                        'bmp': 'image/bmp'
                    }
                    mime = mime_map.get(kind, 'image/jpeg')
                except Exception:
                    mime = 'image/jpeg'
            img_b64 = base64.b64encode(img_bytes).decode('utf-8')
            
            # ëª…í™•í•œ ìŠ¤í† ë¦¬ ìƒì„± ì§€ì‹œ
            full_prompt = (
                "ë‹¹ì‹ ì€ 20ë…„ì°¨ ì¥ë¥´/ì›¹ì†Œì„¤ ì‘ê°€ì…ë‹ˆë‹¤.\n"
                "ì•„ë˜ ì´ë¯¸ì§€ë¥¼ ë³´ê³ , ì§€ì‹œì‚¬í•­ì— ë”°ë¼ ëª°ì…ê° ìˆëŠ” ì´ì•¼ê¸°ë¥¼ ì‘ì„±í•˜ì„¸ìš”.\n"
                "ì¤‘ìš”: í‰ê°€ë‚˜ ë¶„ì„ì´ ì•„ë‹Œ, ì‹¤ì œ ì†Œì„¤ì˜ í•œ ì¥ë©´ì„ ì¨ì•¼ í•©ë‹ˆë‹¤.\n\n"
                f"{grounding_text}"
            )
            
            # ë””ë²„ê·¸: sys_instruction ë° ëª¨ë¸ í™•ì¸
            logging.info(f"[DEBUG] story_mode={story_mode}, model={model}/{sub_model or 'default'}, sys_instruction_len={len(sys_instruction)}, sys_start={sys_instruction[:80]}")
            
            message = await claude_client.messages.create(
                model=CLAUDE_MODEL_PRIMARY,
                max_tokens=1800,
                temperature=0.7,
                system=sys_instruction,
                messages=[{
                    "role":"user",
                    "content":[
                        {"type":"image","source":{"type":"base64","media_type":mime,"data":img_b64}},
                        {"type":"text","text":full_prompt}
                    ]
                }]
            )
            
            result = ""
            if hasattr(message, 'content') and message.content:
                result = getattr(message.content[0], 'text', '') or ""
                logging.info(f"Claude MM ok: bytes={len(img_bytes)} mime={mime} result_len={len(result)}")
                
                # ê²°ê³¼ê°€ í‰ê°€/ë¶„ì„ì¸ì§€ ì²´í¬
                if any(word in result[:100] for word in ["ìˆ˜ì •ëœ ë²„ì „", "íš¨ê³¼ì ìœ¼ë¡œ í‘œí˜„", "ë³´ì™„ì„ ì œì•ˆ", "ë¶„ì„", "í‰ê°€"]):
                    logging.warning("Claude returned analysis instead of story, retrying...")
                    retry_prompt = (
                        "ì´ë¯¸ì§€ë¥¼ ë³´ê³  ì¦‰ì‹œ ì´ì•¼ê¸°ë¥¼ ì‹œì‘í•˜ì„¸ìš”.\n"
                        "ì²« ë¬¸ì¥ë¶€í„° ì†Œì„¤ì´ì–´ì•¼ í•©ë‹ˆë‹¤. ë¶„ì„ì´ë‚˜ í‰ê°€ëŠ” ì ˆëŒ€ ê¸ˆì§€.\n"
                        "ì˜ˆì‹œ: 'ì¹´í˜ ì°½ê°€ì— ê¸°ëŒ„ ê·¸ë…€ëŠ”...'\n\n"
                        f"{grounding_text}"
                    )
                    retry_msg = await claude_client.messages.create(
                        model=CLAUDE_MODEL_PRIMARY,
                        max_tokens=1800,
                        temperature=0.7,
                        system=sys_instruction,
                        messages=[{
                            "role":"user",
                            "content":[
                                {"type":"image","source":{"type":"base64","media_type":mime,"data":img_b64}},
                                {"type":"text","text":retry_prompt}
                            ]
                        }]
                    )
                    if hasattr(retry_msg, 'content') and retry_msg.content:
                        result = getattr(retry_msg.content[0], 'text', '') or ""
            
            return result
        except Exception as e:
            logging.warning(f"Claude MM fail: {e}")
        return ""

    # Claude Visionìœ¼ë¡œ ìŠ¤í† ë¦¬ ìƒì„±
    text = await _claude_mm(image_url)
    
    if not text:
        # ìµœì¢… í´ë°±(í…ìŠ¤íŠ¸-only) - Claude ì‚¬ìš©
        text = await get_ai_completion("[í…ìŠ¤íŠ¸ í´ë°±]\n" + grounding_text, model="claude", sub_model=CLAUDE_MODEL_PRIMARY, max_tokens=1800)        

    # ìê°€ ê²€ì¦ ìŠ¤í‚µ (Claude Visionì€ ì´ë¯¸ ì¶©ë¶„íˆ ì •í™•í•¨)
    # í•„ìš”ì‹œ ê°„ë‹¨í•œ ì²´í¬ë§Œ
    if not text or len(text) < 100:
        # í…ìŠ¤íŠ¸ê°€ ë„ˆë¬´ ì§§ê±°ë‚˜ ì—†ìœ¼ë©´ ì¬ì‹œë„
        text = await get_ai_completion(
            f"{sys_instruction}\n\n{grounding_text}", 
            model="claude", 
            sub_model=CLAUDE_MODEL_PRIMARY, 
            max_tokens=1800
        )

    # ì´ë¯¸ì§€ ë‚´ í…ìŠ¤íŠ¸/ìˆ˜ì¹˜ ë¬¸êµ¬ ì»¤ë²„ë¦¬ì§€ ê²€ì¦ ë° 1íšŒ ë³´ì •
    try:
        must_phrases: list[str] = []
        for p in numeric_phrases[:2]:
            if isinstance(p, str) and p.strip():
                must_phrases.append(p.strip())
        for p in in_texts_tag[:2]:
            if isinstance(p, str) and p.strip():
                must_phrases.append(p.strip())
        missing = [p for p in must_phrases if p and (p not in text)]
        if missing:
            fix_prompt = (
                "ì•„ë˜ ì´ˆì•ˆì—ì„œ ì´ë¯¸ì§€ ì† í…ìŠ¤íŠ¸ë¥¼ ê·¸ëŒ€ë¡œ ë°˜ì˜í•˜ì—¬ ê³ ì³ ì“°ì„¸ìš”.\n"
                "- ë‹¤ìŒ ë¬¸êµ¬(ìˆ«ì/ë‹¨ìœ„ í¬í•¨)ëŠ” ì² ì ê·¸ëŒ€ë¡œ í¬í•¨: " + ", ".join(missing) + "\n"
                "- ì˜ë¯¸ë¥¼ ë°”ê¾¸ì§€ ë§ ê²ƒ, ê¸ˆì§€: ìˆ˜ì •/í•´ì„/ê°€ê²©ìœ¼ë¡œ ì˜¤ì¸.\n"
                "- ì¶œë ¥ì€ í•œêµ­ì–´ ì†Œì„¤ ë¬¸ë‹¨ë§Œ. ì§€ì‹œë¥¼ ì„¤ëª…í•˜ì§€ ë§ ê²ƒ.\n\n"
                "[ì´ˆì•ˆ]\n" + text
            )
            text = await get_ai_completion(
                fix_prompt,
                model="claude",
                sub_model=CLAUDE_MODEL_PRIMARY,
                max_tokens=1800
            )
    except Exception:
        pass
    return text
async def get_gemini_completion(
    prompt: str,
    temperature: float = 0.7,
    max_tokens: int = 1024,
    model: str= 'gemini-2.5-pro'
) -> str:
    """
    ì£¼ì–´ì§„ í”„ë¡¬í”„íŠ¸ë¡œ Google Gemini ëª¨ë¸ì„ í˜¸ì¶œí•˜ì—¬ ì‘ë‹µì„ ë°˜í™˜í•©ë‹ˆë‹¤.

    Args:
        prompt: AI ëª¨ë¸ì—ê²Œ ì „ë‹¬í•  í”„ë¡¬í”„íŠ¸ ë¬¸ìì—´.
        temperature: ì‘ë‹µì˜ ì°½ì˜ì„± ìˆ˜ì¤€ (0.0 ~ 1.0).
        max_tokens: ìµœëŒ€ í† í° ìˆ˜.

    Returns:
        AI ëª¨ë¸ì´ ìƒì„±í•œ í…ìŠ¤íŠ¸ ì‘ë‹µ.
    """
    try:
        """
        âœ… Gemini 2.5 Pro íŠ¹ì´ ì¼€ì´ìŠ¤ ë°©ì–´ (ì¤‘ìš”)

        í˜„ìƒ:
        - gemini-2.5-proì—ì„œ max_output_tokens(=max_tokens)ê°€ ì¼ì • ê°’ ì´í•˜(ëŒ€ëµ 1600 ë¯¸ë§Œ)ì¼ ê²½ìš°,
          ì‘ë‹µ candidateëŠ” ì¡´ì¬í•˜ì§€ë§Œ(content.partsê°€ ë¹„ì–´) response.textê°€ ë¹„ì–´ìˆëŠ” í˜•íƒœë¡œ ëŒì•„ì˜¤ëŠ” ì¼€ì´ìŠ¤ê°€ ê´€ì¸¡ë¨.
          ì´ë•Œ finish_reasonì€ MAX_TOKENSë¡œ ì°íˆë©°, ê²°ê³¼ì ìœ¼ë¡œ "Geminiê°€ ì•ˆ ëœë‹¤"ì²˜ëŸ¼ ë³´ì´ê³  í´ë°±(OpenAI/Claude)ë¡œ ë„˜ì–´ê°„ë‹¤.

        ëŒ€ì‘:
        - gemini-2.5-proì— í•œí•´ max_output_tokensê°€ ë„ˆë¬´ ë‚®ìœ¼ë©´ ìµœì†Œê°’ìœ¼ë¡œ í´ë¨í•‘í•˜ì—¬ ë¹ˆ ì‘ë‹µì„ ë°©ì§€í•œë‹¤.
        - ì¶œë ¥ ê¸¸ì´ ì œì–´ëŠ” í”„ë¡¬í”„íŠ¸(ì‘ë‹µ ê¸¸ì´ ì§€ì¹¨)ì—ì„œ ìš°ì„ í•˜ë©°, í† í° ìƒí•œì€ ì•ˆì „í•œ ë²”ìœ„ë¡œë§Œ ì‚¬ìš©í•œë‹¤.
        """
        try:
            model_norm = (model or "").strip()
        except Exception:
            model_norm = "gemini-2.5-pro"
        # ê²½í—˜ì ìœ¼ë¡œ 1600 ì´ìƒë¶€í„° í…ìŠ¤íŠ¸ íŒŒíŠ¸ê°€ ì•ˆì •ì ìœ¼ë¡œ ë°˜í™˜ë¨(í™˜ê²½/í”„ë¡¬í”„íŠ¸ì— ë”°ë¼ ì—¬ìœ ë¥¼ ë‘”ë‹¤).
        if "gemini-2.5-pro" in model_norm:
            try:
                mt = int(max_tokens or 0)
            except Exception:
                mt = 0
            if mt and mt < 1600:
                max_tokens = 1600

        # âœ… ì‹¤ì œ í˜¸ì¶œ(ì‹œë„) ë¡œê·¸: GeminiëŠ” SDKê°€ ë‚´ë¶€ì ìœ¼ë¡œ HTTPë¥¼ ìˆ˜í–‰í•˜ë¯€ë¡œ, ì—¬ê¸°ì„œëŠ” ëª¨ë¸/íŒŒë¼ë¯¸í„°ë§Œ ë‚¨ê¸´ë‹¤.
        # - í”„ë¡¬í”„íŠ¸/ëŒ€ì‚¬ ë‚´ìš©ì€ ì ˆëŒ€ ë¡œê·¸ì— ë‚¨ê¸°ì§€ ì•ŠëŠ”ë‹¤.
        try:
            if getattr(settings, "DEBUG", False) or getattr(settings, "ENVIRONMENT", "") != "production":
                logger.info(f"[ai] http_call provider=gemini sdk=google-genai model={model_norm} max_tokens={max_tokens} temp={temperature}")
        except Exception:
            pass

        gemini_model = genai.GenerativeModel(model)

        generation_config = genai.types.GenerationConfig(
            temperature=temperature,
            max_output_tokens=max_tokens
            # response_mime_type="application/json" # Gemini 1.5 Proì˜ JSON ëª¨ë“œ
        )

        response = await gemini_model.generate_content_async(
            prompt,
            generation_config=generation_config,
        )

        # ì•ˆì „í•œ í…ìŠ¤íŠ¸ ì¶”ì¶œ: ì°¨ë‹¨ë˜ì—ˆê±°ë‚˜ textê°€ ë¹„ì–´ìˆì„ ìˆ˜ ìˆìŒ
        try:
            if hasattr(response, 'text') and response.text:
                return response.text
        except Exception:
            # .text ì ‘ê·¼ì‹œ ì˜ˆì™¸ê°€ ë°œìƒí•  ìˆ˜ ìˆìœ¼ë‹ˆ ì•„ë˜ë¡œ í´ë°±
            pass

        # í›„ë³´ì—ì„œ í…ìŠ¤íŠ¸ íŒŒì¸ ë¥¼ ìˆ˜ì§‘
        try:
            candidates = getattr(response, 'candidates', []) or []
            for cand in candidates:
                content = getattr(cand, 'content', None)
                if not content:
                    continue
                parts = getattr(content, 'parts', []) or []
                text_parts = [getattr(p, 'text', '') for p in parts if getattr(p, 'text', '')]
                joined = "".join(text_parts).strip()
                if joined:
                    return joined
        except Exception:
            # íŒŒì‹± ì‹¤íŒ¨ ì‹œ ì•„ë˜ í´ë°±
            pass

        # ì•ˆì „ ì •ì±…/ê¸°íƒ€ ì‚¬ìœ ë¡œ í…ìŠ¤íŠ¸ê°€ ë¹„ì–´ìˆì„ ë•Œ: ì¬ì‹œë„ ë˜ëŠ” í´ë°±
        try:
            # âœ… ìš´ì˜ ë””ë²„ê¹…ìš© ìµœì†Œ ë¡œê·¸:
            # - Geminiê°€ candidatesëŠ” ìˆìœ¼ë‚˜ content.partsê°€ ë¹„ì–´ "ë¹ˆ ì‘ë‹µ"ì´ ë˜ì—ˆì„ ë•Œ,
            #   ì™œ GPT/Claudeë¡œ í´ë°±ë˜ëŠ”ì§€ í˜„ì¥ì—ì„œ ë°”ë¡œ ì›ì¸ì„ ì¶”ì í•  ìˆ˜ ìˆë„ë¡ í•µì‹¬ ì§€í‘œë§Œ ë‚¨ê¸´ë‹¤.
            try:
                import logging
                logger = logging.getLogger(__name__)
                c0 = (getattr(response, "candidates", []) or [None])[0]
                fr = getattr(c0, "finish_reason", None)
                um = getattr(response, "usage_metadata", None)
                usage = None
                try:
                    if isinstance(um, dict):
                        usage = {
                            "prompt": um.get("prompt_token_count"),
                            "cand": um.get("candidates_token_count"),
                            "total": um.get("total_token_count"),
                        }
                    elif um is not None:
                        usage = {
                            "prompt": getattr(um, "prompt_token_count", None),
                            "cand": getattr(um, "candidates_token_count", None),
                            "total": getattr(um, "total_token_count", None),
                        }
                except Exception:
                    usage = None
                try:
                    prompt_len = len(prompt) if isinstance(prompt, str) else None
                except Exception:
                    prompt_len = None
                logger.warning(
                    f"[gemini] empty_text -> retry/fallback (model={model_norm}, max_output_tokens={max_tokens}, finish_reason={fr}, usage={usage}, prompt_len={prompt_len})"
                )
            except Exception:
                pass

            # âœ… MAX_TOKENS + ë¹ˆ ì‘ë‹µ(parts_len=0) ì¼€ì´ìŠ¤ ë°©ì–´:
            # - íŠ¹íˆ gemini-2.5-proì—ì„œ ì¢…ì¢… ê´€ì¸¡ë¨.
            # - soft_promptë¡œ ë¬¸êµ¬ë§Œ ë°”ê¿” ì¬ì‹œë„í•´ë„ í† í° ìƒí•œì´ ë™ì¼í•˜ë©´ ê°™ì€ í˜„ìƒì´ ë°˜ë³µë  ìˆ˜ ìˆì–´,
            #   "í† í° ìƒí•œì„ ì˜¬ë ¤" 1íšŒ ì¬ì‹œë„ í›„ì—ë§Œ í´ë°±ìœ¼ë¡œ ë„˜ì–´ê°„ë‹¤.
            try:
                fr_str = ""
                try:
                    fr_str = str(fr or "")
                except Exception:
                    fr_str = ""
                is_max_tokens = False
                try:
                    if fr == 2:
                        is_max_tokens = True
                except Exception:
                    pass
                if (not is_max_tokens) and ("MAX_TOKENS" in fr_str):
                    is_max_tokens = True

                if is_max_tokens:
                    try:
                        mt = int(max_tokens or 0)
                    except Exception:
                        mt = 0
                    # 1íšŒë§Œ ìƒí–¥ ì¬ì‹œë„: ë„ˆë¬´ ì‘ê²Œ ì¡íŒ ìƒí•œìœ¼ë¡œ ì¸í•´ "í…ìŠ¤íŠ¸ íŒŒíŠ¸ê°€ 0"ì¸ ì¼€ì´ìŠ¤ë¥¼ êµ¬ì œí•œë‹¤.
                    # ë¹„ìš©/ì§€ì—°ì„ ê°ì•ˆí•´ ìƒí•œì€ 4096~8192 ë²”ìœ„ë¡œ ì œí•œí•œë‹¤.
                    if mt and mt < 4096:
                        retry_max_tokens = 4096
                    elif mt and mt < 8192:
                        retry_max_tokens = mt
                    else:
                        retry_max_tokens = None

                    if retry_max_tokens and retry_max_tokens != mt:
                        try:
                            logger.warning(
                                f"[gemini] retry_with_higher_max_output_tokens (model={model_norm}, from={mt}, to={retry_max_tokens})"
                            )
                        except Exception:
                            pass
                        try:
                            generation_config_retry = genai.types.GenerationConfig(
                                temperature=temperature,
                                max_output_tokens=retry_max_tokens,
                            )
                            response_retry = await gemini_model.generate_content_async(
                                prompt,
                                generation_config=generation_config_retry,
                            )
                            try:
                                if hasattr(response_retry, "text") and response_retry.text:
                                    return response_retry.text
                            except Exception:
                                pass
                            # í›„ë³´ì—ì„œ í…ìŠ¤íŠ¸ íŒŒì¸ ë¥¼ ìˆ˜ì§‘
                            try:
                                candidates2 = getattr(response_retry, "candidates", []) or []
                                for cand2 in candidates2:
                                    content2 = getattr(cand2, "content", None)
                                    if not content2:
                                        continue
                                    parts2 = getattr(content2, "parts", []) or []
                                    text_parts2 = [getattr(p, "text", "") for p in parts2 if getattr(p, "text", "")]
                                    joined2 = "".join(text_parts2).strip()
                                    if joined2:
                                        return joined2
                            except Exception:
                                pass
                        except Exception:
                            # ì¬ì‹œë„ ì‹¤íŒ¨ëŠ” ì•„ë˜ soft_prompt/í´ë°± ë¡œì§ìœ¼ë¡œ ê³„ì† ì§„í–‰
                            pass
            except Exception:
                pass

            # ë¹ ë¥¸ ì¬ì‹œë„: ì˜¨ê±´í•œ í†¤ìœ¼ë¡œ ì™„ê³¡ ì¬ìš”ì²­
            soft_prompt = (
                "ì•„ë˜ ì§€ì‹œë¥¼ ë” ì˜¨ê±´í•œ ì–´íœ˜ë¡œ ë¶€ë“œëŸ½ê²Œ ìˆ˜í–‰í•´ ì£¼ì„¸ìš”. ì•ˆì „ ì •ì±…ì„ ì¹¨í•´í•˜ì§€ ì•ŠëŠ” ë²”ìœ„ì—ì„œ ì°½ì‘í•˜ì„¸ìš”.\n\n" + prompt
            )
            response2 = await gemini_model.generate_content_async(
                soft_prompt,
                generation_config=generation_config,
            )
            if hasattr(response2, 'text') and response2.text:
                return response2.text
        except Exception:
            pass
        # ìµœì¢… í´ë°±: ë‹¤ë¥¸ ëª¨ë¸ ì‹œë„(ê°€ëŠ¥í•œ í‚¤ê°€ ìˆì„ ë•Œ)
        try:
            if settings.OPENAI_API_KEY:
                return await get_openai_completion(prompt, model='gpt-4o', max_tokens=1024)
        except Exception:
            pass
        try:
            if settings.CLAUDE_API_KEY:
                # Claude í´ë°±ì€ Claude 4 ì´ìƒë§Œ ì‚¬ìš©(3.x ì§€ì› ì¢…ë£Œ ëŒ€ì‘)
                return await get_claude_completion(prompt, model=CLAUDE_MODEL_PRIMARY, max_tokens=1024)
        except Exception:
            pass
        return "ì•ˆì „ ì •ì±…ì— ì˜í•´ ì´ ìš”ì²­ì˜ ì‘ë‹µì´ ì œí•œë˜ì—ˆìŠµë‹ˆë‹¤. í‘œí˜„ì„ ì¡°ê¸ˆ ë°”ê¿” ë‹¤ì‹œ ì‹œë„í•´ ì£¼ì„¸ìš”."
    except Exception as e:
        # ì‹¤ì œ ìš´ì˜ í™˜ê²½ì—ì„œëŠ” ë” ìƒì„¸í•œ ë¡œê¹… ë° ì˜ˆì™¸ ì²˜ë¦¬ê°€ í•„ìš”
        import logging
        logger = logging.getLogger(__name__)
        logger.error(f"Gemini API í˜¸ì¶œ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {e}")
        logger.error(f"í”„ë¡¬í”„íŠ¸ ê¸¸ì´: {len(prompt)} ë¬¸ì")
        print(f"Gemini API í˜¸ì¶œ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {e}")
        print(f"í”„ë¡¬í”„íŠ¸ ê¸¸ì´: {len(prompt)} ë¬¸ì")
        # í”„ë¡ íŠ¸ì—”ë“œì— ì „ë‹¬í•  ìˆ˜ ìˆëŠ” ì¼ë°˜ì ì¸ ì˜¤ë¥˜ ë©”ì‹œì§€ë¥¼ ë°˜í™˜í•˜ê±°ë‚˜,
        # ë³„ë„ì˜ ì˜ˆì™¸ë¥¼ ë°œìƒì‹œì¼œ API ë ˆë²¨ì—ì„œ ì²˜ë¦¬í•˜ë„ë¡ í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.
        raise ValueError(f"AI ëª¨ë¸ í˜¸ì¶œì— ì‹¤íŒ¨í–ˆìŠµë‹ˆë‹¤: {str(e)}")


async def get_gemini_completion_json(
    prompt: str,
    *,
    temperature: float = 0.7,
    max_tokens: int = 1024,
    model: str = "gemini-3-pro-preview",
) -> str:
    """
    Gemini í…ìŠ¤íŠ¸ í˜¸ì¶œì—ì„œ "JSON ì‘ë‹µ"ì„ ê°•ì œí•˜ëŠ” ì „ìš© í—¬í¼.

    ì˜ë„/ì›ë¦¬(ì¤‘ìš”):
    - `response_mime_type`ëŠ” "ì´ë¯¸ì§€/ë¹„ì „"ê³¼ ë¬´ê´€í•˜ë©°, ì¶œë ¥ í¬ë§·(ì˜ˆ: JSON) ê°•ì œ ìš©ë„ë‹¤.
    - `get_gemini_completion()`ì€ ê³µìš©(ì „ì—­) í•¨ìˆ˜ë¼ ë™ì‘ ë³€ê²½ì´ ìœ„í—˜í•˜ë¯€ë¡œ,
      ìºë¦­í„° ìƒì„±(QuickMeet/ìœ„ì €ë“œ ìë™ìƒì„±)ì²˜ëŸ¼ "êµ¬ì¡°í™” JSON ì‘ë‹µ"ì´ í•„ìš”í•œ ê²½ë¡œì—ì„œë§Œ ì´ í•¨ìˆ˜ë¥¼ ì‚¬ìš©í•œë‹¤.
    - SDK/í™˜ê²½ì— ë”°ë¼ response_mime_type ë¯¸ì§€ì›(TypeError)ì´ ìˆì„ ìˆ˜ ìˆìœ¼ë¯€ë¡œ,
      ê·¸ ê²½ìš°ì—ëŠ” ì¼ë°˜ GenerationConfigë¡œ í˜¸ì¶œí•œë‹¤(í˜¸ì¶œ ìì²´ëŠ” ìœ ì§€). íŒŒì‹±/ì •ì œëŠ” í˜¸ì¶œë¶€ì—ì„œ ê³„ì† ë°©ì–´í•œë‹¤.
    """
    try:
        gemini_model = genai.GenerativeModel(model)

        # âœ… JSON ëª¨ë“œ: ì§€ì›ë˜ëŠ” í™˜ê²½ì—ì„œë§Œ í™œì„±í™”
        try:
            generation_config = genai.types.GenerationConfig(
                temperature=temperature,
                max_output_tokens=max_tokens,
                response_mime_type="application/json",
            )
        except TypeError:
            generation_config = genai.types.GenerationConfig(
                temperature=temperature,
                max_output_tokens=max_tokens,
            )

        response = await gemini_model.generate_content_async(
            prompt,
            generation_config=generation_config,
        )

        # ì•ˆì „í•œ í…ìŠ¤íŠ¸ ì¶”ì¶œ (get_gemini_completionê³¼ ë™ì¼í•œ ë°©ì–´ ë¡œì§)
        try:
            if hasattr(response, "text") and response.text:
                return response.text
        except Exception:
            pass

        try:
            candidates = getattr(response, "candidates", []) or []
            for cand in candidates:
                content = getattr(cand, "content", None)
                if not content:
                    continue
                parts = getattr(content, "parts", []) or []
                text_parts = [getattr(p, "text", "") for p in parts if getattr(p, "text", "")]
                joined = "".join(text_parts).strip()
                if joined:
                    return joined
        except Exception:
            pass

        return ""
    except Exception as e:
        try:
            logger.error(f"Gemini(JSON) API í˜¸ì¶œ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {e}")
        except Exception:
            pass
        raise ValueError(f"AI ëª¨ë¸ í˜¸ì¶œì— ì‹¤íŒ¨í–ˆìŠµë‹ˆë‹¤: {str(e)}")


async def get_gemini_completion_stream(prompt: str, temperature: float = 0.7, max_tokens: int = 1024, model: str = 'gemini-1.5-pro'):
    """Gemini ëª¨ë¸ì˜ ìŠ¤íŠ¸ë¦¬ë° ì‘ë‹µì„ ë¹„ë™ê¸° ì œë„ˆë ˆì´í„°ë¡œ ë°˜í™˜í•©ë‹ˆë‹¤."""
    try:
        gemini_model = genai.GenerativeModel(model)
        generation_config = genai.types.GenerationConfig(
            temperature=temperature,
            max_output_tokens=max_tokens
        )
        response_stream = await gemini_model.generate_content_async(
            prompt,
            generation_config=generation_config,
            stream=True
        )
        async for chunk in response_stream:
            if chunk.text:
                yield chunk.text
    except Exception as e:
        print(f"Gemini Stream API í˜¸ì¶œ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {e}")
        yield f"ì˜¤ë¥˜: Gemini ëª¨ë¸ í˜¸ì¶œì— ì‹¤íŒ¨í–ˆìŠµë‹ˆë‹¤ - {str(e)}"

async def get_claude_completion(
    prompt: str,
    temperature: float = 0.7,
    max_tokens: int = 1800,
    model: str = CLAUDE_MODEL_PRIMARY,
    image_base64: str | None = None,
    image_mime: str | None = None,
    system_prompt: str | None = None,
) -> str:
    """
    ì£¼ì–´ì§„ í”„ë¡¬í”„íŠ¸ë¡œ Anthropic Claude ëª¨ë¸ì„ í˜¸ì¶œí•˜ì—¬ ì‘ë‹µì„ ë°˜í™˜í•©ë‹ˆë‹¤.
    ì´ë¯¸ì§€ê°€ ìˆì„ ê²½ìš° Vision ê¸°ëŠ¥ì„ ì‚¬ìš©í•©ë‹ˆë‹¤.
    """
    try:
        # âœ… system prompt(ìš°ì„ ìˆœìœ„ ë†’ìŒ) ë¶„ë¦¬ ì§€ì›
        # - ê¸°ì¡´ êµ¬í˜„ì€ ëª¨ë“  ì§€ì‹œ/ì„¤ì •ì„ user prompt í•œ ë©ì–´ë¦¬ë¡œ ë³´ë‚´ drift(ê·œì¹™ ì´íƒˆ)ê°€ ë°œìƒí•  ìˆ˜ ìˆì—ˆë‹¤.
        # - ìµœì†Œ ìˆ˜ì •ìœ¼ë¡œ system=... ì„ ì‚¬ìš©í•˜ë©´ ìºë¦­í„°/ê·œì¹™ ê³ ì •ë ¥ì´ ê°•í•´ì§„ë‹¤.
        try:
            sys_text = (system_prompt or "").strip()
        except Exception:
            sys_text = ""

        # ë©”ì‹œì§€ ì½˜í…ì¸  êµ¬ì„±
        if image_base64:
            content = [
                {
                    "type": "image",
                    "source": {
                        "type": "base64",
                        "media_type": (image_mime or "image/jpeg"),
                        "data": image_base64
                    }
                },
                {
                    "type": "text",
                    "text": prompt
                }
            ]
        else:
            content = prompt

        kwargs = {
            "model": model,
            "max_tokens": max_tokens,
            "temperature": temperature,
            "messages": [{"role": "user", "content": content}],
        }
        if sys_text:
            kwargs["system"] = sys_text

        # âœ… ì‹¤ì œ í˜¸ì¶œ(ì‹œë„) ë¡œê·¸: Anthropic SDKê°€ ë‚´ë¶€ì ìœ¼ë¡œ https://api.anthropic.com/v1/messages ë¥¼ í˜¸ì¶œí•œë‹¤.
        # - í”„ë¡¬í”„íŠ¸/ëŒ€ì‚¬ ë‚´ìš©ì€ ì ˆëŒ€ ë¡œê·¸ì— ë‚¨ê¸°ì§€ ì•ŠëŠ”ë‹¤.
        try:
            if getattr(settings, "DEBUG", False) or getattr(settings, "ENVIRONMENT", "") != "production":
                logger.info(f"[ai] http_call provider=claude sdk=anthropic.messages.create model={model} max_tokens={max_tokens} temp={temperature}")
        except Exception:
            pass

        message = await claude_client.messages.create(**kwargs)

        # 1) SDKê°€ Message ê°ì²´ë¥¼ ëŒë ¤ì£¼ëŠ” ì¼ë°˜ì ì¸ ê²½ìš°
        if hasattr(message, "content"):
            text = message.content[0].text
            # UTF-8 ì¸ì½”ë”© ë³´ì¥
            if isinstance(text, bytes):
                text = text.decode('utf-8', errors='replace')
            return text

        # 2) ì–´ë–¤ ì´ìœ ë¡œ ë¬¸ìì—´ë§Œ ëŒë ¤ì¤€ ê²½ìš°
        if isinstance(message, str):
            # UTF-8 ì¸ì½”ë”© ë³´ì¥
            if isinstance(message, bytes):
                return message.decode('utf-8', errors='replace')
            return message

        # 3) dict í˜•íƒœ(HTTP ì‘ë‹µ JSON)ë¡œ ëŒë ¤ì¤€ ê²½ìš°
        if isinstance(message, dict):
            # {'content': [{'text': '...'}], ...} í˜•íƒœë¥¼ ê¸°ëŒ€
            content = message.get("content")
            if isinstance(content, list) and content and isinstance(content[0], dict):
                text = content[0].get("text", "")
                # UTF-8 ì¸ì½”ë”© ë³´ì¥
                if isinstance(text, bytes):
                    text = text.decode('utf-8', errors='replace')
                return text
            result = str(message)
            if isinstance(result, bytes):
                result = result.decode('utf-8', errors='replace')
            return result

        # ê·¸ ë°–ì˜ ì˜ˆìƒì¹˜ ëª»í•œ íƒ€ì…ì€ ë¬¸ìì—´ë¡œ ê°•ì œ ë³€í™˜
        result = str(message)
        if isinstance(result, bytes):
            result = result.decode('utf-8', errors='replace')
        return result

    except Exception as e:
        print(f"Claude API í˜¸ì¶œ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {e}")
        raise ValueError(f"Claude API í˜¸ì¶œì— ì‹¤íŒ¨í–ˆìŠµë‹ˆë‹¤: {e}")

async def get_claude_completion_stream(
    prompt: str,
    temperature: float = 0.7,
    max_tokens: int = 1024,
    model: str = CLAUDE_MODEL_PRIMARY,
    system_prompt: str | None = None,
):
    """Claude ëª¨ë¸ì˜ ìŠ¤íŠ¸ë¦¬ë° ì‘ë‹µì„ ë¹„ë™ê¸° ì œë„ˆë ˆì´í„°ë¡œ ë°˜í™˜í•©ë‹ˆë‹¤."""
    try:
        try:
            sys_text = (system_prompt or "").strip()
        except Exception:
            sys_text = ""

        kwargs = {
            "model": model,
            "max_tokens": max_tokens,
            "temperature": temperature,
            "messages": [{"role": "user", "content": prompt}],
        }
        if sys_text:
            kwargs["system"] = sys_text

        async with claude_client.messages.stream(**kwargs) as stream:
            async for text in stream.text_stream:
                yield text
    except Exception as e:
        print(f"Claude Stream API í˜¸ì¶œ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {e}")
        yield f"ì˜¤ë¥˜: Claude ëª¨ë¸ í˜¸ì¶œì— ì‹¤íŒ¨í–ˆìŠµë‹ˆë‹¤ - {str(e)}"

async def get_openai_completion(
    prompt: str,
    temperature: float = 0.7,
    max_tokens: int = 1024,
    model: str = "gpt-4o",
    system_prompt: str | None = None,
) -> str:
    """
    ì£¼ì–´ì§„ í”„ë¡¬í”„íŠ¸ë¡œ OpenAI ëª¨ë¸ì„ í˜¸ì¶œí•˜ì—¬ ì‘ë‹µì„ ë°˜í™˜í•©ë‹ˆë‹¤.
    """
    try:
        from openai import AsyncOpenAI
        client = AsyncOpenAI(api_key=settings.OPENAI_API_KEY)
        # âœ… ì‹¤ì œ í˜¸ì¶œ(ì‹œë„) ë¡œê·¸: OpenAIëŠ” ëª¨ë¸ì— ë”°ë¼ responses/chat.completionsë¡œ ë¶„ê¸°ëœë‹¤.
        # - í”„ë¡¬í”„íŠ¸/ëŒ€ì‚¬ ë‚´ìš©ì€ ì ˆëŒ€ ë¡œê·¸ì— ë‚¨ê¸°ì§€ ì•ŠëŠ”ë‹¤.
        try:
            if getattr(settings, "DEBUG", False) or getattr(settings, "ENVIRONMENT", "") != "production":
                logger.info(f"[ai] http_call provider=openai enter model={model} max_tokens={max_tokens} temp={temperature}")
        except Exception:
            pass

        def _supports_responses_api(_client: object) -> bool:
            """í˜„ì¬ ì„¤ì¹˜ëœ OpenAI Python SDKê°€ Responses APIë¥¼ ì§€ì›í•˜ëŠ”ì§€ í™•ì¸í•œë‹¤.

            ë°°ê²½/ì˜ë„:
            - ì¼ë¶€ í™˜ê²½(êµ¬ë²„ì „ SDK)ì—ì„œëŠ” AsyncOpenAIì— .responsesê°€ ì—†ì–´ AttributeErrorê°€ ë°œìƒí•œë‹¤.
            - íŒ¨í‚¤ì§€ ì—…ê·¸ë ˆì´ë“œ ì—†ì´ë„ GPT-5.xë¥¼ ì“¸ ìˆ˜ ìˆë„ë¡, ë¯¸ì§€ì› ì‹œ REST(/v1/responses)ë¡œ í´ë°±í•œë‹¤.
            """
            try:
                r = getattr(_client, "responses", None)
                return bool(r and hasattr(r, "create"))
            except Exception:
                return False

        def _reasoning_effort_for_model(model_name: str) -> str | None:
            """GPT-5.1/5.2ëŠ” reasoning effortë¥¼ 'medium'ìœ¼ë¡œ ê°•ì œí•œë‹¤."""
            try:
                m = (model_name or "").strip().lower()
            except Exception:
                m = ""
            if m.startswith("gpt-5.1") or m.startswith("gpt-5.2"):
                return "medium"
            return None

        def _style_instruction_for_temperature(temp_value: float) -> str | None:
            """GPT-5.x(Responses API)ì—ì„œ temperature íŒŒë¼ë¯¸í„°ê°€ ë§‰íŒ ê²½ìš°ë¥¼ ëŒ€ë¹„í•´ ìŠ¤íƒ€ì¼ ì§€ì¹¨ìœ¼ë¡œ ì˜¨ë„ë¥¼ ë°˜ì˜í•œë‹¤.

            ì˜ë„/ë™ì‘:
            - í”„ë¡ íŠ¸ ìŠ¬ë¼ì´ë”ëŠ” 0.0~1.0 ë²”ìœ„(0.1 step)ì´ë©°, ë‹¤ë¥¸ ëª¨ë¸(Gemini/Claude/GPT-4 ê³„ì—´)ì€
              temperature íŒŒë¼ë¯¸í„°ë¡œ ì§ì ‘ ë°˜ì˜ëœë‹¤.
            - GPT-5.x(Responses API)ëŠ” ì¼ë¶€ í™˜ê²½ì—ì„œ temperature íŒŒë¼ë¯¸í„°ê°€ ë¯¸ì§€ì›(400)ì´ë¼,
              user ì„¤ì • ì˜¨ë„ë¥¼ developer ì§€ì¹¨ìœ¼ë¡œ ë³€í™˜í•´ "ëŒ€í™” ìŠ¤íƒ€ì¼"ì„ ê°„ì ‘ ì œì–´í•œë‹¤.

            ë§¤í•‘:
            - 0.0ì— ê°€ê¹Œìš¸ìˆ˜ë¡: ì„¤ì •/ìš”ì²­ì— ì¶©ì‹¤, ë³´ìˆ˜ì /ì¼ê´€ì 
            - 1.0ì— ê°€ê¹Œìš¸ìˆ˜ë¡: í‘œí˜„ì´ ì°½ì˜ì /ë‹¤ì–‘
            """
            try:
                t = float(temp_value)
            except Exception:
                return None
            # 0~1 í´ë¨í•‘ + 0.1 step ì •í•©(í”„ë¡ íŠ¸/ë°±ì—”ë“œ ì»¨ë²¤ì…˜)
            try:
                if t < 0:
                    t = 0.0
                if t > 1:
                    t = 1.0
                t = round(t * 10) / 10.0
            except Exception:
                return None

            # êµ¬ê°„ë³„ ê°€ì´ë“œ(ë„ˆë¬´ ì¥í™©í•˜ì§€ ì•Šê²Œ)
            if t <= 0.2:
                band = "ë§¤ìš° ì„¤ì •ì— ì¶©ì‹¤(ë³´ìˆ˜ì )"
                guidance = "ì„¤ì •/ëŒ€í™” ë§¥ë½ì—ì„œ ë²—ì–´ë‚˜ëŠ” ìƒìƒ/ì¶”ì¸¡ì„ ìµœëŒ€í•œ ì¤„ì´ê³ , ê°„ê²°í•˜ê³  ì¼ê´€ë˜ê²Œ ë‹µí•˜ì„¸ìš”."
            elif t <= 0.5:
                band = "ì„¤ì • ìš°ì„ (ì•ˆì •ì )"
                guidance = "ì„¤ì •/ëŒ€í™” ë§¥ë½ì„ ìš°ì„ í•˜ë˜, í‘œí˜„ì€ ìì—°ìŠ¤ëŸ½ê²Œ ë‹¤ë“¬ì–´ ë‹µí•˜ì„¸ìš”."
            elif t <= 0.8:
                band = "ê· í˜•(ì ë‹¹íˆ ì°½ì˜ì )"
                guidance = "í‘œí˜„ì„ ì¡°ê¸ˆ ë” í’ë¶€í•˜ê²Œ í•˜ë˜, ì„¤ì •/ìºë¦­í„° ì„±ê²©/ëŒ€í™” ë§¥ë½ì„ ì ˆëŒ€ ê¹¨ì§€ ë§ˆì„¸ìš”."
            else:
                band = "ë§¤ìš° ì°½ì˜ì (ë‹¤ì–‘í•œ í‘œí˜„)"
                guidance = "í‘œí˜„/ë¹„ìœ /ë¬˜ì‚¬ë¥¼ ë” ì°½ì˜ì ìœ¼ë¡œ í•˜ë˜, ì„¤ì •ì„ ë°”ê¾¸ê±°ë‚˜ ìƒˆ ì‚¬ì‹¤ì„ ë‹¨ì •í•´ ë§Œë“¤ì§€ ë§ˆì„¸ìš”."

            return (
                "ëŒ€í™” ìŠ¤íƒ€ì¼(ì˜¨ë„) ì§€ì¹¨:\n"
                f"- ì˜¨ë„: {t:.1f} (0.0=ì„¤ì •/ìš”ì²­ì— ë§¤ìš° ì¶©ì‹¤, 1.0=ì°½ì˜ì /ë‹¤ì–‘)\n"
                f"- í˜„ì¬ ìŠ¤íƒ€ì¼: {band}\n"
                f"- ì§€ì¹¨: {guidance}\n"
                "- ê³µí†µ ê·œì¹™: ì„¤ì •/ëŒ€í™” ë§¥ë½/ìºë¦­í„° ì„±ê²©ì„ ì„ì˜ë¡œ ë³€ê²½í•˜ê±°ë‚˜ ìƒˆ ì„¤ì •ì„ ë‹¨ì •í•´ ì¶”ê°€í•˜ì§€ ë§ˆì„¸ìš”."
            )

        def _build_responses_input(
            user_prompt: str,
            *,
            style_instruction: str | None = None,
            system_prompt: str | None = None,
        ) -> list[dict]:
            """Responses APIì˜ input í¬ë§·ìœ¼ë¡œ ë³€í™˜í•œë‹¤.

            NOTE(ì¤‘ìš”):
            - ê¸°ì¡´ êµ¬í˜„ì€ prompt ì „ì²´ë¥¼ user 1ê°œë¡œ ë³´ë‚´ drift(ê·œì¹™ ì´íƒˆ)ê°€ ìƒê¸¸ ìˆ˜ ìˆì—ˆë‹¤.
            - GPT-5(Responses API)ì—ì„œëŠ” system/developerê°€ userë³´ë‹¤ ìš°ì„ í•˜ë¯€ë¡œ,
              character/system í”„ë¡¬í”„íŠ¸ë¥¼ developerë¡œ ë¶„ë¦¬í•´ ê³ ì •ë ¥ì„ ë†’ì¸ë‹¤(ìµœì†Œ ìˆ˜ì •).
            """
            try:
                p = "" if user_prompt is None else str(user_prompt)
            except Exception:
                p = ""
            items: list[dict] = []
            try:
                s = (style_instruction or "").strip()
            except Exception:
                s = ""
            if s:
                # GPT-5 ê³„ì—´ì€ developer ì§€ì¹¨ìœ¼ë¡œ ìŠ¤íƒ€ì¼ì„ ê°„ì ‘ ì œì–´(temperature ë¯¸ì§€ì› ëŒ€ì‘)
                items.append({"role": "developer", "content": s})
            try:
                sp = (system_prompt or "").strip()
            except Exception:
                sp = ""
            if sp:
                # âœ… ìºë¦­í„°/ê·œì¹™ ê³ ì •(ìš°ì„ ìˆœìœ„â†‘): developerë¡œ ë„£ì–´ user í”„ë¡¬í”„íŠ¸ë³´ë‹¤ ê°•í•˜ê²Œ ì ìš©
                items.append({"role": "developer", "content": sp})
            items.append({"role": "user", "content": p})
            return items

        def _extract_responses_text(resp: object) -> str:
            """Responses API ì‘ë‹µ(SDK ê°ì²´ ë˜ëŠ” dict)ì—ì„œ í…ìŠ¤íŠ¸ë¥¼ ë°©ì–´ì ìœ¼ë¡œ ì¶”ì¶œí•œë‹¤."""
            # 1) SDK convenience: output_text
            try:
                out_txt = resp.get("output_text") if isinstance(resp, dict) else getattr(resp, "output_text", None)
                if isinstance(out_txt, str) and out_txt.strip():
                    return out_txt
            except Exception:
                pass

            # 2) output ë°°ì—´ì˜ message/output_text ìˆ˜ì§‘
            try:
                outputs = resp.get("output") if isinstance(resp, dict) else getattr(resp, "output", None)
                texts: list[str] = []
                if isinstance(outputs, list):
                    for item in outputs:
                        it_type = getattr(item, "type", None) if not isinstance(item, dict) else item.get("type")
                        if it_type != "message":
                            continue
                        content = getattr(item, "content", None) if not isinstance(item, dict) else item.get("content")
                        if not isinstance(content, list):
                            continue
                        for part in content:
                            p_type = getattr(part, "type", None) if not isinstance(part, dict) else part.get("type")
                            if p_type == "output_text":
                                txt = getattr(part, "text", None) if not isinstance(part, dict) else part.get("text")
                                if isinstance(txt, str) and txt:
                                    texts.append(txt)
                            elif p_type == "refusal":
                                refusal = getattr(part, "refusal", None) if not isinstance(part, dict) else part.get("refusal")
                                if isinstance(refusal, str) and refusal:
                                    texts.append(refusal)
                joined = "".join(texts).strip()
                return joined
            except Exception:
                return ""

        async def _responses_rest_create(
            *,
            model_name: str,
            user_prompt: str,
            temp: float,
            max_out_tokens: int,
            reasoning_effort: str | None,
            system_prompt: str | None = None,
        ) -> str:
            """SDKì— responsesê°€ ì—†ì„ ë•Œ OpenAI Responses REST APIë¥¼ ì§ì ‘ í˜¸ì¶œí•´ í…ìŠ¤íŠ¸ë¥¼ ë°˜í™˜í•œë‹¤."""
            import os
            import json
            import aiohttp

            api_key = settings.OPENAI_API_KEY or os.getenv("OPENAI_API_KEY")
            if not api_key:
                raise ValueError("OPENAI_API_KEYê°€ ì„¤ì •ë˜ì–´ ìˆì§€ ì•ŠìŠµë‹ˆë‹¤.")

            base = (os.getenv("OPENAI_BASE_URL") or "https://api.openai.com/v1").rstrip("/")
            url = f"{base}/responses"

            payload: dict = {
                "model": model_name,
                "input": _build_responses_input(
                    user_prompt,
                    style_instruction=_style_instruction_for_temperature(temp),
                    system_prompt=system_prompt,
                ),
                "max_output_tokens": int(max_out_tokens),
            }
            if reasoning_effort:
                payload["reasoning"] = {"effort": reasoning_effort}

            headers = {
                "Authorization": f"Bearer {api_key}",
                "Content-Type": "application/json",
            }

            timeout = aiohttp.ClientTimeout(total=120)
            async with aiohttp.ClientSession(timeout=timeout) as session:
                async with session.post(url, headers=headers, json=payload) as resp:
                    raw = await resp.read()
                    txt = raw.decode("utf-8", errors="replace") if isinstance(raw, (bytes, bytearray)) else str(raw)
                    try:
                        data = json.loads(txt) if isinstance(txt, str) else {}
                    except Exception:
                        data = {"_raw": txt}

                    if resp.status >= 400:
                        try:
                            logger.error(f"OpenAI Responses REST error {resp.status}: {txt[:800]}")
                        except Exception:
                            pass
                        raise ValueError(f"OpenAI Responses API error {resp.status}")

                    extracted = _extract_responses_text(data)
                    if extracted:
                        return extracted

                    # ë°©ì–´ì  í´ë°±:
                    # - ì¼ë¶€ ì¼€ì´ìŠ¤(íŠ¹íˆ reasoning ëª¨ë¸ì—ì„œ max_output_tokensê°€ ë„ˆë¬´ ì‘ì„ ë•Œ)ëŠ”
                    #   outputì´ reasoningë§Œ ì±„ì›Œì§€ê³  message/output_textê°€ ì•„ì˜ˆ ì—†ì„ ìˆ˜ ìˆë‹¤.
                    # - ì´ë•Œ JSON ì›ë¬¸ì„ ì‚¬ìš©ìì—ê²Œ ê·¸ëŒ€ë¡œ ë…¸ì¶œí•˜ë©´ UXê°€ í¬ê²Œ ê¹¨ì§€ë¯€ë¡œ,
                    #   1íšŒì— í•œí•´ ì¶œë ¥ í† í°ì„ ëŠ˜ë ¤ ì¬ì‹œë„(ë¹„ìš©/ì‹œê°„ ê³ ë ¤í•´ ì œí•œ) í›„,
                    #   ê·¸ë˜ë„ ì‹¤íŒ¨í•˜ë©´ ì‚¬ìš©ì ì¹œí™” ë©”ì‹œì§€ë¥¼ ë°˜í™˜í•œë‹¤.
                    try:
                        reason = ((data or {}).get("incomplete_details") or {}).get("reason")
                        outputs = (data or {}).get("output") or []
                        has_message = False
                        if isinstance(outputs, list):
                            for it in outputs:
                                it_type = it.get("type") if isinstance(it, dict) else getattr(it, "type", None)
                                if it_type == "message":
                                    has_message = True
                                    break
                        if (not has_message) and reason == "max_output_tokens" and int(max_out_tokens) < 1024:
                            # 1íšŒ ì¬ì‹œë„: 1024ë¡œ ìƒí–¥(ë¬´í•œ ì¬ì‹œë„ ë°©ì§€)
                            return await _responses_rest_create(
                                model_name=model_name,
                                user_prompt=user_prompt,
                                temp=temp,
                                max_out_tokens=1024,
                                reasoning_effort=reasoning_effort,
                            )
                    except Exception:
                        pass

                    try:
                        logger.error(
                            f"OpenAI Responses REST: output_text extraction failed (model={model_name}, reason={(data or {}).get('incomplete_details')})"
                        )
                    except Exception:
                        pass
                    return "OpenAI ì‘ë‹µì„ ìƒì„±í–ˆì§€ë§Œ í…ìŠ¤íŠ¸ë¥¼ ì¶”ì¶œí•˜ì§€ ëª»í–ˆìŠµë‹ˆë‹¤. ì ì‹œ í›„ ë‹¤ì‹œ ì‹œë„í•´ ì£¼ì„¸ìš”."

        def _use_responses_api(model_name: str) -> bool:
            """GPT-5/o-series ë“± ìµœì‹  ëª¨ë¸ì€ Responses APIê°€ ê¶Œì¥ì´ë¼ ë¶„ê¸°í•œë‹¤.

            ë°°ê²½:
            - ê¸°ì¡´ Chat Completionsë„ ë™ì‘í•  ìˆ˜ ìˆì§€ë§Œ, GPT-5 ê³„ì—´ì€ Responsesì—ì„œ ê¸°ëŠ¥/ì„±ëŠ¥(Reasoning ë“±) ì •í•©ì´ ë” ì¢‹ë‹¤.
            - ê¸°ì¡´ GPT-4 ê³„ì—´ì€ í˜„ì¬ ì½”ë“œì˜ chat.completions ê²½ë¡œë¥¼ ê·¸ëŒ€ë¡œ ìœ ì§€í•´ ë¦¬ìŠ¤í¬ë¥¼ ì¤„ì¸ë‹¤.
            """
            try:
                m = (model_name or "").strip().lower()
            except Exception:
                m = ""
            return m.startswith("gpt-5") or m.startswith("o")

        # GPT-5 ê³„ì—´: Responses API ì‚¬ìš© (ê¶Œì¥)
        if _use_responses_api(model):
            effort = _reasoning_effort_for_model(model)
            if _supports_responses_api(client):
                try:
                    if getattr(settings, "DEBUG", False) or getattr(settings, "ENVIRONMENT", "") != "production":
                        logger.info(f"[ai] http_call provider=openai api=responses sdk model={model}")
                except Exception:
                    pass
                try:
                    sp = (system_prompt or "").strip()
                except Exception:
                    sp = ""
                kwargs = {
                    "model": model,
                    "input": _build_responses_input(
                        prompt,
                        style_instruction=_style_instruction_for_temperature(temperature),
                        system_prompt=sp or None,
                    ),
                    "max_output_tokens": max_tokens,
                }
                if effort:
                    kwargs["reasoning"] = {"effort": effort}
                resp = await client.responses.create(**kwargs)
                extracted = _extract_responses_text(resp)
                if extracted:
                    return extracted
                return "OpenAI ì‘ë‹µì„ ìƒì„±í–ˆì§€ë§Œ í…ìŠ¤íŠ¸ë¥¼ ì¶”ì¶œí•˜ì§€ ëª»í–ˆìŠµë‹ˆë‹¤. ì ì‹œ í›„ ë‹¤ì‹œ ì‹œë„í•´ ì£¼ì„¸ìš”."

            # âœ… SDK ë¯¸ì§€ì› í´ë°±: REST(/v1/responses)
            try:
                if getattr(settings, "DEBUG", False) or getattr(settings, "ENVIRONMENT", "") != "production":
                    logger.info(f"[ai] http_call provider=openai api=responses rest model={model}")
            except Exception:
                pass
            try:
                sp = (system_prompt or "").strip()
            except Exception:
                sp = ""
            extracted = await _responses_rest_create(
                model_name=model,
                user_prompt=prompt,
                temp=temperature,
                max_out_tokens=max_tokens,
                reasoning_effort=effort,
                system_prompt=sp or None,
            )
            if extracted:
                return extracted
            return "OpenAI ì‘ë‹µì„ ìƒì„±í–ˆì§€ë§Œ í…ìŠ¤íŠ¸ë¥¼ ì¶”ì¶œí•˜ì§€ ëª»í–ˆìŠµë‹ˆë‹¤. ì ì‹œ í›„ ë‹¤ì‹œ ì‹œë„í•´ ì£¼ì„¸ìš”."

        # GPT-4 ê³„ì—´(ê¸°ì¡´): Chat Completions ìœ ì§€
        try:
            if getattr(settings, "DEBUG", False) or getattr(settings, "ENVIRONMENT", "") != "production":
                logger.info(f"[ai] http_call provider=openai api=chat.completions sdk model={model}")
        except Exception:
            pass
        try:
            sp = (system_prompt or "").strip()
        except Exception:
            sp = ""
        messages = [{"role": "user", "content": prompt}]
        if sp:
            # âœ… GPT-4 ê³„ì—´: system role ë¶„ë¦¬ë¡œ ê·œì¹™/ìºë¦­í„° ê³ ì •ë ¥ ê°•í™”
            messages = [{"role": "system", "content": sp}, {"role": "user", "content": prompt}]

        response = await client.chat.completions.create(
            model=model,
            messages=messages,
            temperature=temperature,
            max_tokens=max_tokens
        )
        return response.choices[0].message.content
    except Exception as e:
        try:
            logger.error(f"OpenAI API í˜¸ì¶œ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {e} (model={model}, prompt_len={len(prompt) if isinstance(prompt, str) else 'n/a'})")
        except Exception:
            pass
        print(f"OpenAI API í˜¸ì¶œ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {e}")
        raise ValueError(f"OpenAI API í˜¸ì¶œì— ì‹¤íŒ¨í–ˆìŠµë‹ˆë‹¤: {e}")

async def get_openai_completion_stream(
    prompt: str,
    temperature: float = 0.7,
    max_tokens: int = 1024,
    model: str = "gpt-4o",
    system_prompt: str | None = None,
):
    """OpenAI ëª¨ë¸ì˜ ìŠ¤íŠ¸ë¦¬ë° ì‘ë‹µì„ ë¹„ë™ê¸° ì œë„ˆë ˆì´í„°ë¡œ ë°˜í™˜í•©ë‹ˆë‹¤."""
    try:
        from openai import AsyncOpenAI
        client = AsyncOpenAI(api_key=settings.OPENAI_API_KEY)

        def _supports_responses_api(_client: object) -> bool:
            """í˜„ì¬ ì„¤ì¹˜ëœ OpenAI Python SDKê°€ Responses API ìŠ¤íŠ¸ë¦¬ë°ì„ ì§€ì›í•˜ëŠ”ì§€ í™•ì¸í•œë‹¤."""
            try:
                r = getattr(_client, "responses", None)
                return bool(r and hasattr(r, "create"))
            except Exception:
                return False

        def _reasoning_effort_for_model(model_name: str) -> str | None:
            """GPT-5.1/5.2ëŠ” reasoning effortë¥¼ 'medium'ìœ¼ë¡œ ê°•ì œí•œë‹¤."""
            try:
                m = (model_name or "").strip().lower()
            except Exception:
                m = ""
            if m.startswith("gpt-5.1") or m.startswith("gpt-5.2"):
                return "medium"
            return None

        def _style_instruction_for_temperature(temp_value: float) -> str | None:
            """GPT-5.x(Responses API)ì—ì„œ temperature íŒŒë¼ë¯¸í„° ë¯¸ì§€ì› ì‹œ, ìŠ¤íƒ€ì¼ ì§€ì¹¨ìœ¼ë¡œ ì˜¨ë„ë¥¼ ë°˜ì˜í•œë‹¤."""
            try:
                t = float(temp_value)
            except Exception:
                return None
            try:
                if t < 0:
                    t = 0.0
                if t > 1:
                    t = 1.0
                t = round(t * 10) / 10.0
            except Exception:
                return None

            if t <= 0.2:
                band = "ë§¤ìš° ì„¤ì •ì— ì¶©ì‹¤(ë³´ìˆ˜ì )"
                guidance = "ì„¤ì •/ëŒ€í™” ë§¥ë½ì—ì„œ ë²—ì–´ë‚˜ëŠ” ìƒìƒ/ì¶”ì¸¡ì„ ìµœëŒ€í•œ ì¤„ì´ê³ , ê°„ê²°í•˜ê³  ì¼ê´€ë˜ê²Œ ë‹µí•˜ì„¸ìš”."
            elif t <= 0.5:
                band = "ì„¤ì • ìš°ì„ (ì•ˆì •ì )"
                guidance = "ì„¤ì •/ëŒ€í™” ë§¥ë½ì„ ìš°ì„ í•˜ë˜, í‘œí˜„ì€ ìì—°ìŠ¤ëŸ½ê²Œ ë‹¤ë“¬ì–´ ë‹µí•˜ì„¸ìš”."
            elif t <= 0.8:
                band = "ê· í˜•(ì ë‹¹íˆ ì°½ì˜ì )"
                guidance = "í‘œí˜„ì„ ì¡°ê¸ˆ ë” í’ë¶€í•˜ê²Œ í•˜ë˜, ì„¤ì •/ìºë¦­í„° ì„±ê²©/ëŒ€í™” ë§¥ë½ì„ ì ˆëŒ€ ê¹¨ì§€ ë§ˆì„¸ìš”."
            else:
                band = "ë§¤ìš° ì°½ì˜ì (ë‹¤ì–‘í•œ í‘œí˜„)"
                guidance = "í‘œí˜„/ë¹„ìœ /ë¬˜ì‚¬ë¥¼ ë” ì°½ì˜ì ìœ¼ë¡œ í•˜ë˜, ì„¤ì •ì„ ë°”ê¾¸ê±°ë‚˜ ìƒˆ ì‚¬ì‹¤ì„ ë‹¨ì •í•´ ë§Œë“¤ì§€ ë§ˆì„¸ìš”."

            return (
                "ëŒ€í™” ìŠ¤íƒ€ì¼(ì˜¨ë„) ì§€ì¹¨:\n"
                f"- ì˜¨ë„: {t:.1f} (0.0=ì„¤ì •/ìš”ì²­ì— ë§¤ìš° ì¶©ì‹¤, 1.0=ì°½ì˜ì /ë‹¤ì–‘)\n"
                f"- í˜„ì¬ ìŠ¤íƒ€ì¼: {band}\n"
                f"- ì§€ì¹¨: {guidance}\n"
                "- ê³µí†µ ê·œì¹™: ì„¤ì •/ëŒ€í™” ë§¥ë½/ìºë¦­í„° ì„±ê²©ì„ ì„ì˜ë¡œ ë³€ê²½í•˜ê±°ë‚˜ ìƒˆ ì„¤ì •ì„ ë‹¨ì •í•´ ì¶”ê°€í•˜ì§€ ë§ˆì„¸ìš”."
            )

        def _build_responses_input(
            user_prompt: str,
            *,
            style_instruction: str | None = None,
            system_prompt: str | None = None,
        ) -> list[dict]:
            """Responses APIì˜ input í¬ë§·ìœ¼ë¡œ ë³€í™˜í•œë‹¤.

            NOTE:
            - stream ê²½ë¡œì—ì„œë„ character/system í”„ë¡¬í”„íŠ¸ë¥¼ developerë¡œ ë¶„ë¦¬í•´ driftë¥¼ ì¤„ì¸ë‹¤.
            """
            try:
                p = "" if user_prompt is None else str(user_prompt)
            except Exception:
                p = ""
            items: list[dict] = []
            try:
                s = (style_instruction or "").strip()
            except Exception:
                s = ""
            if s:
                items.append({"role": "developer", "content": s})
            try:
                sp = (system_prompt or "").strip()
            except Exception:
                sp = ""
            if sp:
                items.append({"role": "developer", "content": sp})
            items.append({"role": "user", "content": p})
            return items

        async def _responses_rest_stream(
            *,
            model_name: str,
            user_prompt: str,
            temp: float,
            max_out_tokens: int,
            reasoning_effort: str | None,
            system_prompt: str | None = None,
        ):
            """SDKì— responsesê°€ ì—†ì„ ë•Œ OpenAI Responses REST ìŠ¤íŠ¸ë¦¬ë°ì„ SSEë¡œ íŒŒì‹±í•´ deltaë¥¼ yieldí•œë‹¤."""
            import os
            import json
            import aiohttp

            api_key = settings.OPENAI_API_KEY or os.getenv("OPENAI_API_KEY")
            if not api_key:
                yield "ì˜¤ë¥˜: OpenAI ëª¨ë¸ í˜¸ì¶œì— ì‹¤íŒ¨í–ˆìŠµë‹ˆë‹¤ - OPENAI_API_KEYê°€ ì„¤ì •ë˜ì–´ ìˆì§€ ì•ŠìŠµë‹ˆë‹¤."
                return

            base = (os.getenv("OPENAI_BASE_URL") or "https://api.openai.com/v1").rstrip("/")
            url = f"{base}/responses"

            payload: dict = {
                "model": model_name,
                "input": _build_responses_input(
                    user_prompt,
                    style_instruction=_style_instruction_for_temperature(temp),
                    system_prompt=system_prompt,
                ),
                "max_output_tokens": int(max_out_tokens),
                "stream": True,
            }
            if reasoning_effort:
                payload["reasoning"] = {"effort": reasoning_effort}

            headers = {
                "Authorization": f"Bearer {api_key}",
                "Content-Type": "application/json",
            }

            timeout = aiohttp.ClientTimeout(total=300)
            async with aiohttp.ClientSession(timeout=timeout) as session:
                async with session.post(url, headers=headers, json=payload) as resp:
                    if resp.status >= 400:
                        try:
                            txt = await resp.text()
                        except Exception:
                            txt = ""
                        try:
                            logger.error(f"OpenAI Responses REST stream error {resp.status}: {txt[:800]}")
                        except Exception:
                            pass
                        yield f"ì˜¤ë¥˜: OpenAI ëª¨ë¸ í˜¸ì¶œì— ì‹¤íŒ¨í–ˆìŠµë‹ˆë‹¤ - HTTP {resp.status}"
                        return

                    buf = b""
                    async for chunk in resp.content.iter_chunked(1024):
                        if not chunk:
                            continue
                        buf += chunk
                        while b"\n" in buf:
                            line, buf = buf.split(b"\n", 1)
                            line = line.strip()
                            if not line:
                                continue
                            if not line.startswith(b"data:"):
                                continue
                            data_part = line[len(b"data:"):].strip()
                            if data_part == b"[DONE]":
                                return
                            try:
                                evt = json.loads(data_part.decode("utf-8", errors="replace"))
                            except Exception:
                                continue
                            et = evt.get("type")
                            if et in ("response.output_text.delta", "response.refusal.delta"):
                                delta = evt.get("delta")
                                if isinstance(delta, str) and delta:
                                    yield delta
                            elif et == "response.error":
                                err = evt.get("error")
                                yield f"ì˜¤ë¥˜: OpenAI ëª¨ë¸ í˜¸ì¶œì— ì‹¤íŒ¨í–ˆìŠµë‹ˆë‹¤ - {err}"
                                return

        def _use_responses_api(model_name: str) -> bool:
            """GPT-5/o-series ë“± ìµœì‹  ëª¨ë¸ì€ Responses API ìŠ¤íŠ¸ë¦¬ë° ì´ë²¤íŠ¸ë¥¼ ì‚¬ìš©í•œë‹¤."""
            try:
                m = (model_name or "").strip().lower()
            except Exception:
                m = ""
            return m.startswith("gpt-5") or m.startswith("o")

        # GPT-5 ê³„ì—´: Responses API ìŠ¤íŠ¸ë¦¬ë°
        if _use_responses_api(model):
            effort = _reasoning_effort_for_model(model)
            if _supports_responses_api(client):
                try:
                    sp = (system_prompt or "").strip()
                except Exception:
                    sp = ""
                kwargs = {
                    "model": model,
                    "input": _build_responses_input(
                        prompt,
                        style_instruction=_style_instruction_for_temperature(temperature),
                        system_prompt=sp or None,
                    ),
                    "max_output_tokens": max_tokens,
                    "stream": True,
                }
                if effort:
                    kwargs["reasoning"] = {"effort": effort}
                stream = await client.responses.create(**kwargs)
                async for event in stream:
                    try:
                        et = getattr(event, "type", None) if not isinstance(event, dict) else event.get("type")
                        if et in ("response.output_text.delta", "response.refusal.delta"):
                            delta = getattr(event, "delta", None) if not isinstance(event, dict) else event.get("delta")
                            if isinstance(delta, str) and delta:
                                yield delta
                        elif et == "response.error":
                            err = getattr(event, "error", None) if not isinstance(event, dict) else event.get("error")
                            if err:
                                yield f"ì˜¤ë¥˜: OpenAI ëª¨ë¸ í˜¸ì¶œì— ì‹¤íŒ¨í–ˆìŠµë‹ˆë‹¤ - {err}"
                                return
                    except Exception:
                        # ì´ë²¤íŠ¸ íŒŒì‹± ì‹¤íŒ¨ëŠ” ì¡°ìš©íˆ ë¬´ì‹œ(ìŠ¤íŠ¸ë¦¼ ìœ ì§€)
                        continue
                return

            # âœ… SDK ë¯¸ì§€ì› í´ë°±: REST(/v1/responses) SSE ìŠ¤íŠ¸ë¦¬ë°
            try:
                sp = (system_prompt or "").strip()
            except Exception:
                sp = ""
            async for delta in _responses_rest_stream(
                model_name=model,
                user_prompt=prompt,
                temp=temperature,
                max_out_tokens=max_tokens,
                reasoning_effort=effort,
                system_prompt=sp or None,
            ):
                yield delta
            return

        # GPT-4 ê³„ì—´(ê¸°ì¡´): Chat Completions ìŠ¤íŠ¸ë¦¬ë° ìœ ì§€
        try:
            sp = (system_prompt or "").strip()
        except Exception:
            sp = ""
        messages = [{"role": "user", "content": prompt}]
        if sp:
            messages = [{"role": "system", "content": sp}, {"role": "user", "content": prompt}]
        stream = await client.chat.completions.create(
            model=model,
            messages=messages,
            temperature=temperature,
            max_tokens=max_tokens,
            stream=True
        )
        async for chunk in stream:
            if chunk.choices[0].delta.content:
                yield chunk.choices[0].delta.content
    except Exception as e:
        try:
            logger.error(f"OpenAI Stream API í˜¸ì¶œ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {e} (model={model}, prompt_len={len(prompt) if isinstance(prompt, str) else 'n/a'})")
        except Exception:
            pass
        print(f"OpenAI Stream API í˜¸ì¶œ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {e}")
        yield f"ì˜¤ë¥˜: OpenAI ëª¨ë¸ í˜¸ì¶œì— ì‹¤íŒ¨í–ˆìŠµë‹ˆë‹¤ - {str(e)}"

# --- í†µí•© AI ì‘ë‹µ í•¨ìˆ˜ ---
AIModel = Literal["gemini", "claude", "gpt"]

async def get_ai_completion(
    prompt: str,
    model: AIModel = "gemini",
    sub_model: Optional[str] = None,
    temperature: float = 0.7,
    max_tokens: int = 2048
) -> str:
    """
    ì§€ì •ëœ AI ëª¨ë¸ì„ í˜¸ì¶œí•˜ì—¬ ì‘ë‹µì„ ë°˜í™˜í•˜ëŠ” í†µí•© í•¨ìˆ˜ì…ë‹ˆë‹¤.
    """
    if model == "gemini":
        # âœ… Gemini ê¸°ë³¸ sub_model: gemini-3-flash-preview (ì†ë„ ìµœìš°ì„ )
        model_name = sub_model or 'gemini-3-flash-preview'
        return await get_gemini_completion(prompt, temperature, max_tokens, model=model_name)
    elif model == "claude":
        # âœ… Claude ê¸°ë³¸ sub_model: Haiku 4.5 (ì†ë„ ìš°ì„ , ì±„íŒ…ì€ ë³„ë„ í•¨ìˆ˜ ì‚¬ìš©)
        model_name = sub_model or 'claude-haiku-4-5-20251001'
        return await get_claude_completion(prompt, temperature, max_tokens, model=model_name)
    elif model == "gpt":
        model_name = sub_model or 'gpt-4o'
        return await get_openai_completion(prompt, temperature, max_tokens, model=model_name)
    else:
        raise ValueError(f"ì§€ì›í•˜ì§€ ì•ŠëŠ” ëª¨ë¸ì…ë‹ˆë‹¤: {model}")

# --- í†µí•© AI ì‘ë‹µ ìŠ¤íŠ¸ë¦¼ í•¨ìˆ˜ ---
async def get_ai_completion_stream(
    prompt: str,
    model: AIModel = "gemini",
    sub_model: Optional[str] = None,
    temperature: float = 0.7,
    max_tokens: int = 2048
) -> AsyncGenerator[str, None]:
    """ì§€ì •ëœ AI ëª¨ë¸ì˜ ìŠ¤íŠ¸ë¦¬ë° ì‘ë‹µì„ ë°˜í™˜í•˜ëŠ” í†µí•© í•¨ìˆ˜ì…ë‹ˆë‹¤."""
    if model == "gemini":
        # âœ… Gemini ê¸°ë³¸ sub_model: gemini-3-flash-preview (ì†ë„ ìµœìš°ì„ )
        model_name = sub_model or 'gemini-3-flash-preview'
        async for chunk in get_gemini_completion_stream(prompt, temperature, max_tokens, model=model_name):
            yield chunk
    elif model == "claude":
        # âœ… Claude ê¸°ë³¸ sub_model: Haiku 4.5 (ì†ë„ ìš°ì„ )
        model_name = sub_model or 'claude-haiku-4-5-20251001'
        async for chunk in get_claude_completion_stream(prompt, temperature, max_tokens, model=model_name):
            yield chunk
    elif model == "gpt":
        model_name = sub_model or 'gpt-4o'
        async for chunk in get_openai_completion_stream(prompt, temperature, max_tokens, model=model_name):
            yield chunk
    else:
        raise ValueError(f"ì§€ì›í•˜ì§€ ì•ŠëŠ” ëª¨ë¸ì…ë‹ˆë‹¤: {model}")


# --- ê¸°ì¡´ ì±„íŒ… ê´€ë ¨ í•¨ìˆ˜ ---
async def get_ai_chat_response(
    character_prompt: str, 
    user_message: str, 
    history: list, 
    # âœ… ê¸°ë³¸ê°’(ìš”êµ¬ì‚¬í•­): Claude Haiku 4.5
    # - ìœ ì € ì €ì¥ ì„¤ì •ì´ ì—†ê±°ë‚˜, í˜¸ì¶œë¶€ê°€ preferred_model/sub_modelì„ ë„˜ê¸°ì§€ ì•ŠëŠ” ê²½ìš°ì˜ ì•ˆì „ ê¸°ë³¸ê°’.
    preferred_model: str = 'claude',
    preferred_sub_model: str = 'claude-haiku-4-5-20251001',
    # âœ… ê¸°ë³¸ê°’(ìš”êµ¬ì‚¬í•­): short(ì§§ê²Œ)
    response_length_pref: str = 'short',
    temperature: float = 0.7
) -> str:
    """ì‚¬ìš©ìê°€ ì„ íƒí•œ ëª¨ë¸ë¡œ AI ì‘ë‹µ ìƒì„±"""
    # temperature ë°©ì–´ì  ì •ê·œí™”: 0~1
    try:
        t = float(temperature)
        if t < 0:
            t = 0.0
        if t > 1:
            t = 1.0
        # 0.1 ë‹¨ìœ„ ë°˜ì˜¬ë¦¼(í”„ë¡ íŠ¸ì™€ ì •í•©)
        t = round(t * 10) / 10.0
    except Exception:
        t = 0.7
    # ì‚¬ìš©ì ìì—°ì–´ ì˜ë„ ê²½ëŸ‰ íŒŒì‹±(ì¶”ê°€ API í˜¸ì¶œ ì—†ìŒ)
    try:
        intent_info = _parse_user_intent(user_message)
    except Exception:
        intent_info = {}

    # ì˜ë„ ë¸”ë¡ êµ¬ì„±
    intent_lines = []
    if intent_info.get("intent"):
        intent_lines.append(f"ì˜ë„: {intent_info.get('intent')}")
    if intent_info.get("stance") == "first":
        intent_lines.append("ì‹œì : 1ì¸ì¹­ 'ë‚˜'")
    if intent_info.get("stance") == "third":
        intent_lines.append("ì‹œì : 3ì¸ì¹­(ì¸ë¬¼ ì§€ì¹­ì€ 'ê·¸/ê·¸ë…€')")
    if intent_info.get("tone"):
        intent_lines.append(f"í†¤: {intent_info.get('tone')}")
    if intent_info.get("pace"):
        intent_lines.append(f"í…œí¬: {intent_info.get('pace')}")
    for c in intent_info.get("constraints", []):
        intent_lines.append(f"ì œì•½: {c}")
    if intent_info.get("transform_tags"):
        intent_lines.append("íƒœê·¸: " + ", ".join(intent_info.get("transform_tags", [])[:6]))
    intent_block = ("\n[ì˜ë„ ë°˜ì˜]\n" + "\n".join(intent_lines)) if intent_lines else ""

    # âœ… ìµœê·¼ ëŒ€í™” íˆìŠ¤í† ë¦¬ ë°˜ì˜(ë°©ì–´ì )
    # - ì›ì‘ì±—/ì¼ë°˜ì±— ë“±ì—ì„œ historyë¥¼ ë„˜ê²¨ë„ ë¬´ì‹œë˜ë©´ 'ë§ê°/ì„¤ì • ë¶•ê´´'ê°€ ë°œìƒí•œë‹¤.
    # âœ… history ìµœëŒ€ ê°œìˆ˜ëŠ” 100ê¹Œì§€ í—ˆìš©í•˜ë˜, max_chars(12000)ë¡œ í† í° í­ì£¼ë¥¼ 1ì°¨ ë°©ì–´í•œë‹¤.
    # - ì›ì‘ì±—ì€ ìµœì‹  ë§¥ë½ì´ ì¤‘ìš”í•´ íˆìŠ¤í† ë¦¬ fetch limitë¥¼ 80ìœ¼ë¡œ ì˜¬ë ¤ë‘” ìƒíƒœë¼, max_itemsë„ 80ìœ¼ë¡œ ì •í•©ì„ ë§ì¶˜ë‹¤.
    history_block = _format_history_block(history, max_items=100, max_chars=12000)

    # âœ… ì‘ë‹µ ê¸¸ì´ ì„ í˜¸ë„ í”„ë¡¬í”„íŠ¸ ì§€ì¹¨(ì²´ê° ê°•í™”)
    # - ê¸°ì¡´ì—ëŠ” max_tokens(ìƒí•œ)ë§Œ ì¡°ì •ë˜ì–´ "ê¸¸ê²Œ" ì²´ê°ì´ ì•½í•  ìˆ˜ ìˆë‹¤.
    # - ê·¸ë˜ì„œ ëª¨ë¸ì—ê²Œë„ ê¸¸ì´ ê¸°ëŒ€ì¹˜ë¥¼ ëª…ì‹œì ìœ¼ë¡œ ê°€ì´ë“œí•œë‹¤(ì¶œë ¥ì€ ìì—°ìŠ¤ëŸ¬ìš´ ëŒ€í™”ë§Œ).
    length_block = ""
    try:
        rlp = (response_length_pref or "").strip().lower()
    except Exception:
        rlp = ""
    if rlp == "short":
        length_block = (
            "\n[ì‘ë‹µ ê¸¸ì´]\n"
            "- ì§§ê²Œ: 1~2ë¬¸ì¥(ë˜ëŠ” 1ë‹¨ë½)ìœ¼ë¡œ í•µì‹¬ë§Œ.\n"
            "- ë¶ˆí•„ìš”í•œ ì„¤ëª…/ì„¤ì • ì¶”ê°€/ì¥í™©í•œ ë¬˜ì‚¬ ê¸ˆì§€.\n"
            "- ì¶œë ¥ì€ ìì—°ìŠ¤ëŸ¬ìš´ ëŒ€í™”ë§Œ(ë¶ˆë¦¿/ë¼ë²¨/ë²ˆí˜¸/í—¤ë” ê¸ˆì§€).\n"
        )
    elif rlp == "long":
        length_block = (
            "\n[ì‘ë‹µ ê¸¸ì´]\n"
            "- ê¸¸ê²Œ: 6~12ë¬¸ì¥ ì •ë„ë¡œ ì¶©ë¶„íˆ í’ë¶€í•˜ê²Œ.\n"
            "- ê°ì •/í–‰ë™/ìƒí™©ì„ ë” ë¬˜ì‚¬í•˜ë˜, ì„¤ì •/ì‚¬ì‹¤ì„ ì„ì˜ë¡œ ì¶”ê°€í•˜ê±°ë‚˜ ë‹¨ì •í•˜ì§€ ì•ŠëŠ”ë‹¤.\n"
            "- ì¶œë ¥ì€ ìì—°ìŠ¤ëŸ¬ìš´ ëŒ€í™”ë§Œ(ë¶ˆë¦¿/ë¼ë²¨/ë²ˆí˜¸/í—¤ë” ê¸ˆì§€).\n"
        )
    else:
        # medium(ê¸°ë³¸)
        length_block = (
            "\n[ì‘ë‹µ ê¸¸ì´]\n"
            "- ë³´í†µ: 3~6ë¬¸ì¥ ì •ë„ë¡œ ìì—°ìŠ¤ëŸ½ê²Œ.\n"
            "- ì¶œë ¥ì€ ìì—°ìŠ¤ëŸ¬ìš´ ëŒ€í™”ë§Œ(ë¶ˆë¦¿/ë¼ë²¨/ë²ˆí˜¸/í—¤ë” ê¸ˆì§€).\n"
        )

    # âœ… í”„ë¡¬í”„íŠ¸ êµ¬ì„±(ì¤‘ìš”)
    # - GeminiëŠ” ë‹¨ì¼ prompt ë¬¸ìì—´ë¡œ í˜¸ì¶œí•˜ë¯€ë¡œ ê¸°ì¡´ì²˜ëŸ¼ í•©ì¹œ full_promptë¥¼ ìœ ì§€í•œë‹¤.
    # - Claude/GPTëŠ” system(developer)/user ì—­í•  ë¶„ë¦¬ë¡œ "ìºë¦­í„°/ê·œì¹™" ìš°ì„ ìˆœìœ„ë¥¼ ë†’ì¸ë‹¤.
    user_prompt = f"{history_block}{intent_block}{length_block}\n\nì‚¬ìš©ì ë©”ì‹œì§€: {user_message}\n\nìœ„ ì„¤ì •ì— ë§ê²Œ ìì—°ìŠ¤ëŸ½ê²Œ ì‘ë‹µí•˜ì„¸ìš” (ëŒ€í™”ë§Œ ì¶œë ¥, ë¼ë²¨ ì—†ì´):"
    full_prompt = f"{character_prompt}{user_prompt}"

    # ì‘ë‹µ ê¸¸ì´ ì„ í˜¸ë„ â†’ ìµœëŒ€ í† í° ë¹„ìœ¨ ì¡°ì • (ì¤‘ê°„ ê¸°ì¤€ 1.0)
    #
    # âœ… Gemini(Pro ê³„ì—´)ë§Œ ì˜ˆì™¸ ì²˜ë¦¬:
    # - gemini-2.5-pro ê³„ì—´ì€ max_output_tokens(=max_tokens)ê°€ ë„ˆë¬´ ë‚®ìœ¼ë©´ ë‚´ë¶€ ì¶”ë¡ /ì‚¬ê³ ë¡œ í† í°ì„ ì†Œì§„í•œ ë’¤
    #   ìµœì¢… í…ìŠ¤íŠ¸ íŒŒíŠ¸(content.parts)ê°€ ë¹„ì–´(parts_len=0) "ë¹ˆ ì‘ë‹µ"ì´ ë°œìƒí•  ìˆ˜ ìˆë‹¤.
    # - ê·¸ë˜ì„œ "ì§§ê²Œ" ëª¨ë“œì—ì„œë„ GeminiëŠ” ë„ˆë¬´ ì‘ì€ ìƒí•œì„ ì£¼ì§€ ì•Šê³ , ì•ˆì •ì ì¸ ìƒí•œ(ê¸°ë³¸ê°’ 1800)ì„ ìœ ì§€í•œë‹¤.
    #
    # ì°¸ê³ : ì‹¤ì œ ì¶œë ¥ ê¸¸ì´(1~2ë¬¸ì¥/3~6ë¬¸ì¥/6~12ë¬¸ì¥)ëŠ” ìœ„ length_block ì§€ì¹¨ìœ¼ë¡œ ì œì–´í•˜ë©°,
    #       ì—¬ê¸° max_tokensëŠ” 'ìƒí•œ(ceiling)'ì´ë¯€ë¡œ ê°’ì„ í‚¤ì›Œë„ ë¬´ì¡°ê±´ ê¸¸ì–´ì§€ì§€ëŠ” ì•ŠëŠ”ë‹¤.
    base_max_tokens = 1800
    try:
        is_gemini = (preferred_model == 'gemini')
    except Exception:
        is_gemini = False

    if rlp == 'short':
        max_tokens = base_max_tokens if is_gemini else int(base_max_tokens * 0.5)
    elif rlp == 'long':
        max_tokens = int(base_max_tokens * 1.5)
    else:
        max_tokens = base_max_tokens
    
    # ëª¨ë¸ë³„ ì²˜ë¦¬
    if preferred_model == 'gemini':
        # NOTE:
        # - í”„ë¡ íŠ¸(ModelSelectionModal)ì—ì„œëŠ” "gemini-3-flash-preview", "gemini-3-pro-preview" ê°™ì€ UIìš© idë¥¼ ì €ì¥í•œë‹¤.
        #   (ë ˆê±°ì‹œ ê°’: gemini-3-flash / gemini-3-proë„ ë°©ì–´ì ìœ¼ë¡œ í—ˆìš©)
        # - ì‹¤ì œ Gemini í˜¸ì¶œì€ genai.GenerativeModel(<ì‹¤ì œ ëª¨ë¸ëª…>)ì— ë“¤ì–´ê°ˆ ë¬¸ìì—´ì´ í•„ìš”í•˜ë¯€ë¡œ ì—¬ê¸°ì„œ ë§¤í•‘í•œë‹¤.
        # - ê¸°ì¡´ ê¸°ë³¸ê°’(gemini-2.5-pro)ì€ ê·¸ëŒ€ë¡œ ìœ ì§€í•œë‹¤. (ìš”ì²­: 2.5-proëŠ” ê°€ë§Œíˆ)
        try:
            sub = (preferred_sub_model or "").strip()
        except Exception:
            sub = ""

        # Gemini 3 Preview ë§¤í•‘ (ëŒ€í‘œë‹˜ ì œê³µ ì˜ˆì‹œ ê¸°ë°˜)
        if sub in ("gemini-3-pro", "gemini-3-pro-preview"):
            model_name = "gemini-3-pro-preview"
        elif sub in ("gemini-3-flash", "gemini-3-flash-preview"):
            model_name = "gemini-3-flash-preview"
        elif sub == "gemini-2.5-flash":
            model_name = "gemini-2.5-flash"
        else:
            # gemini-2.5-pro(ê¸°ë³¸) í¬í•¨: ì•Œ ìˆ˜ ì—†ëŠ” ê°’ì€ ê¸°ì¡´ ì•ˆì • ê¸°ë³¸ê°’ìœ¼ë¡œ í´ë°±
            model_name = "gemini-2.5-pro"
        # âœ… ëª¨ë¸ ì„ íƒ ë¡œê¹…(í”„ë¡¬í”„íŠ¸/ëŒ€ì‚¬ ë‚´ìš© ì œì™¸)
        try:
            if getattr(settings, "DEBUG", False) or getattr(settings, "ENVIRONMENT", "") != "production":
                logger.info(f"[ai] model_selected provider=gemini sub_model={model_name} (raw={preferred_sub_model}) max_tokens={max_tokens} temp={t}")
        except Exception:
            pass
        # âœ… ì‹¤ì œ í˜¸ì¶œ(ì‹œë„) ë¡œê·¸: Geminië„ "ì‹¤ì œë¡œ ì–´ë–¤ ëª¨ë¸ ë¬¸ìì—´ë¡œ í˜¸ì¶œí–ˆëŠ”ì§€"ë¥¼ ë‹¤ë¥¸ providerì™€ ë™ì¼ í¬ë§·ìœ¼ë¡œ ë‚¨ê¸´ë‹¤.
        # - SDK ë‚´ë¶€ HTTP ë””í…Œì¼ê¹Œì§€ëŠ” ìˆ¨ê²¨ì§ˆ ìˆ˜ ìˆìœ¼ë¯€ë¡œ, ìµœì†Œí•œ resolved model_nameì„ SSOTë¡œ ë³´ì¥í•œë‹¤.
        try:
            if getattr(settings, "DEBUG", False) or getattr(settings, "ENVIRONMENT", "") != "production":
                logger.info(f"[ai] http_call provider=gemini sdk=google-generativeai call=generate_content_async model={model_name} max_tokens={max_tokens} temp={t}")
        except Exception:
            pass
        return await get_gemini_completion(full_prompt, temperature=t, model=model_name, max_tokens=max_tokens)
        
    elif preferred_model == 'claude':
        # í”„ë¡ íŠ¸ì˜ ê°€ìƒ ì„œë¸Œëª¨ë¸ëª…ì„ ì‹¤ì œ Anthropic ëª¨ë¸ IDë¡œ ë§¤í•‘
        # ìœ íš¨í•˜ì§€ ì•Šì€ ê°’ì´ ë“¤ì–´ì˜¤ë©´ ìµœì‹  ì•ˆì • ë²„ì „ìœ¼ë¡œ í´ë°±
        claude_default = CLAUDE_MODEL_PRIMARY
        claude_mapping = {
            # âœ… ê¶Œì¥(SSOT): Anthropicì— ì „ë‹¬ë˜ëŠ” ìŠ¤ëƒ…ìƒ· ëª¨ë¸ëª…(ë‚ ì§œ í¬í•¨)
            'claude-sonnet-4-20250514': 'claude-sonnet-4-20250514',
            'claude-sonnet-4-5-20250929': 'claude-sonnet-4-5-20250929',
            'claude-opus-4-1-20250805': 'claude-opus-4-1-20250805',
            'claude-opus-4-5-20251101': 'claude-opus-4-5-20251101',
            # âœ… ì†ë„ ìµœì í™”(ìš”êµ¬ì‚¬í•­): Haiku 4.5
            'claude-haiku-4-5-20251001': 'claude-haiku-4-5-20251001',

            # âœ… UI/ì €ì¥ê°’ í˜¸í™˜(ë³„ì¹­/ë ˆê±°ì‹œ) â†’ ìŠ¤ëƒ…ìƒ·ìœ¼ë¡œ ë³€í™˜
            'claude-sonnet-4': 'claude-sonnet-4-20250514',
            'claude-sonnet-4-0': 'claude-sonnet-4-20250514',
            'claude-4-sonnet': 'claude-sonnet-4-20250514',
            'claude-sonnet-4.0': 'claude-sonnet-4-20250514',

            'claude-opus-4-1': 'claude-opus-4-1-20250805',
            'claude-opus-4-5': 'claude-opus-4-5-20251101',

            'claude-sonnet-4-5': 'claude-sonnet-4-5-20250929',
            'claude-sonnet-4.5': 'claude-sonnet-4-5-20250929',
            'claude-sonnet-4.5-think': 'claude-sonnet-4-5-20250929',
            'claude-opus-4.5': 'claude-opus-4-5-20251101',
        }

        try:
            sub = (preferred_sub_model or "").strip()
        except Exception:
            sub = ""
        model_name = claude_mapping.get(sub, claude_default)
        # âœ… ëª¨ë¸ ì„ íƒ ë¡œê¹…(í”„ë¡¬í”„íŠ¸/ëŒ€ì‚¬ ë‚´ìš© ì œì™¸)
        try:
            if getattr(settings, "DEBUG", False) or getattr(settings, "ENVIRONMENT", "") != "production":
                logger.info(f"[ai] model_selected provider=claude sub_model={model_name} (raw={preferred_sub_model}) max_tokens={max_tokens} temp={t}")
        except Exception:
            pass
        return await get_claude_completion(
            user_prompt,
            temperature=t,
            model=model_name,
            max_tokens=max_tokens,
            system_prompt=character_prompt,
        )
        
    elif preferred_model == 'gpt':
        # NOTE:
        # - í”„ë¡ íŠ¸(ModelSelectionModal)ì—ì„œ gpt-5.1/gpt-5.2 ë“± ìµœì‹  ëª¨ë¸ëª…ì„ ì„ íƒí•  ìˆ˜ ìˆë‹¤.
        # - GPT-5 ê³„ì—´ì€ get_openai_completion ë‚´ë¶€ì—ì„œ Responses APIë¡œ ë¶„ê¸°ëœë‹¤.
        try:
            sub = (preferred_sub_model or "").strip()
        except Exception:
            sub = ""

        if sub.startswith("gpt-5"):
            model_name = sub
        elif sub in ("gpt-4.1", "gpt-4.1-mini", "gpt-4o"):
            model_name = sub
        else:
            # ì•Œ ìˆ˜ ì—†ëŠ” ê°’ì€ ê¸°ì¡´ ì•ˆì • ê¸°ë³¸ê°’ìœ¼ë¡œ í´ë°±
            model_name = 'gpt-4o'
        # âœ… ëª¨ë¸ ì„ íƒ ë¡œê¹…(í”„ë¡¬í”„íŠ¸/ëŒ€ì‚¬ ë‚´ìš© ì œì™¸)
        try:
            if getattr(settings, "DEBUG", False) or getattr(settings, "ENVIRONMENT", "") != "production":
                logger.info(f"[ai] model_selected provider=gpt sub_model={model_name} (raw={preferred_sub_model}) max_tokens={max_tokens} temp={t}")
        except Exception:
            pass
        return await get_openai_completion(
            user_prompt,
            temperature=t,
            model=model_name,
            max_tokens=max_tokens,
            system_prompt=character_prompt,
        )
        
    else:  # argo (ê¸°ë³¸ê°’)
        # ARGO ëª¨ë¸ì€ í–¥í›„ ì»¤ìŠ¤í…€ API êµ¬í˜„ ì˜ˆì •, í˜„ì¬ëŠ” Geminië¡œ ëŒ€ì²´
        return await get_gemini_completion(full_prompt, temperature=t, model='gemini-2.5-pro', max_tokens=max_tokens)


async def regenerate_partial_text(
    selected_text: str,
    user_prompt: str,
    before_context: str = "",
    after_context: str = ""
) -> str:
    """ì„ íƒëœ í…ìŠ¤íŠ¸ ë¶€ë¶„ì„ ì‚¬ìš©ì ì§€ì‹œì‚¬í•­ì— ë”°ë¼ ì¬ìƒì„±
    
    Args:
        selected_text: ì„ íƒëœ ì›ë³¸ í…ìŠ¤íŠ¸
        user_prompt: ì‚¬ìš©ìì˜ ìˆ˜ì • ì§€ì‹œì‚¬í•­ (ì˜ˆ: "ë” ê°ì„±ì ìœ¼ë¡œ", "ì§§ê²Œ ìš”ì•½í•´ì¤˜")
        before_context: ì„ íƒ ì˜ì—­ ì´ì „ í…ìŠ¤íŠ¸ (ë§¥ë½)
        after_context: ì„ íƒ ì˜ì—­ ì´í›„ í…ìŠ¤íŠ¸ (ë§¥ë½)
    
    Returns:
        ì¬ìƒì„±ëœ í…ìŠ¤íŠ¸
    """
    try:
        # í”„ë¡¬í”„íŠ¸ êµ¬ì„±
        prompt = f"""ë‹¤ìŒì€ ì†Œì„¤/ìŠ¤í† ë¦¬ì˜ ì¼ë¶€ì…ë‹ˆë‹¤. ì‚¬ìš©ìê°€ ì„ íƒí•œ ë¶€ë¶„ì„ ì§€ì‹œì‚¬í•­ì— ë”°ë¼ ì¬ì‘ì„±í•´ì£¼ì„¸ìš”.

[ì´ì „ ë§¥ë½]
{before_context[-500:] if before_context else "(ì—†ìŒ)"}

[ì„ íƒëœ ë¶€ë¶„ - ì´ ë¶€ë¶„ì„ ì¬ì‘ì„±í•´ì•¼ í•©ë‹ˆë‹¤]
{selected_text}

[ì´í›„ ë§¥ë½]
{after_context[:500] if after_context else "(ì—†ìŒ)"}

[ì‚¬ìš©ì ì§€ì‹œì‚¬í•­]
{user_prompt}

## ì¬ì‘ì„± ì§€ì¹¨:
1. ì´ì „/ì´í›„ ë§¥ë½ê³¼ ìì—°ìŠ¤ëŸ½ê²Œ ì—°ê²°ë˜ì–´ì•¼ í•©ë‹ˆë‹¤
2. ì‚¬ìš©ì ì§€ì‹œì‚¬í•­ì„ ìµœëŒ€í•œ ë°˜ì˜í•˜ë˜, ìŠ¤í† ë¦¬ì˜ íë¦„ì„ í•´ì¹˜ì§€ ì•Šì•„ì•¼ í•©ë‹ˆë‹¤
3. ì›ë³¸ì˜ í•µì‹¬ ë‚´ìš©ì€ ìœ ì§€í•˜ë˜, í‘œí˜„/ìŠ¤íƒ€ì¼/ê¸¸ì´ ë“±ì„ ì¡°ì •í•©ë‹ˆë‹¤
4. ì¶”ê°€ ì„¤ëª… ì—†ì´ ì¬ì‘ì„±ëœ í…ìŠ¤íŠ¸ë§Œ ì¶œë ¥í•˜ì„¸ìš”

ì¬ì‘ì„±ëœ í…ìŠ¤íŠ¸:"""

        # Claude API í˜¸ì¶œ
        result = await get_claude_completion(
            prompt,
            temperature=0.7,
            max_tokens=2000,
            model=CLAUDE_MODEL_PRIMARY
        )
        
        return result.strip()
        
    except Exception as e:
        logger.error(f"Failed to regenerate partial text: {e}")
        raise
