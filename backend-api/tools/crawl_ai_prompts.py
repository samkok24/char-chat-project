"""
ë””ì‹œì¸ì‚¬ì´ë“œ AI ì±„íŒ… ê°¤ëŸ¬ë¦¬ / ì•„ì¹´ë¼ì´ë¸Œ AI ì±„íŒ… ì±„ë„ì—ì„œ
ê³ í’ˆì§ˆ RP í”„ë¡¬í”„íŠ¸ë¥¼ í¬ë¡¤ë§í•˜ëŠ” ìŠ¤í¬ë¦½íŠ¸.

ì‚¬ìš©ë²•:
    python crawl_ai_prompts.py --site dcinside --pages 10
    python crawl_ai_prompts.py --site arca --pages 10

ì¶œë ¥:
    outputs/crawled_prompts_dcinside_YYYYMMDD.json
    outputs/crawled_prompts_arca_YYYYMMDD.json
"""

import argparse
import json
import os
import re
import time
from datetime import datetime
from typing import List, Dict, Optional
from urllib.parse import urljoin

import requests
from bs4 import BeautifulSoup

# ============================================================================
# ì„¤ì •
# ============================================================================

HEADERS = {
    "User-Agent": "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36",
    "Accept": "text/html,application/xhtml+xml,application/xml;q=0.9,image/webp,*/*;q=0.8",
    "Accept-Language": "ko-KR,ko;q=0.9,en-US;q=0.8,en;q=0.7",
}

# ë””ì‹œì¸ì‚¬ì´ë“œ ê°¤ëŸ¬ë¦¬ë“¤
DC_BASE_URL = "https://gall.dcinside.com"

# í¬ë¡¤ë§ ëŒ€ìƒ ê°¤ëŸ¬ë¦¬ ëª©ë¡ (ê°¤ëŸ¬ë¦¬ ID: í‘œì‹œ ì´ë¦„)
DC_GALLERIES = {
    "wrtnai": "í¬ë™(ë¤¼íŠ¼)",    # https://gall.dcinside.com/mgallery/board/lists/?id=wrtnai (ë©”ì¸!)
    "babechat": "ë°”ë² ì±—",      # https://gall.dcinside.com/mgallery/board/lists/?id=babechat
    "aichatting": "AI ì±„íŒ…",   # https://gall.dcinside.com/mgallery/board/lists/?id=aichatting
    "aicharacter": "AI ìºë¦­í„°", # https://gall.dcinside.com/mgallery/board/lists/?id=aicharacter
}

# ì•„ì¹´ë¼ì´ë¸Œ AI ì±„íŒ… ì±„ë„
ARCA_BASE_URL = "https://arca.live"
ARCA_CHANNEL = "characterai"
ARCA_LIST_URL = f"{ARCA_BASE_URL}/b/{ARCA_CHANNEL}"

# RP ìºë¦­í„° í”„ë¡¬í”„íŠ¸ í‚¤ì›Œë“œ (ì œëª©ì—ì„œ í•„í„°ë§) - ì™„í™”ë¨
PROMPT_KEYWORDS = [
    "ìºë¦­í„°", "ë´‡", "í˜ë¥´ì†Œë‚˜", "í˜ì†Œ", "ì‹œíŠ¸", "ì„¤ì •",
    "rp", "ë¡¤í”Œ", "ë¡¤í”Œë ˆì´", "ì‹œë®¬", "ì‹œë®¬ë ˆì´í„°",
    "ë°°í¬", "ê³µìœ ", "í¼ë©”", "í¼ë¨¸",
    "í”„ë¡¬í”„íŠ¸", "ì„¸ê³„ê´€", "ìŠ¤í† ë¦¬",
    "ì œì‘", "ì™„ì„±", "ì—…ë¡œë“œ", "ì˜¬ë¦¼",
]

# í”„ë¡¬í”„íŠ¸ ê´€ë ¨ ë§ë¨¸ë¦¬ (ì´ê²Œ ìˆìœ¼ë©´ í‚¤ì›Œë“œ ì—†ì–´ë„ í†µê³¼)
PROMPT_CATEGORIES = [
    "í™ë³´", "ğŸ”°í™ë³´", "ğŸ”´í™ë³´",  # ìºë¦­í„° ë°°í¬
    "ì œì‘í˜„í™©", "ì œì‘ì¤‘",
    "ìœ ì €ë…¸íŠ¸",
    "ğŸ”¨ì œì‘",
    "ğŸ“¢í™ë³´",
]

# ì œì™¸ í‚¤ì›Œë“œ (ì œëª©) - ì™„í™”ë¨
EXCLUDE_KEYWORDS = [
    "ì§ˆë¬¸", "ë„ì›€", "ì–´ë–»ê²Œ", "ì¶”ì²œí•´", "ë­ê°€", "ì™œì´ëŸ¬",
    "í›„ê¸°", "ë¦¬ë·°ë§Œ", "ê°ìƒë§Œ",
]

# ì´ë¯¸ì§€ ìƒì„± í”„ë¡¬í”„íŠ¸ ê°ì§€ íŒ¨í„´ (ë³¸ë¬¸ì—ì„œ ì œì™¸)
IMAGE_PROMPT_PATTERNS = [
    r"solo,?\s",  # NAI/SD íƒœê·¸
    r"looking_at",
    r"cowboy_shot",
    r"upper_body",
    r"from_above",
    r"from_below",
    r"depth_of_field",
    r"bokeh",
    r"cinematic",
    r"lighting,?\s",
    r"\d+::",  # ê°€ì¤‘ì¹˜ ë¬¸ë²• 1.2::
    r"::,",
    r"pov,?\s",
    r"indoor,?\s",
    r"outdoor,?\s",
]

OUTPUT_DIR = "outputs"


# ============================================================================
# ë””ì‹œì¸ì‚¬ì´ë“œ í¬ë¡¤ëŸ¬
# ============================================================================

def crawl_dcinside_list(gallery_id: str, page: int = 1) -> List[Dict]:
    """
    ë””ì‹œì¸ì‚¬ì´ë“œ ê°¤ëŸ¬ë¦¬ ëª©ë¡ì—ì„œ í”„ë¡¬í”„íŠ¸ ê´€ë ¨ ê²Œì‹œë¬¼ ë§í¬ë¥¼ ìˆ˜ì§‘í•œë‹¤.
    """
    url = f"{DC_BASE_URL}/mgallery/board/lists/?id={gallery_id}&page={page}"
    gallery_name = DC_GALLERIES.get(gallery_id, gallery_id)
    print(f"[DC:{gallery_name}] ëª©ë¡ í¬ë¡¤ë§: page {page}")
    
    try:
        resp = requests.get(url, headers=HEADERS, timeout=10)
        resp.raise_for_status()
    except Exception as e:
        print(f"[DC] ëª©ë¡ ìš”ì²­ ì‹¤íŒ¨: {e}")
        return []
    
    soup = BeautifulSoup(resp.text, "html.parser")
    posts = []
    
    # ê²Œì‹œë¬¼ ëª©ë¡ íŒŒì‹±
    rows = soup.select("tr.ub-content")
    for row in rows:
        try:
            # ê³µì§€ ì œì™¸
            if row.select_one(".icon_notice"):
                continue
            
            # ì œëª© ì¶”ì¶œ
            title_elem = row.select_one(".gall_tit a:first-child")
            if not title_elem:
                continue
            
            title = title_elem.get_text(strip=True)
            href = title_elem.get("href", "")
            
            # ë§ë¨¸ë¦¬ ì¶”ì¶œ
            em_elem = row.select_one(".gall_tit em")
            category = em_elem.get_text(strip=True) if em_elem else ""
            
            # í”„ë¡¬í”„íŠ¸ ê´€ë ¨ ê²Œì‹œë¬¼ë§Œ í•„í„°ë§
            title_lower = title.lower()
            category_lower = category.lower()
            
            # ë””ë²„ê·¸: ì²˜ìŒ 3ê°œ ê²Œì‹œë¬¼ ì¶œë ¥
            if len(posts) < 3:
                print(f"    [DEBUG] ë§ë¨¸ë¦¬=[{category}] ì œëª©=[{title[:30]}...]")
            
            # 1) í”„ë¡¬í”„íŠ¸ ê´€ë ¨ ë§ë¨¸ë¦¬ ì²´í¬ (ìµœìš°ì„ )
            is_prompt = any(cat in category for cat in PROMPT_CATEGORIES)
            
            # 2) í”„ë¡¬í”„íŠ¸ í‚¤ì›Œë“œ ì²´í¬ (ë§ë¨¸ë¦¬ ë˜ëŠ” ì œëª©)
            if not is_prompt:
                is_prompt = any(kw in title_lower or kw in category_lower for kw in PROMPT_KEYWORDS)
            
            # 3) ê¸´ ì œëª©ì€ í”„ë¡¬í”„íŠ¸ì¼ ê°€ëŠ¥ì„± (ì„¤ì • ê³µìœ )
            if not is_prompt and len(title) > 50:
                is_prompt = True
            
            # ì œì™¸: ì´ë¯¸ì§€ ê´€ë ¨, ì§ˆë¬¸
            if any(kw in title_lower for kw in EXCLUDE_KEYWORDS):
                continue
            if any(kw in title_lower for kw in ["ìœ¶ìº", "ëšê±°", "ê·¸ë½‘", "pov", "í”„ë¡¬í”„ë¡¬"]):
                continue
            
            if not is_prompt:
                continue
            
            # ì¡°íšŒìˆ˜/ì¶”ì²œìˆ˜ ì¶”ì¶œ
            view_elem = row.select_one(".gall_count")
            rec_elem = row.select_one(".gall_recommend")
            views = int(view_elem.get_text(strip=True)) if view_elem else 0
            recs = int(rec_elem.get_text(strip=True)) if rec_elem else 0
            
            # ì¶”ì²œ 0ê°œ ì´ìƒ (ì¼ë‹¨ ë‹¤ ìˆ˜ì§‘, ë‚˜ì¤‘ì— í•„í„°ë§)
            # if recs < 1:
            #     continue
            
            post_url = urljoin(DC_BASE_URL, href)
            posts.append({
                "title": title,
                "category": category,
                "url": post_url,
                "views": views,
                "recs": recs,
            })
            
        except Exception as e:
            print(f"[DC] í–‰ íŒŒì‹± ì˜¤ë¥˜: {e}")
            continue
    
    print(f"[DC] í˜ì´ì§€ {page}ì—ì„œ {len(posts)}ê°œ ê²Œì‹œë¬¼ ë°œê²¬")
    return posts


def crawl_dcinside_post(url: str) -> Optional[Dict]:
    """
    ë””ì‹œì¸ì‚¬ì´ë“œ ê²Œì‹œë¬¼ ë³¸ë¬¸ì—ì„œ í”„ë¡¬í”„íŠ¸ ë‚´ìš©ì„ ì¶”ì¶œí•œë‹¤.
    """
    print(f"[DC] ë³¸ë¬¸ í¬ë¡¤ë§: {url}")
    
    try:
        resp = requests.get(url, headers=HEADERS, timeout=10)
        resp.raise_for_status()
    except Exception as e:
        print(f"[DC] ë³¸ë¬¸ ìš”ì²­ ì‹¤íŒ¨: {e}")
        return None
    
    soup = BeautifulSoup(resp.text, "html.parser")
    
    try:
        # ì œëª©
        title_elem = soup.select_one(".title_subject")
        title = title_elem.get_text(strip=True) if title_elem else ""
        
        # ë³¸ë¬¸
        content_elem = soup.select_one(".write_div")
        if not content_elem:
            return None
        
        # ë³¸ë¬¸ í…ìŠ¤íŠ¸ ì¶”ì¶œ (HTML íƒœê·¸ ì œê±°, ì¤„ë°”ê¿ˆ ë³´ì¡´)
        for br in content_elem.find_all("br"):
            br.replace_with("\n")
        content = content_elem.get_text(separator="\n").strip()
        
        # ë„ˆë¬´ ì§§ìœ¼ë©´ ìŠ¤í‚µ (RP í”„ë¡¬í”„íŠ¸ëŠ” ë³´í†µ 800ì ì´ìƒ)
        if len(content) < 800:
            print(f"[DC] ë³¸ë¬¸ì´ ë„ˆë¬´ ì§§ìŒ ({len(content)}ì), ìŠ¤í‚µ")
            return None
        
        # ì´ë¯¸ì§€ ìƒì„± í”„ë¡¬í”„íŠ¸ ê°ì§€ (NAI/SD íƒœê·¸ íŒ¨í„´)
        image_pattern_count = 0
        content_lower = content.lower()
        for pattern in IMAGE_PROMPT_PATTERNS:
            if re.search(pattern, content_lower):
                image_pattern_count += 1
        
        # ì´ë¯¸ì§€ í”„ë¡¬í”„íŠ¸ íŒ¨í„´ì´ 3ê°œ ì´ìƒì´ë©´ ìŠ¤í‚µ
        if image_pattern_count >= 3:
            print(f"[DC] ì´ë¯¸ì§€ ìƒì„± í”„ë¡¬í”„íŠ¸ë¡œ íŒë‹¨ (íŒ¨í„´ {image_pattern_count}ê°œ), ìŠ¤í‚µ")
            return None
        
        # ì‘ì„±ì
        writer_elem = soup.select_one(".nickname")
        writer = writer_elem.get_text(strip=True) if writer_elem else "ìµëª…"
        
        # ì‘ì„±ì¼
        date_elem = soup.select_one(".gall_date")
        date_str = date_elem.get("title", "") if date_elem else ""
        
        return {
            "source": "dcinside",
            "title": title,
            "content": content,
            "writer": writer,
            "date": date_str,
            "url": url,
            "char_count": len(content),
        }
        
    except Exception as e:
        print(f"[DC] ë³¸ë¬¸ íŒŒì‹± ì˜¤ë¥˜: {e}")
        return None


def crawl_dcinside(pages: int = 10, galleries: List[str] = None) -> List[Dict]:
    """
    ë””ì‹œì¸ì‚¬ì´ë“œ ì—¬ëŸ¬ ê°¤ëŸ¬ë¦¬ì—ì„œ í”„ë¡¬í”„íŠ¸ë¥¼ í¬ë¡¤ë§í•œë‹¤.
    
    Args:
        pages: ê° ê°¤ëŸ¬ë¦¬ë‹¹ í¬ë¡¤ë§í•  í˜ì´ì§€ ìˆ˜
        galleries: í¬ë¡¤ë§í•  ê°¤ëŸ¬ë¦¬ ID ëª©ë¡ (Noneì´ë©´ ì „ì²´)
    """
    if galleries is None:
        galleries = list(DC_GALLERIES.keys())
    
    all_posts = []
    
    for gallery_id in galleries:
        gallery_name = DC_GALLERIES.get(gallery_id, gallery_id)
        print(f"\n{'='*50}")
        print(f"[DC:{gallery_name}] ê°¤ëŸ¬ë¦¬ í¬ë¡¤ë§ ì‹œì‘")
        print(f"{'='*50}")
        
        for page in range(1, pages + 1):
            posts = crawl_dcinside_list(gallery_id, page)
            for post in posts:
                post["gallery"] = gallery_id
                post["gallery_name"] = gallery_name
            all_posts.extend(posts)
            time.sleep(1)  # Rate limiting
    
    print(f"\n[DC] ì´ {len(all_posts)}ê°œ ê²Œì‹œë¬¼ ë°œê²¬, ë³¸ë¬¸ í¬ë¡¤ë§ ì‹œì‘...\n")
    
    results = []
    for i, post in enumerate(all_posts):
        print(f"[{i+1}/{len(all_posts)}] ", end="")
        data = crawl_dcinside_post(post["url"])
        if data:
            data["meta"] = {
                "gallery": post.get("gallery", ""),
                "gallery_name": post.get("gallery_name", ""),
                "category": post.get("category", ""),
                "views": post.get("views", 0),
                "recs": post.get("recs", 0),
            }
            results.append(data)
        time.sleep(0.5)  # Rate limiting
    
    return results


# ============================================================================
# ì•„ì¹´ë¼ì´ë¸Œ í¬ë¡¤ëŸ¬
# ============================================================================

def crawl_arca_list(page: int = 1) -> List[Dict]:
    """
    ì•„ì¹´ë¼ì´ë¸Œ ì±„ë„ ëª©ë¡ì—ì„œ í”„ë¡¬í”„íŠ¸ ê´€ë ¨ ê²Œì‹œë¬¼ ë§í¬ë¥¼ ìˆ˜ì§‘í•œë‹¤.
    """
    url = f"{ARCA_LIST_URL}?p={page}"
    print(f"[ARCA] ëª©ë¡ í¬ë¡¤ë§: {url}")
    
    try:
        resp = requests.get(url, headers=HEADERS, timeout=10)
        resp.raise_for_status()
    except Exception as e:
        print(f"[ARCA] ëª©ë¡ ìš”ì²­ ì‹¤íŒ¨: {e}")
        return []
    
    soup = BeautifulSoup(resp.text, "html.parser")
    posts = []
    
    # ê²Œì‹œë¬¼ ëª©ë¡ íŒŒì‹±
    rows = soup.select(".list-table a.vrow")
    for row in rows:
        try:
            # ê³µì§€ ì œì™¸
            if "notice" in row.get("class", []):
                continue
            
            # ì œëª© ì¶”ì¶œ
            title_elem = row.select_one(".title")
            if not title_elem:
                continue
            
            title = title_elem.get_text(strip=True)
            href = row.get("href", "")
            
            # ì¹´í…Œê³ ë¦¬ ì¶”ì¶œ
            badge_elem = row.select_one(".badge")
            category = badge_elem.get_text(strip=True) if badge_elem else ""
            
            # í”„ë¡¬í”„íŠ¸ ê´€ë ¨ ê²Œì‹œë¬¼ë§Œ í•„í„°ë§
            title_lower = title.lower()
            
            # ì œì™¸ í‚¤ì›Œë“œ ì²´í¬
            if any(kw in title_lower for kw in EXCLUDE_KEYWORDS):
                continue
            
            # í”„ë¡¬í”„íŠ¸ í‚¤ì›Œë“œ ì²´í¬
            is_prompt = any(kw in title_lower or kw in category.lower() for kw in PROMPT_KEYWORDS)
            
            # íŠ¹ì • ì¹´í…Œê³ ë¦¬ëŠ” ë†’ì€ í™•ë¥ ë¡œ í”„ë¡¬í”„íŠ¸
            if category in ["ë°°í¬", "ê³µìœ ", "í”„ë¡¬í”„íŠ¸", "ìºë¦­í„°"]:
                is_prompt = True
            
            if not is_prompt:
                continue
            
            # ì¶”ì²œìˆ˜ ì¶”ì¶œ
            rec_elem = row.select_one(".vcol.col-rate")
            recs = 0
            if rec_elem:
                rec_text = rec_elem.get_text(strip=True)
                try:
                    recs = int(rec_text) if rec_text else 0
                except:
                    pass
            
            # ì¶”ì²œ 1ê°œ ì´ìƒë§Œ
            if recs < 1:
                continue
            
            post_url = urljoin(ARCA_BASE_URL, href)
            posts.append({
                "title": title,
                "category": category,
                "url": post_url,
                "recs": recs,
            })
            
        except Exception as e:
            print(f"[ARCA] í–‰ íŒŒì‹± ì˜¤ë¥˜: {e}")
            continue
    
    print(f"[ARCA] í˜ì´ì§€ {page}ì—ì„œ {len(posts)}ê°œ ê²Œì‹œë¬¼ ë°œê²¬")
    return posts


def crawl_arca_post(url: str) -> Optional[Dict]:
    """
    ì•„ì¹´ë¼ì´ë¸Œ ê²Œì‹œë¬¼ ë³¸ë¬¸ì—ì„œ í”„ë¡¬í”„íŠ¸ ë‚´ìš©ì„ ì¶”ì¶œí•œë‹¤.
    """
    print(f"[ARCA] ë³¸ë¬¸ í¬ë¡¤ë§: {url}")
    
    try:
        resp = requests.get(url, headers=HEADERS, timeout=10)
        resp.raise_for_status()
    except Exception as e:
        print(f"[ARCA] ë³¸ë¬¸ ìš”ì²­ ì‹¤íŒ¨: {e}")
        return None
    
    soup = BeautifulSoup(resp.text, "html.parser")
    
    try:
        # ì œëª©
        title_elem = soup.select_one(".title-row .title")
        title = title_elem.get_text(strip=True) if title_elem else ""
        
        # ë³¸ë¬¸
        content_elem = soup.select_one(".article-body")
        if not content_elem:
            return None
        
        # ë³¸ë¬¸ í…ìŠ¤íŠ¸ ì¶”ì¶œ
        for br in content_elem.find_all("br"):
            br.replace_with("\n")
        content = content_elem.get_text(separator="\n").strip()
        
        # ë„ˆë¬´ ì§§ìœ¼ë©´ ìŠ¤í‚µ
        if len(content) < 500:
            print(f"[ARCA] ë³¸ë¬¸ì´ ë„ˆë¬´ ì§§ìŒ ({len(content)}ì), ìŠ¤í‚µ")
            return None
        
        # ì‘ì„±ì
        writer_elem = soup.select_one(".user-info .username")
        writer = writer_elem.get_text(strip=True) if writer_elem else "ìµëª…"
        
        # ì‘ì„±ì¼
        date_elem = soup.select_one(".date")
        date_str = date_elem.get_text(strip=True) if date_elem else ""
        
        return {
            "source": "arca",
            "title": title,
            "content": content,
            "writer": writer,
            "date": date_str,
            "url": url,
            "char_count": len(content),
        }
        
    except Exception as e:
        print(f"[ARCA] ë³¸ë¬¸ íŒŒì‹± ì˜¤ë¥˜: {e}")
        return None


def crawl_arca(pages: int = 10) -> List[Dict]:
    """
    ì•„ì¹´ë¼ì´ë¸Œ AI ì±„íŒ… ì±„ë„ì—ì„œ í”„ë¡¬í”„íŠ¸ë¥¼ í¬ë¡¤ë§í•œë‹¤.
    """
    all_posts = []
    
    for page in range(1, pages + 1):
        posts = crawl_arca_list(page)
        all_posts.extend(posts)
        time.sleep(1)  # Rate limiting
    
    print(f"\n[ARCA] ì´ {len(all_posts)}ê°œ ê²Œì‹œë¬¼ ë°œê²¬, ë³¸ë¬¸ í¬ë¡¤ë§ ì‹œì‘...\n")
    
    results = []
    for i, post in enumerate(all_posts):
        print(f"[{i+1}/{len(all_posts)}] ", end="")
        data = crawl_arca_post(post["url"])
        if data:
            data["meta"] = {
                "category": post.get("category", ""),
                "recs": post.get("recs", 0),
            }
            results.append(data)
        time.sleep(0.5)  # Rate limiting
    
    return results


# ============================================================================
# ë©”ì¸
# ============================================================================

def save_results(results: List[Dict], site: str):
    """
    í¬ë¡¤ë§ ê²°ê³¼ë¥¼ JSON íŒŒì¼ë¡œ ì €ì¥í•œë‹¤.
    """
    os.makedirs(OUTPUT_DIR, exist_ok=True)
    
    date_str = datetime.now().strftime("%Y%m%d_%H%M%S")
    filename = f"crawled_prompts_{site}_{date_str}.json"
    filepath = os.path.join(OUTPUT_DIR, filename)
    
    with open(filepath, "w", encoding="utf-8") as f:
        json.dump(results, f, ensure_ascii=False, indent=2)
    
    print(f"\nâœ… ì €ì¥ ì™„ë£Œ: {filepath}")
    print(f"   - ì´ {len(results)}ê°œ í”„ë¡¬í”„íŠ¸")
    
    # í†µê³„ ì¶œë ¥
    if results:
        char_counts = [r.get("char_count", 0) for r in results]
        print(f"   - í‰ê·  ê¸¸ì´: {sum(char_counts) // len(char_counts)}ì")
        print(f"   - ìµœì†Œ ê¸¸ì´: {min(char_counts)}ì")
        print(f"   - ìµœëŒ€ ê¸¸ì´: {max(char_counts)}ì")


def main():
    parser = argparse.ArgumentParser(description="AI RP í”„ë¡¬í”„íŠ¸ í¬ë¡¤ëŸ¬")
    parser.add_argument("--site", choices=["dcinside", "arca", "all"], default="dcinside",
                        help="í¬ë¡¤ë§ ëŒ€ìƒ ì‚¬ì´íŠ¸")
    parser.add_argument("--pages", type=int, default=10,
                        help="ê° ê°¤ëŸ¬ë¦¬ë‹¹ í¬ë¡¤ë§í•  í˜ì´ì§€ ìˆ˜")
    parser.add_argument("--gallery", type=str, default="all",
                        help="ë””ì‹œ ê°¤ëŸ¬ë¦¬ ì„ íƒ (crack, babychat, aichatting, aicharacter, all)")
    args = parser.parse_args()
    
    print("=" * 60)
    print("AI RP í”„ë¡¬í”„íŠ¸ í¬ë¡¤ëŸ¬")
    print("=" * 60)
    print(f"ëŒ€ìƒ ì‚¬ì´íŠ¸: {args.site}")
    print(f"í˜ì´ì§€/ê°¤ëŸ¬ë¦¬: {args.pages}")
    print(f"ê°¤ëŸ¬ë¦¬: {args.gallery}")
    print("=" * 60)
    
    # ê°¤ëŸ¬ë¦¬ ì„ íƒ
    if args.gallery == "all":
        galleries = None  # ì „ì²´
    else:
        galleries = [g.strip() for g in args.gallery.split(",")]
    
    if args.site in ["dcinside", "all"]:
        print("\n[1] ë””ì‹œì¸ì‚¬ì´ë“œ ê°¤ëŸ¬ë¦¬ í¬ë¡¤ë§ ì‹œì‘")
        print(f"    ëŒ€ìƒ: {galleries if galleries else 'ì „ì²´ (' + ', '.join(DC_GALLERIES.keys()) + ')'}\n")
        dc_results = crawl_dcinside(args.pages, galleries)
        if dc_results:
            save_results(dc_results, "dcinside_rp")
    
    if args.site in ["arca", "all"]:
        print("\n[2] ì•„ì¹´ë¼ì´ë¸Œ AI ì±„íŒ… ì±„ë„ í¬ë¡¤ë§ ì‹œì‘\n")
        arca_results = crawl_arca(args.pages)
        if arca_results:
            save_results(arca_results, "arca_rp")
    
    print("\nâœ… í¬ë¡¤ë§ ì™„ë£Œ!")


if __name__ == "__main__":
    main()
